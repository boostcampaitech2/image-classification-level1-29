{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9deef31c-37fe-4105-bf97-f1c76700ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_optimizer\n",
      "  Downloading torch_optimizer-0.1.0-py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 724 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pytorch-ranger>=0.1.1\n",
      "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: torch>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from torch_optimizer) (1.7.1)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.1.0->torch_optimizer) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torch>=1.1.0->torch_optimizer) (1.19.2)\n",
      "Installing collected packages: pytorch-ranger, torch-optimizer\n",
      "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "from torchsummary import summary\n",
    "import torch_optimizer as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77238f9-9fa5-45fa-97c9-68cd3724be52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170d98a-18e5-4a7f-91eb-575469d09378",
   "metadata": {},
   "source": [
    "## 1. 데이터셋 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03128f-f827-4668-b213-8ae16d0f08f7",
   "metadata": {},
   "source": [
    "### (1) Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc6a210-a832-4bd7-b68f-466aa2761d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터셋 폴더 경로를 지정해주세요.\n",
    "train_dir = '/opt/ml/input/data/train'\n",
    "trainimage_dir = os.path.join(train_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c412b27-edf7-446e-b965-40a7ffc154d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "0     000001  female  Asian   45  000001_female_Asian_45\n",
       "1     000002  female  Asian   52  000002_female_Asian_52\n",
       "2     000004    male  Asian   54    000004_male_Asian_54\n",
       "3     000005  female  Asian   58  000005_female_Asian_58\n",
       "4     000006  female  Asian   59  000006_female_Asian_59\n",
       "...      ...     ...    ...  ...                     ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "train_df = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87297828-b7a3-421b-9812-59a0abed5f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>mask3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>mask4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>mask5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>incorrect_mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  gender   race   age                    path            mask\n",
       "0      000001  female  Asian  45.0  000001_female_Asian_45           mask1\n",
       "1      000001  female  Asian  45.0  000001_female_Asian_45           mask2\n",
       "2      000001  female  Asian  45.0  000001_female_Asian_45           mask3\n",
       "3      000001  female  Asian  45.0  000001_female_Asian_45           mask4\n",
       "4      000001  female  Asian  45.0  000001_female_Asian_45           mask5\n",
       "...       ...     ...    ...   ...                     ...             ...\n",
       "18895  006959    male  Asian  19.0    006959_male_Asian_19           mask3\n",
       "18896  006959    male  Asian  19.0    006959_male_Asian_19           mask4\n",
       "18897  006959    male  Asian  19.0    006959_male_Asian_19           mask5\n",
       "18898  006959    male  Asian  19.0    006959_male_Asian_19  incorrect_mask\n",
       "18899  006959    male  Asian  19.0    006959_male_Asian_19          normal\n",
       "\n",
       "[18900 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = ['mask1', 'mask2', 'mask3', 'mask4', 'mask5', 'incorrect_mask', 'normal']\n",
    "wears = ['Wear', 'Wear', 'Wear', 'Wear', 'Wear', 'Incorrect', 'Not Wear']\n",
    "mask_df = pd.DataFrame()\n",
    "for person in train_df.values:\n",
    "    for mask, wear in zip(masks, wears):\n",
    "        mask_df = mask_df.append(pd.Series(np.append(person, (mask, wear))), ignore_index=True)\n",
    "mask_df.columns = np.append(train_df.columns.values, ('mask', 'wear'))\n",
    "mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53e94ec1-2d04-4170-8847-ca7baff2a987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001840-1</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>22.0</td>\n",
       "      <td>001840-1_male_Asian_22</td>\n",
       "      <td>mask5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001309</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>23.0</td>\n",
       "      <td>001309_male_Asian_23</td>\n",
       "      <td>mask5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005404</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>25.0</td>\n",
       "      <td>005404_female_Asian_25</td>\n",
       "      <td>mask3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001762</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>40.0</td>\n",
       "      <td>001762_male_Asian_40</td>\n",
       "      <td>mask2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003188</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>003188_female_Asian_19</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>001555</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59.0</td>\n",
       "      <td>001555_female_Asian_59</td>\n",
       "      <td>mask5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>004390</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>51.0</td>\n",
       "      <td>004390_female_Asian_51</td>\n",
       "      <td>mask1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>005255</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>005255_male_Asian_19</td>\n",
       "      <td>mask4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>001100</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>43.0</td>\n",
       "      <td>001100_female_Asian_43</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>006491</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>18.0</td>\n",
       "      <td>006491_female_Asian_18</td>\n",
       "      <td>mask2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  gender   race   age                    path    mask\n",
       "0      001840-1    male  Asian  22.0  001840-1_male_Asian_22   mask5\n",
       "1        001309    male  Asian  23.0    001309_male_Asian_23   mask5\n",
       "2        005404  female  Asian  25.0  005404_female_Asian_25   mask3\n",
       "3        001762    male  Asian  40.0    001762_male_Asian_40   mask2\n",
       "4        003188  female  Asian  19.0  003188_female_Asian_19  normal\n",
       "...         ...     ...    ...   ...                     ...     ...\n",
       "18895    001555  female  Asian  59.0  001555_female_Asian_59   mask5\n",
       "18896    004390  female  Asian  51.0  004390_female_Asian_51   mask1\n",
       "18897    005255    male  Asian  19.0    005255_male_Asian_19   mask4\n",
       "18898    001100  female  Asian  43.0  001100_female_Asian_43  normal\n",
       "18899    006491  female  Asian  18.0  006491_female_Asian_18   mask2\n",
       "\n",
       "[18900 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_df = mask_df.sample(frac=1).reset_index(drop=True)\n",
    "mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac641793-bbb2-4913-9f5f-802eab0014e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set dim : (15120, 6)\n",
      "Valid Set dim : (3780, 6)\n"
     ]
    }
   ],
   "source": [
    "train, valid = train_test_split(mask_df, test_size=0.2, stratify=mask_df['wear'])\n",
    "print(f'Train Set dim : (%d, %d)' % (train.shape))\n",
    "print(f'Valid Set dim : (%d, %d)' % (valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c56debf3-50ef-4d40-8c80-831cfc855dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((224, 224), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87fa1617-a096-49f4-84e0-df0c17e2a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderDataset(Dataset):\n",
    "    def __init__(self, path, mask_df, transform):\n",
    "        super(GenderDataset).__init__()\n",
    "        self.path = path\n",
    "        self.mask_df = mask_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        full_path = os.path.join(self.path, self.mask_df.iloc[idx]['path'])\n",
    "        img_list = glob.glob(full_path + '/*')\n",
    "        file_name = self.mask_df.iloc[idx]['mask']\n",
    "        for img_name in img_list:\n",
    "            if img_name.startswith(file_name):\n",
    "                break\n",
    "        image = Image.open(os.path.join(full_path, img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.mask_df.iloc[idx]['gender']\n",
    "        label = 0 if label=='male' else 1\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea279a85-e1a8-496b-af9f-92d31ef9b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_train_data = GenderDataset(trainimage_dir, train, transform)\n",
    "gender_valid_data = GenderDataset(trainimage_dir, valid, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "363c8983-ac4f-4cf7-a022-85bc18477512",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_train = DataLoader(gender_train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "gender_valid = DataLoader(gender_valid_data, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23c20e3f-bd45-49b9-88aa-d666c3eb2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, path, mask_df, transform):\n",
    "        super(AgeDataset).__init__()\n",
    "        self.path = path\n",
    "        self.mask_df = mask_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        full_path = os.path.join(self.path, self.mask_df.iloc[idx]['path'])\n",
    "        img_list = glob.glob(full_path + '/*')\n",
    "        file_name = self.mask_df.iloc[idx]['mask']\n",
    "        for img_name in img_list:\n",
    "            if img_name.startswith(file_name):\n",
    "                break\n",
    "        image = Image.open(os.path.join(full_path, img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.mask_df.iloc[idx]['age']\n",
    "        if label >= 60.0:\n",
    "            label = 2\n",
    "        elif label >= 30.0:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89a53049-c2b8-4208-b771-fdf55f667c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_train_data = AgeDataset(trainimage_dir, train, transform)\n",
    "age_valid_data = AgeDataset(trainimage_dir, valid, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "828112b0-5d23-4026-85f6-c46899c3c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_train = DataLoader(age_train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "age_valid = DataLoader(age_valid_data, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df28340b-779c-4cb7-860b-f8d537c6e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, path, mask_df, transform):\n",
    "        super(MaskDataset).__init__()\n",
    "        self.path = path\n",
    "        self.mask_df = mask_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        full_path = os.path.join(self.path, self.mask_df.iloc[idx]['path'])\n",
    "        img_list = glob.glob(full_path + '/*')\n",
    "        file_name = self.mask_df.iloc[idx]['mask']\n",
    "        for img_name in img_list:\n",
    "            if img_name.startswith(file_name):\n",
    "                break\n",
    "        image = Image.open(os.path.join(full_path, img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.mask_df.iloc[idx]['mask']\n",
    "        if label.startswith('mask'):\n",
    "            label = 0\n",
    "        elif label.startswith('incorrect'):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 2\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24d518fa-091e-48bb-929e-51e38d7e39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train_data = MaskDataset(trainimage_dir, train, transform)\n",
    "mask_valid_data = MaskDataset(trainimage_dir, valid, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6f6cb31-33d5-4382-8967-919d6c03b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = DataLoader(mask_train_data, batch_size=32, shuffle=True)\n",
    "mask_valid = DataLoader(mask_valid_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "### (2) Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        super(TestDataset).__init__()\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6c8f3ab-f585-4dd8-a261-d7ec57eac464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4630f279-a6d1-4e5f-bafe-650e74586582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "testimage_dir = os.path.join(test_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e633ea7-6b01-4192-8b22-023ca2299de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(testimage_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((224, 224), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "test_dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73381a1b-a63b-4e79-97ee-f3a42e689588",
   "metadata": {},
   "source": [
    "## 2. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a8d0ea5-ecca-448b-a427-ab3786b42693",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=True, progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2d38d1f-51fd-45f5-97f4-2d2ceb669512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b57d0b7e-0bef-4df1-ad03-55604688cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = resnet50(pretrained=True, progress=False)\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b3e7b-3bf2-4bc2-ac3e-18d6ac5ce01c",
   "metadata": {},
   "source": [
    "### 마스크 착용 여부 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d75eed4b-a194-4cad-9298-4b082a193806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_model = MyModel(num_classes=3)\n",
    "for param in mask_model.parameters():\n",
    "    param.requires_grad = False # frozen\n",
    "for param in mask_model.model.fc.parameters():\n",
    "    param.requires_grad = True # 마지막 레이어 살리기\n",
    "mask_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f76b9-3e2d-481c-99bb-4e04ec100fd8",
   "metadata": {},
   "source": [
    "### 나이 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0219f0-4fad-49ec-8fe0-7a8507b09a23",
   "metadata": {},
   "source": [
    "### 성별 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e5e38-9b5a-428e-afcc-663285338a34",
   "metadata": {},
   "source": [
    "### 성별 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11bf0059-0e2a-499e-aab6-377e059877c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_model = MyModel(num_classes=2)\n",
    "for param in gender_model.parameters():\n",
    "    param.requires_grad = False # frozen\n",
    "for param in gender_model.model.fc.parameters():\n",
    "    param.requires_grad = True # 마지막 레이어 살리기\n",
    "gender_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d90fdc5c-c098-4f2e-97e2-62e0bb71993f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_model = MyModel(num_classes=3)\n",
    "for param in age_model.parameters():\n",
    "    param.requires_grad = False # frozen\n",
    "for param in age_model.model.fc.parameters():\n",
    "    param.requires_grad = True # 마지막 레이어 살리기\n",
    "age_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef342b-e96c-4169-93c5-455cc67abb17",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ffc9345-3bce-47e3-ac79-fe34046e753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0015\n",
    "betas = (0.9, 0.999)\n",
    "weight_decay = 1e-4\n",
    "T_max = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f016b-18b2-43a5-b956-4ca8abcf7d07",
   "metadata": {},
   "source": [
    "### 마스크 착용 여부 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23053052-b145-47d3-8db7-3033ff72dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  1.0621 | Classifier Accuracy 50.00\n",
      "Iteration  50 | Train Loss  1.1816 | Classifier Accuracy 50.00\n",
      "Iteration 100 | Train Loss  1.1725 | Classifier Accuracy 56.25\n",
      "Iteration 150 | Train Loss  0.7348 | Classifier Accuracy 78.12\n",
      "Iteration 200 | Train Loss  1.0063 | Classifier Accuracy 59.38\n",
      "Iteration 250 | Train Loss  1.1035 | Classifier Accuracy 56.25\n",
      "Iteration 300 | Train Loss  0.7957 | Classifier Accuracy 81.25\n",
      "Iteration 350 | Train Loss  1.0444 | Classifier Accuracy 65.62\n",
      "Iteration 400 | Train Loss  0.7279 | Classifier Accuracy 78.12\n",
      "Iteration 450 | Train Loss  0.6462 | Classifier Accuracy 84.38\n",
      "\n",
      "[Summary] Elapsed time : 3 m 21 s\n",
      "Train Loss Mean 0.8234 | Accuracy 71.29 \n",
      "Valid Loss Mean 0.8213 | Accuracy 70.90 \n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.7477 | Classifier Accuracy 75.00\n",
      "Iteration  50 | Train Loss  0.9708 | Classifier Accuracy 62.50\n",
      "Iteration 100 | Train Loss  0.7798 | Classifier Accuracy 75.00\n",
      "Iteration 150 | Train Loss  0.8133 | Classifier Accuracy 75.00\n",
      "Iteration 200 | Train Loss  0.9907 | Classifier Accuracy 65.62\n",
      "Iteration 250 | Train Loss  0.9241 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.9735 | Classifier Accuracy 65.62\n",
      "Iteration 350 | Train Loss  0.8145 | Classifier Accuracy 71.88\n",
      "Iteration 400 | Train Loss  0.6764 | Classifier Accuracy 78.12\n",
      "Iteration 450 | Train Loss  0.6271 | Classifier Accuracy 81.25\n",
      "\n",
      "[Summary] Elapsed time : 4 m 12 s\n",
      "Train Loss Mean 0.8395 | Accuracy 71.35 \n",
      "Valid Loss Mean 0.8580 | Accuracy 70.88 \n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.7685 | Classifier Accuracy 75.00\n",
      "Iteration  50 | Train Loss  0.8634 | Classifier Accuracy 68.75\n",
      "Iteration 100 | Train Loss  0.9528 | Classifier Accuracy 62.50\n",
      "Iteration 150 | Train Loss  0.7161 | Classifier Accuracy 78.12\n",
      "Iteration 200 | Train Loss  0.8548 | Classifier Accuracy 68.75\n",
      "Iteration 250 | Train Loss  0.8505 | Classifier Accuracy 68.75\n",
      "Iteration 300 | Train Loss  0.6201 | Classifier Accuracy 84.38\n",
      "Iteration 350 | Train Loss  0.9742 | Classifier Accuracy 65.62\n",
      "Iteration 400 | Train Loss  0.9416 | Classifier Accuracy 65.62\n",
      "Iteration 450 | Train Loss  1.2289 | Classifier Accuracy 56.25\n",
      "\n",
      "[Summary] Elapsed time : 4 m 12 s\n",
      "Train Loss Mean 0.8482 | Accuracy 71.21 \n",
      "Valid Loss Mean 0.8772 | Accuracy 70.22 \n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.8715 | Classifier Accuracy 68.75\n",
      "Iteration  50 | Train Loss  0.8721 | Classifier Accuracy 65.62\n",
      "Iteration 100 | Train Loss  0.6755 | Classifier Accuracy 78.12\n",
      "Iteration 150 | Train Loss  0.8423 | Classifier Accuracy 71.88\n",
      "Iteration 200 | Train Loss  1.0702 | Classifier Accuracy 28.12\n",
      "Iteration 250 | Train Loss  1.0277 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.9001 | Classifier Accuracy 68.75\n",
      "Iteration 350 | Train Loss  0.9296 | Classifier Accuracy 71.88\n",
      "Iteration 400 | Train Loss  0.4665 | Classifier Accuracy 87.50\n",
      "Iteration 450 | Train Loss  0.7906 | Classifier Accuracy 75.00\n",
      "\n",
      "[Summary] Elapsed time : 4 m 16 s\n",
      "Train Loss Mean 0.8672 | Accuracy 70.80 \n",
      "Valid Loss Mean 0.9205 | Accuracy 70.90 \n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.6136 | Classifier Accuracy 81.25\n",
      "Iteration  50 | Train Loss  0.9476 | Classifier Accuracy 68.75\n",
      "Iteration 100 | Train Loss  0.9912 | Classifier Accuracy 62.50\n",
      "Iteration 150 | Train Loss  0.7747 | Classifier Accuracy 75.00\n",
      "Iteration 200 | Train Loss  1.1406 | Classifier Accuracy 53.12\n",
      "Iteration 250 | Train Loss  0.9117 | Classifier Accuracy 68.75\n",
      "Iteration 300 | Train Loss  0.6138 | Classifier Accuracy 84.38\n",
      "Iteration 350 | Train Loss  0.8025 | Classifier Accuracy 71.88\n",
      "Iteration 400 | Train Loss  0.8750 | Classifier Accuracy 68.75\n",
      "Iteration 450 | Train Loss  0.8634 | Classifier Accuracy 68.75\n",
      "\n",
      "[Summary] Elapsed time : 4 m 10 s\n",
      "Train Loss Mean 0.8609 | Accuracy 70.70 \n",
      "Valid Loss Mean 0.8788 | Accuracy 70.90 \n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.6365 | Classifier Accuracy 81.25\n",
      "Iteration  50 | Train Loss  0.8387 | Classifier Accuracy 68.75\n",
      "Iteration 100 | Train Loss  0.9409 | Classifier Accuracy 62.50\n",
      "Iteration 150 | Train Loss  0.9898 | Classifier Accuracy 65.62\n",
      "Iteration 200 | Train Loss  0.7070 | Classifier Accuracy 78.12\n",
      "Iteration 250 | Train Loss  0.6620 | Classifier Accuracy 75.00\n",
      "Iteration 300 | Train Loss  0.7354 | Classifier Accuracy 78.12\n",
      "Iteration 350 | Train Loss  0.9571 | Classifier Accuracy 59.38\n",
      "Iteration 400 | Train Loss  0.9754 | Classifier Accuracy 62.50\n",
      "Iteration 450 | Train Loss  0.8894 | Classifier Accuracy 68.75\n",
      "\n",
      "[Summary] Elapsed time : 4 m 12 s\n",
      "Train Loss Mean 0.8546 | Accuracy 70.78 \n",
      "Valid Loss Mean 0.8498 | Accuracy 70.90 \n",
      "\n",
      "EARLY STOPPING!!\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.RAdam(mask_model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=T_max)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "\n",
    "    # train\n",
    "    mask_model.train()\n",
    "    for i, (images, targets) in enumerate(mask_train) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = mask_model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 50 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    mask_model.eval()\n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    for i, (images, targets) in enumerate(mask_valid) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = mask_model(images)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "\n",
    "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        mask_best_model = mask_model\n",
    "        path = './mask_model/'\n",
    "        torch.save(mask_best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    lr_sched.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601c94d-8113-4e75-a024-d027369d9df0",
   "metadata": {},
   "source": [
    "### 성별 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5215c74b-e926-4239-a572-cae4e8bd7278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  0.7551 | Classifier Accuracy 40.62\n",
      "Iteration  50 | Train Loss  0.5906 | Classifier Accuracy 62.50\n",
      "Iteration 100 | Train Loss  0.4163 | Classifier Accuracy 87.50\n",
      "Iteration 150 | Train Loss  0.2997 | Classifier Accuracy 90.62\n",
      "Iteration 200 | Train Loss  0.2705 | Classifier Accuracy 90.62\n",
      "Iteration 250 | Train Loss  0.1658 | Classifier Accuracy 96.88\n",
      "Iteration 300 | Train Loss  0.3838 | Classifier Accuracy 84.38\n",
      "Iteration 350 | Train Loss  0.2241 | Classifier Accuracy 90.62\n",
      "Iteration 400 | Train Loss  0.2420 | Classifier Accuracy 90.62\n",
      "Iteration 450 | Train Loss  0.2654 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 0 m 54 s\n",
      "Train Loss Mean 0.3526 | Accuracy 85.19 \n",
      "Valid Loss Mean 0.2117 | Accuracy 91.57 \n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.2019 | Classifier Accuracy 90.62\n",
      "Iteration  50 | Train Loss  0.2633 | Classifier Accuracy 90.62\n",
      "Iteration 100 | Train Loss  0.1898 | Classifier Accuracy 93.75\n",
      "Iteration 150 | Train Loss  0.2081 | Classifier Accuracy 90.62\n",
      "Iteration 200 | Train Loss  0.1133 | Classifier Accuracy 96.88\n",
      "Iteration 250 | Train Loss  0.2159 | Classifier Accuracy 90.62\n",
      "Iteration 300 | Train Loss  0.2583 | Classifier Accuracy 84.38\n",
      "Iteration 350 | Train Loss  0.2304 | Classifier Accuracy 87.50\n",
      "Iteration 400 | Train Loss  0.1655 | Classifier Accuracy 93.75\n",
      "Iteration 450 | Train Loss  0.2536 | Classifier Accuracy 87.50\n",
      "\n",
      "[Summary] Elapsed time : 1 m 7 s\n",
      "Train Loss Mean 0.2208 | Accuracy 91.34 \n",
      "Valid Loss Mean 0.1733 | Accuracy 92.86 \n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.1139 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1904 | Classifier Accuracy 90.62\n",
      "Iteration 100 | Train Loss  0.3174 | Classifier Accuracy 81.25\n",
      "Iteration 150 | Train Loss  0.1150 | Classifier Accuracy 96.88\n",
      "Iteration 200 | Train Loss  0.2101 | Classifier Accuracy 96.88\n",
      "Iteration 250 | Train Loss  0.2061 | Classifier Accuracy 90.62\n",
      "Iteration 300 | Train Loss  0.1850 | Classifier Accuracy 90.62\n",
      "Iteration 350 | Train Loss  0.2871 | Classifier Accuracy 84.38\n",
      "Iteration 400 | Train Loss  0.1573 | Classifier Accuracy 96.88\n",
      "Iteration 450 | Train Loss  0.1298 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 7 s\n",
      "Train Loss Mean 0.1932 | Accuracy 92.18 \n",
      "Valid Loss Mean 0.1659 | Accuracy 93.22 \n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.2135 | Classifier Accuracy 90.62\n",
      "Iteration  50 | Train Loss  0.1787 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.0507 | Classifier Accuracy 100.00\n",
      "Iteration 150 | Train Loss  0.3231 | Classifier Accuracy 84.38\n",
      "Iteration 200 | Train Loss  0.2820 | Classifier Accuracy 84.38\n",
      "Iteration 250 | Train Loss  0.4802 | Classifier Accuracy 81.25\n",
      "Iteration 300 | Train Loss  0.1044 | Classifier Accuracy 93.75\n",
      "Iteration 350 | Train Loss  0.0803 | Classifier Accuracy 96.88\n",
      "Iteration 400 | Train Loss  0.0616 | Classifier Accuracy 100.00\n",
      "Iteration 450 | Train Loss  0.1951 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 7 s\n",
      "Train Loss Mean 0.1791 | Accuracy 92.74 \n",
      "Valid Loss Mean 0.1622 | Accuracy 92.86 \n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.1488 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0752 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.0779 | Classifier Accuracy 100.00\n",
      "Iteration 150 | Train Loss  0.0440 | Classifier Accuracy 100.00\n",
      "Iteration 200 | Train Loss  0.1117 | Classifier Accuracy 93.75\n",
      "Iteration 250 | Train Loss  0.0803 | Classifier Accuracy 96.88\n",
      "Iteration 300 | Train Loss  0.1954 | Classifier Accuracy 96.88\n",
      "Iteration 350 | Train Loss  0.0924 | Classifier Accuracy 96.88\n",
      "Iteration 400 | Train Loss  0.0445 | Classifier Accuracy 100.00\n",
      "Iteration 450 | Train Loss  0.3402 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.1634 | Accuracy 93.21 \n",
      "Valid Loss Mean 0.1349 | Accuracy 94.54 \n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.1727 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1555 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.0848 | Classifier Accuracy 100.00\n",
      "Iteration 150 | Train Loss  0.5196 | Classifier Accuracy 78.12\n",
      "Iteration 200 | Train Loss  0.2784 | Classifier Accuracy 90.62\n",
      "Iteration 250 | Train Loss  0.1205 | Classifier Accuracy 93.75\n",
      "Iteration 300 | Train Loss  0.2459 | Classifier Accuracy 87.50\n",
      "Iteration 350 | Train Loss  0.3763 | Classifier Accuracy 78.12\n",
      "Iteration 400 | Train Loss  0.2591 | Classifier Accuracy 87.50\n",
      "Iteration 450 | Train Loss  0.0603 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 7 s\n",
      "Train Loss Mean 0.1664 | Accuracy 93.04 \n",
      "Valid Loss Mean 0.1431 | Accuracy 93.91 \n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.0736 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1163 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.2729 | Classifier Accuracy 90.62\n",
      "Iteration 150 | Train Loss  0.2104 | Classifier Accuracy 90.62\n",
      "Iteration 200 | Train Loss  0.0857 | Classifier Accuracy 96.88\n",
      "Iteration 250 | Train Loss  0.2966 | Classifier Accuracy 87.50\n",
      "Iteration 300 | Train Loss  0.2950 | Classifier Accuracy 93.75\n",
      "Iteration 350 | Train Loss  0.1219 | Classifier Accuracy 93.75\n",
      "Iteration 400 | Train Loss  0.1852 | Classifier Accuracy 90.62\n",
      "Iteration 450 | Train Loss  0.2505 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 8 s\n",
      "Train Loss Mean 0.1473 | Accuracy 93.97 \n",
      "Valid Loss Mean 0.1477 | Accuracy 93.17 \n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.1461 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1617 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.1271 | Classifier Accuracy 93.75\n",
      "Iteration 150 | Train Loss  0.0642 | Classifier Accuracy 96.88\n",
      "Iteration 200 | Train Loss  0.1098 | Classifier Accuracy 96.88\n",
      "Iteration 250 | Train Loss  0.2224 | Classifier Accuracy 90.62\n",
      "Iteration 300 | Train Loss  0.1607 | Classifier Accuracy 90.62\n",
      "Iteration 350 | Train Loss  0.0391 | Classifier Accuracy 100.00\n",
      "Iteration 400 | Train Loss  0.0821 | Classifier Accuracy 96.88\n",
      "Iteration 450 | Train Loss  0.2109 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 9 s\n",
      "Train Loss Mean 0.1462 | Accuracy 94.07 \n",
      "Valid Loss Mean 0.2671 | Accuracy 88.60 \n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.4739 | Classifier Accuracy 68.75\n",
      "Iteration  50 | Train Loss  0.1490 | Classifier Accuracy 90.62\n",
      "Iteration 100 | Train Loss  0.0687 | Classifier Accuracy 96.88\n",
      "Iteration 150 | Train Loss  0.0636 | Classifier Accuracy 96.88\n",
      "Iteration 200 | Train Loss  0.2382 | Classifier Accuracy 90.62\n",
      "Iteration 250 | Train Loss  0.0301 | Classifier Accuracy 100.00\n",
      "Iteration 300 | Train Loss  0.1218 | Classifier Accuracy 96.88\n",
      "Iteration 350 | Train Loss  0.3156 | Classifier Accuracy 87.50\n",
      "Iteration 400 | Train Loss  0.1341 | Classifier Accuracy 96.88\n",
      "Iteration 450 | Train Loss  0.0930 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 1 m 10 s\n",
      "Train Loss Mean 0.1308 | Accuracy 94.62 \n",
      "Valid Loss Mean 0.2193 | Accuracy 91.07 \n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.3165 | Classifier Accuracy 90.62\n",
      "Iteration  50 | Train Loss  0.1460 | Classifier Accuracy 90.62\n",
      "Iteration 100 | Train Loss  0.0886 | Classifier Accuracy 100.00\n",
      "Iteration 150 | Train Loss  0.0319 | Classifier Accuracy 100.00\n",
      "Iteration 200 | Train Loss  0.0489 | Classifier Accuracy 100.00\n",
      "Iteration 250 | Train Loss  0.1089 | Classifier Accuracy 93.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7fab6fc264c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tqdm/std.py\", line 1128, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py\", line 261, in close\n",
      "    self.sp(bar_style='danger')\n",
      "AttributeError: 'tqdm' object has no attribute 'sp'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x7fab6fc264c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tqdm/std.py\", line 1128, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py\", line 261, in close\n",
      "    self.sp(bar_style='danger')\n",
      "AttributeError: 'tqdm' object has no attribute 'sp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300 | Train Loss  0.1162 | Classifier Accuracy 93.75\n",
      "Iteration 350 | Train Loss  0.1497 | Classifier Accuracy 93.75\n",
      "Iteration 400 | Train Loss  0.1619 | Classifier Accuracy 90.62\n",
      "Iteration 450 | Train Loss  0.1533 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 10 s\n",
      "Train Loss Mean 0.1409 | Accuracy 94.18 \n",
      "Valid Loss Mean 0.1263 | Accuracy 94.17 \n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.1489 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1455 | Classifier Accuracy 90.62\n",
      "Iteration 100 | Train Loss  0.0879 | Classifier Accuracy 96.88\n",
      "Iteration 150 | Train Loss  0.0705 | Classifier Accuracy 93.75\n",
      "Iteration 200 | Train Loss  0.0175 | Classifier Accuracy 100.00\n",
      "Iteration 250 | Train Loss  0.1181 | Classifier Accuracy 93.75\n",
      "Iteration 300 | Train Loss  0.2891 | Classifier Accuracy 87.50\n",
      "Iteration 350 | Train Loss  0.0437 | Classifier Accuracy 100.00\n",
      "Iteration 400 | Train Loss  0.1214 | Classifier Accuracy 93.75\n",
      "Iteration 450 | Train Loss  0.0429 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 10 s\n",
      "Train Loss Mean 0.1328 | Accuracy 94.44 \n",
      "Valid Loss Mean 0.1301 | Accuracy 94.28 \n",
      "\n",
      " ====================== epoch 12 ======================\n",
      "Iteration   0 | Train Loss  0.1417 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.0410 | Classifier Accuracy 100.00\n",
      "Iteration 100 | Train Loss  0.0679 | Classifier Accuracy 96.88\n",
      "Iteration 150 | Train Loss  0.1387 | Classifier Accuracy 93.75\n",
      "Iteration 200 | Train Loss  0.5292 | Classifier Accuracy 78.12\n",
      "Iteration 250 | Train Loss  0.0294 | Classifier Accuracy 100.00\n",
      "Iteration 300 | Train Loss  0.1424 | Classifier Accuracy 93.75\n",
      "Iteration 350 | Train Loss  0.1491 | Classifier Accuracy 90.62\n",
      "Iteration 400 | Train Loss  0.0698 | Classifier Accuracy 96.88\n",
      "Iteration 450 | Train Loss  0.2134 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 9 s\n",
      "Train Loss Mean 0.1300 | Accuracy 94.64 \n",
      "Valid Loss Mean 0.1031 | Accuracy 95.56 \n",
      "\n",
      " ====================== epoch 13 ======================\n",
      "Iteration   0 | Train Loss  0.1188 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.4420 | Classifier Accuracy 78.12\n",
      "Iteration 100 | Train Loss  0.0613 | Classifier Accuracy 96.88\n",
      "Iteration 150 | Train Loss  0.2502 | Classifier Accuracy 81.25\n",
      "Iteration 200 | Train Loss  0.0461 | Classifier Accuracy 100.00\n",
      "Iteration 250 | Train Loss  0.0995 | Classifier Accuracy 96.88\n",
      "Iteration 300 | Train Loss  0.0217 | Classifier Accuracy 100.00\n",
      "Iteration 350 | Train Loss  0.1175 | Classifier Accuracy 93.75\n",
      "Iteration 400 | Train Loss  0.0360 | Classifier Accuracy 100.00\n",
      "Iteration 450 | Train Loss  0.0213 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 1 m 10 s\n",
      "Train Loss Mean 0.1160 | Accuracy 95.33 \n",
      "Valid Loss Mean 0.1230 | Accuracy 94.75 \n",
      "\n",
      " ====================== epoch 14 ======================\n",
      "Iteration   0 | Train Loss  0.0808 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.1183 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.1818 | Classifier Accuracy 87.50\n",
      "Iteration 150 | Train Loss  0.0442 | Classifier Accuracy 100.00\n",
      "Iteration 200 | Train Loss  0.0502 | Classifier Accuracy 96.88\n",
      "Iteration 250 | Train Loss  0.1698 | Classifier Accuracy 93.75\n",
      "Iteration 300 | Train Loss  0.0644 | Classifier Accuracy 96.88\n",
      "Iteration 350 | Train Loss  0.1591 | Classifier Accuracy 93.75\n",
      "Iteration 400 | Train Loss  0.0459 | Classifier Accuracy 100.00\n",
      "Iteration 450 | Train Loss  0.0894 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 10 s\n",
      "Train Loss Mean 0.1169 | Accuracy 95.36 \n",
      "Valid Loss Mean 0.1354 | Accuracy 93.88 \n",
      "\n",
      " ====================== epoch 15 ======================\n",
      "Iteration   0 | Train Loss  0.1051 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1580 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.0456 | Classifier Accuracy 100.00\n",
      "Iteration 150 | Train Loss  0.0353 | Classifier Accuracy 96.88\n",
      "Iteration 200 | Train Loss  0.2512 | Classifier Accuracy 93.75\n",
      "Iteration 250 | Train Loss  0.0626 | Classifier Accuracy 96.88\n",
      "Iteration 300 | Train Loss  0.0300 | Classifier Accuracy 100.00\n",
      "Iteration 350 | Train Loss  0.0685 | Classifier Accuracy 96.88\n",
      "Iteration 400 | Train Loss  0.0559 | Classifier Accuracy 100.00\n",
      "Iteration 450 | Train Loss  0.0600 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 1 m 9 s\n",
      "Train Loss Mean 0.1121 | Accuracy 95.47 \n",
      "Valid Loss Mean 0.1173 | Accuracy 94.77 \n",
      "\n",
      " ====================== epoch 16 ======================\n",
      "Iteration   0 | Train Loss  0.0937 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.2121 | Classifier Accuracy 87.50\n",
      "Iteration 100 | Train Loss  0.0306 | Classifier Accuracy 100.00\n",
      "Iteration 150 | Train Loss  0.0305 | Classifier Accuracy 100.00\n",
      "Iteration 200 | Train Loss  0.1399 | Classifier Accuracy 96.88\n",
      "Iteration 250 | Train Loss  0.0285 | Classifier Accuracy 100.00\n",
      "Iteration 300 | Train Loss  0.0626 | Classifier Accuracy 96.88\n",
      "Iteration 350 | Train Loss  0.0236 | Classifier Accuracy 100.00\n",
      "Iteration 400 | Train Loss  0.0415 | Classifier Accuracy 96.88\n",
      "Iteration 450 | Train Loss  0.1316 | Classifier Accuracy 87.50\n",
      "\n",
      "[Summary] Elapsed time : 1 m 9 s\n",
      "Train Loss Mean 0.1127 | Accuracy 95.34 \n",
      "Valid Loss Mean 0.0968 | Accuracy 95.88 \n",
      "\n",
      " ====================== epoch 17 ======================\n",
      "Iteration   0 | Train Loss  0.0989 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1430 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.2897 | Classifier Accuracy 87.50\n",
      "Iteration 150 | Train Loss  0.0443 | Classifier Accuracy 100.00\n",
      "Iteration 200 | Train Loss  0.1072 | Classifier Accuracy 93.75\n",
      "Iteration 250 | Train Loss  0.0212 | Classifier Accuracy 100.00\n",
      "Iteration 300 | Train Loss  0.0303 | Classifier Accuracy 100.00\n",
      "Iteration 350 | Train Loss  0.0573 | Classifier Accuracy 96.88\n",
      "Iteration 400 | Train Loss  0.0193 | Classifier Accuracy 100.00\n",
      "Iteration 450 | Train Loss  0.1324 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 7 s\n",
      "Train Loss Mean 0.1026 | Accuracy 95.86 \n",
      "Valid Loss Mean 0.1001 | Accuracy 95.40 \n",
      "\n",
      " ====================== epoch 18 ======================\n",
      "Iteration   0 | Train Loss  0.1115 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1097 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.1069 | Classifier Accuracy 96.88\n",
      "Iteration 150 | Train Loss  0.0368 | Classifier Accuracy 100.00\n",
      "Iteration 200 | Train Loss  0.1497 | Classifier Accuracy 96.88\n",
      "Iteration 250 | Train Loss  0.1394 | Classifier Accuracy 93.75\n",
      "Iteration 300 | Train Loss  0.2951 | Classifier Accuracy 87.50\n",
      "Iteration 350 | Train Loss  0.0181 | Classifier Accuracy 100.00\n",
      "Iteration 400 | Train Loss  0.1966 | Classifier Accuracy 93.75\n",
      "Iteration 450 | Train Loss  0.0718 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 7 s\n",
      "Train Loss Mean 0.1033 | Accuracy 95.89 \n",
      "Valid Loss Mean 0.0913 | Accuracy 95.90 \n",
      "\n",
      " ====================== epoch 19 ======================\n",
      "Iteration   0 | Train Loss  0.1063 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1030 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.0610 | Classifier Accuracy 100.00\n",
      "Iteration 150 | Train Loss  0.0761 | Classifier Accuracy 96.88\n",
      "Iteration 200 | Train Loss  0.0550 | Classifier Accuracy 100.00\n",
      "Iteration 250 | Train Loss  0.1231 | Classifier Accuracy 93.75\n",
      "Iteration 300 | Train Loss  0.1182 | Classifier Accuracy 93.75\n",
      "Iteration 350 | Train Loss  0.6250 | Classifier Accuracy 84.38\n",
      "Iteration 400 | Train Loss  0.0986 | Classifier Accuracy 93.75\n",
      "Iteration 450 | Train Loss  0.0600 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 1 m 7 s\n",
      "Train Loss Mean 0.0979 | Accuracy 96.12 \n",
      "Valid Loss Mean 0.1228 | Accuracy 95.33 \n",
      "\n",
      " ====================== epoch 20 ======================\n",
      "Iteration   0 | Train Loss  0.1242 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0934 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.0434 | Classifier Accuracy 100.00\n",
      "Iteration 150 | Train Loss  0.2894 | Classifier Accuracy 87.50\n",
      "Iteration 200 | Train Loss  0.4507 | Classifier Accuracy 81.25\n",
      "Iteration 250 | Train Loss  0.1235 | Classifier Accuracy 93.75\n",
      "Iteration 300 | Train Loss  0.0385 | Classifier Accuracy 100.00\n",
      "Iteration 350 | Train Loss  0.1947 | Classifier Accuracy 93.75\n",
      "Iteration 400 | Train Loss  0.0768 | Classifier Accuracy 100.00\n",
      "Iteration 450 | Train Loss  0.0947 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.1034 | Accuracy 95.76 \n",
      "Valid Loss Mean 0.0898 | Accuracy 96.11 \n",
      "\n",
      " ====================== epoch 21 ======================\n",
      "Iteration   0 | Train Loss  0.0425 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.1132 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.0097 | Classifier Accuracy 100.00\n",
      "Iteration 150 | Train Loss  0.1038 | Classifier Accuracy 93.75\n",
      "Iteration 200 | Train Loss  0.2036 | Classifier Accuracy 90.62\n",
      "Iteration 250 | Train Loss  0.1143 | Classifier Accuracy 90.62\n",
      "Iteration 300 | Train Loss  0.0454 | Classifier Accuracy 100.00\n",
      "Iteration 350 | Train Loss  0.0354 | Classifier Accuracy 100.00\n",
      "Iteration 400 | Train Loss  0.0995 | Classifier Accuracy 93.75\n",
      "Iteration 450 | Train Loss  0.1337 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.0926 | Accuracy 96.34 \n",
      "Valid Loss Mean 0.1350 | Accuracy 93.59 \n",
      "\n",
      " ====================== epoch 22 ======================\n",
      "Iteration   0 | Train Loss  0.0713 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.3145 | Classifier Accuracy 81.25\n",
      "Iteration 100 | Train Loss  0.0701 | Classifier Accuracy 96.88\n",
      "Iteration 150 | Train Loss  0.0102 | Classifier Accuracy 100.00\n",
      "Iteration 200 | Train Loss  0.0732 | Classifier Accuracy 96.88\n",
      "Iteration 250 | Train Loss  0.0380 | Classifier Accuracy 100.00\n",
      "Iteration 300 | Train Loss  0.2021 | Classifier Accuracy 90.62\n",
      "Iteration 350 | Train Loss  0.0630 | Classifier Accuracy 96.88\n",
      "Iteration 400 | Train Loss  0.0524 | Classifier Accuracy 100.00\n",
      "Iteration 450 | Train Loss  0.2973 | Classifier Accuracy 81.25\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.0906 | Accuracy 96.35 \n",
      "Valid Loss Mean 0.0836 | Accuracy 96.27 \n",
      "\n",
      " ====================== epoch 23 ======================\n",
      "Iteration   0 | Train Loss  0.0137 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0771 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.0758 | Classifier Accuracy 96.88\n",
      "Iteration 150 | Train Loss  0.1098 | Classifier Accuracy 93.75\n",
      "Iteration 200 | Train Loss  0.0167 | Classifier Accuracy 100.00\n",
      "Iteration 250 | Train Loss  0.0586 | Classifier Accuracy 96.88\n",
      "Iteration 300 | Train Loss  0.1491 | Classifier Accuracy 90.62\n",
      "Iteration 350 | Train Loss  0.0924 | Classifier Accuracy 96.88\n",
      "Iteration 400 | Train Loss  0.2761 | Classifier Accuracy 84.38\n",
      "Iteration 450 | Train Loss  0.1389 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.0890 | Accuracy 96.67 \n",
      "Valid Loss Mean 0.1130 | Accuracy 94.54 \n",
      "\n",
      " ====================== epoch 24 ======================\n",
      "Iteration   0 | Train Loss  0.0605 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0442 | Classifier Accuracy 100.00\n",
      "Iteration 100 | Train Loss  0.0791 | Classifier Accuracy 96.88\n",
      "Iteration 150 | Train Loss  0.1273 | Classifier Accuracy 96.88\n",
      "Iteration 200 | Train Loss  0.3080 | Classifier Accuracy 87.50\n",
      "Iteration 250 | Train Loss  0.0957 | Classifier Accuracy 96.88\n",
      "Iteration 300 | Train Loss  0.0827 | Classifier Accuracy 96.88\n",
      "Iteration 350 | Train Loss  0.0663 | Classifier Accuracy 96.88\n",
      "Iteration 400 | Train Loss  0.1460 | Classifier Accuracy 93.75\n",
      "Iteration 450 | Train Loss  0.1296 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.0877 | Accuracy 96.56 \n",
      "Valid Loss Mean 0.0821 | Accuracy 96.77 \n",
      "\n",
      " ====================== epoch 25 ======================\n",
      "Iteration   0 | Train Loss  0.0704 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1323 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.1258 | Classifier Accuracy 93.75\n",
      "Iteration 150 | Train Loss  0.0188 | Classifier Accuracy 100.00\n",
      "Iteration 200 | Train Loss  0.0612 | Classifier Accuracy 100.00\n",
      "Iteration 250 | Train Loss  0.0659 | Classifier Accuracy 96.88\n",
      "Iteration 300 | Train Loss  0.0306 | Classifier Accuracy 100.00\n",
      "Iteration 350 | Train Loss  0.0457 | Classifier Accuracy 100.00\n",
      "Iteration 400 | Train Loss  0.0735 | Classifier Accuracy 93.75\n",
      "Iteration 450 | Train Loss  0.0393 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.0942 | Accuracy 96.20 \n",
      "Valid Loss Mean 0.0754 | Accuracy 96.59 \n",
      "\n",
      " ====================== epoch 26 ======================\n",
      "Iteration   0 | Train Loss  0.1091 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.0376 | Classifier Accuracy 100.00\n",
      "Iteration 100 | Train Loss  0.0560 | Classifier Accuracy 96.88\n",
      "Iteration 150 | Train Loss  0.2419 | Classifier Accuracy 96.88\n",
      "Iteration 200 | Train Loss  0.1247 | Classifier Accuracy 93.75\n",
      "Iteration 250 | Train Loss  0.0574 | Classifier Accuracy 96.88\n",
      "Iteration 300 | Train Loss  0.0438 | Classifier Accuracy 100.00\n",
      "Iteration 350 | Train Loss  0.0958 | Classifier Accuracy 93.75\n",
      "Iteration 400 | Train Loss  0.2002 | Classifier Accuracy 90.62\n",
      "Iteration 450 | Train Loss  0.0402 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 1 m 7 s\n",
      "Train Loss Mean 0.0830 | Accuracy 96.73 \n",
      "Valid Loss Mean 0.0735 | Accuracy 96.61 \n",
      "\n",
      " ====================== epoch 27 ======================\n",
      "Iteration   0 | Train Loss  0.0486 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1086 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.1088 | Classifier Accuracy 93.75\n",
      "Iteration 150 | Train Loss  0.0290 | Classifier Accuracy 100.00\n",
      "Iteration 200 | Train Loss  0.0556 | Classifier Accuracy 96.88\n",
      "Iteration 250 | Train Loss  0.0817 | Classifier Accuracy 96.88\n",
      "Iteration 300 | Train Loss  0.1027 | Classifier Accuracy 96.88\n",
      "Iteration 350 | Train Loss  0.0674 | Classifier Accuracy 96.88\n",
      "Iteration 400 | Train Loss  0.1471 | Classifier Accuracy 90.62\n",
      "Iteration 450 | Train Loss  0.1201 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 5 s\n",
      "Train Loss Mean 0.0777 | Accuracy 97.00 \n",
      "Valid Loss Mean 0.0782 | Accuracy 96.77 \n",
      "\n",
      " ====================== epoch 28 ======================\n",
      "Iteration   0 | Train Loss  0.0673 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.2970 | Classifier Accuracy 84.38\n",
      "Iteration 100 | Train Loss  0.0519 | Classifier Accuracy 100.00\n",
      "Iteration 150 | Train Loss  0.0497 | Classifier Accuracy 100.00\n",
      "Iteration 200 | Train Loss  0.0611 | Classifier Accuracy 100.00\n",
      "Iteration 250 | Train Loss  0.1338 | Classifier Accuracy 93.75\n",
      "Iteration 300 | Train Loss  0.2023 | Classifier Accuracy 93.75\n",
      "Iteration 350 | Train Loss  0.0324 | Classifier Accuracy 100.00\n",
      "Iteration 400 | Train Loss  0.0729 | Classifier Accuracy 96.88\n",
      "Iteration 450 | Train Loss  0.0887 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 5 s\n",
      "Train Loss Mean 0.0761 | Accuracy 97.11 \n",
      "Valid Loss Mean 0.0760 | Accuracy 96.40 \n",
      "\n",
      " ====================== epoch 29 ======================\n",
      "Iteration   0 | Train Loss  0.0625 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0204 | Classifier Accuracy 100.00\n",
      "Iteration 100 | Train Loss  0.0698 | Classifier Accuracy 96.88\n",
      "Iteration 150 | Train Loss  0.0583 | Classifier Accuracy 100.00\n",
      "Iteration 200 | Train Loss  0.0617 | Classifier Accuracy 100.00\n",
      "Iteration 250 | Train Loss  0.0413 | Classifier Accuracy 100.00\n",
      "Iteration 300 | Train Loss  0.1380 | Classifier Accuracy 90.62\n",
      "Iteration 350 | Train Loss  0.0476 | Classifier Accuracy 100.00\n",
      "Iteration 400 | Train Loss  0.0185 | Classifier Accuracy 100.00\n",
      "Iteration 450 | Train Loss  0.0865 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.0785 | Accuracy 96.97 \n",
      "Valid Loss Mean 0.0892 | Accuracy 95.61 \n",
      "\n",
      " ====================== epoch 30 ======================\n",
      "Iteration   0 | Train Loss  0.0898 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1242 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.1131 | Classifier Accuracy 90.62\n",
      "Iteration 150 | Train Loss  0.0256 | Classifier Accuracy 100.00\n",
      "Iteration 200 | Train Loss  0.2660 | Classifier Accuracy 87.50\n",
      "Iteration 250 | Train Loss  0.0535 | Classifier Accuracy 100.00\n",
      "Iteration 300 | Train Loss  0.0389 | Classifier Accuracy 100.00\n",
      "Iteration 350 | Train Loss  0.0487 | Classifier Accuracy 96.88\n",
      "Iteration 400 | Train Loss  0.0343 | Classifier Accuracy 100.00\n",
      "Iteration 450 | Train Loss  0.0602 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.0772 | Accuracy 96.95 \n",
      "Valid Loss Mean 0.0738 | Accuracy 96.74 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.RAdam(gender_model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=T_max)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "\n",
    "    # train\n",
    "    gender_model.train()\n",
    "    for i, (images, targets) in enumerate(gender_train) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = gender_model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 50 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    gender_model.eval()\n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    for i, (images, targets) in enumerate(gender_valid) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = gender_model(images)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "\n",
    "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        gender_best_model = gender_model\n",
    "        path = './gender_model/'\n",
    "        torch.save(gender_best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    lr_sched.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a1662-4a6a-4088-b3e7-b0557ba6408e",
   "metadata": {},
   "source": [
    "### 나이 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec5a4a4b-51b4-4817-8bd9-b89ea55ac896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  1.3526 | Classifier Accuracy 12.50\n",
      "Iteration  50 | Train Loss  0.6562 | Classifier Accuracy 81.25\n",
      "Iteration 100 | Train Loss  0.5637 | Classifier Accuracy 81.25\n",
      "Iteration 150 | Train Loss  0.3235 | Classifier Accuracy 96.88\n",
      "Iteration 200 | Train Loss  0.4422 | Classifier Accuracy 81.25\n",
      "Iteration 250 | Train Loss  0.3524 | Classifier Accuracy 84.38\n",
      "Iteration 300 | Train Loss  0.4074 | Classifier Accuracy 87.50\n",
      "Iteration 350 | Train Loss  0.4893 | Classifier Accuracy 78.12\n",
      "Iteration 400 | Train Loss  0.4263 | Classifier Accuracy 78.12\n",
      "Iteration 450 | Train Loss  0.3068 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 0 m 52 s\n",
      "Train Loss Mean 0.5144 | Accuracy 80.71 \n",
      "Valid Loss Mean 0.3538 | Accuracy 86.87 \n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.3588 | Classifier Accuracy 81.25\n",
      "Iteration  50 | Train Loss  0.3165 | Classifier Accuracy 87.50\n",
      "Iteration 100 | Train Loss  0.3980 | Classifier Accuracy 87.50\n",
      "Iteration 150 | Train Loss  0.3715 | Classifier Accuracy 84.38\n",
      "Iteration 200 | Train Loss  0.2818 | Classifier Accuracy 87.50\n",
      "Iteration 250 | Train Loss  0.2972 | Classifier Accuracy 87.50\n",
      "Iteration 300 | Train Loss  0.2537 | Classifier Accuracy 93.75\n",
      "Iteration 350 | Train Loss  0.3920 | Classifier Accuracy 87.50\n",
      "Iteration 400 | Train Loss  0.3492 | Classifier Accuracy 84.38\n",
      "Iteration 450 | Train Loss  0.1333 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.3424 | Accuracy 86.79 \n",
      "Valid Loss Mean 0.2780 | Accuracy 89.65 \n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.2015 | Classifier Accuracy 90.62\n",
      "Iteration  50 | Train Loss  0.3014 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.1716 | Classifier Accuracy 93.75\n",
      "Iteration 150 | Train Loss  0.3203 | Classifier Accuracy 87.50\n",
      "Iteration 200 | Train Loss  0.2196 | Classifier Accuracy 90.62\n",
      "Iteration 250 | Train Loss  0.1147 | Classifier Accuracy 100.00\n",
      "Iteration 300 | Train Loss  0.3321 | Classifier Accuracy 84.38\n",
      "Iteration 350 | Train Loss  0.2090 | Classifier Accuracy 90.62\n",
      "Iteration 400 | Train Loss  0.4127 | Classifier Accuracy 84.38\n",
      "Iteration 450 | Train Loss  0.2238 | Classifier Accuracy 87.50\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.2978 | Accuracy 88.53 \n",
      "Valid Loss Mean 0.2457 | Accuracy 90.20 \n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.2109 | Classifier Accuracy 90.62\n",
      "Iteration  50 | Train Loss  0.5547 | Classifier Accuracy 78.12\n",
      "Iteration 100 | Train Loss  0.1626 | Classifier Accuracy 93.75\n",
      "Iteration 150 | Train Loss  0.2508 | Classifier Accuracy 84.38\n",
      "Iteration 200 | Train Loss  0.2724 | Classifier Accuracy 90.62\n",
      "Iteration 250 | Train Loss  0.1976 | Classifier Accuracy 93.75\n",
      "Iteration 300 | Train Loss  0.3801 | Classifier Accuracy 81.25\n",
      "Iteration 350 | Train Loss  0.2775 | Classifier Accuracy 87.50\n",
      "Iteration 400 | Train Loss  0.4307 | Classifier Accuracy 71.88\n",
      "Iteration 450 | Train Loss  0.4685 | Classifier Accuracy 81.25\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.2774 | Accuracy 89.26 \n",
      "Valid Loss Mean 0.2275 | Accuracy 91.07 \n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.0754 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.1334 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.2586 | Classifier Accuracy 90.62\n",
      "Iteration 150 | Train Loss  0.1873 | Classifier Accuracy 96.88\n",
      "Iteration 200 | Train Loss  0.2379 | Classifier Accuracy 87.50\n",
      "Iteration 250 | Train Loss  0.1289 | Classifier Accuracy 93.75\n",
      "Iteration 300 | Train Loss  0.3405 | Classifier Accuracy 87.50\n",
      "Iteration 350 | Train Loss  0.2576 | Classifier Accuracy 93.75\n",
      "Iteration 400 | Train Loss  0.2455 | Classifier Accuracy 90.62\n",
      "Iteration 450 | Train Loss  0.3102 | Classifier Accuracy 84.38\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.2542 | Accuracy 90.18 \n",
      "Valid Loss Mean 0.2443 | Accuracy 89.65 \n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.1979 | Classifier Accuracy 87.50\n",
      "Iteration  50 | Train Loss  0.1405 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.1942 | Classifier Accuracy 90.62\n",
      "Iteration 150 | Train Loss  0.2575 | Classifier Accuracy 87.50\n",
      "Iteration 200 | Train Loss  0.1917 | Classifier Accuracy 87.50\n",
      "Iteration 250 | Train Loss  0.1662 | Classifier Accuracy 96.88\n",
      "Iteration 300 | Train Loss  0.3233 | Classifier Accuracy 84.38\n",
      "Iteration 350 | Train Loss  0.3514 | Classifier Accuracy 87.50\n",
      "Iteration 400 | Train Loss  0.4449 | Classifier Accuracy 84.38\n",
      "Iteration 450 | Train Loss  0.1406 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.2387 | Accuracy 90.86 \n",
      "Valid Loss Mean 0.2648 | Accuracy 89.21 \n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.1617 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.2490 | Classifier Accuracy 90.62\n",
      "Iteration 100 | Train Loss  0.3772 | Classifier Accuracy 93.75\n",
      "Iteration 150 | Train Loss  0.3522 | Classifier Accuracy 87.50\n",
      "Iteration 200 | Train Loss  0.6275 | Classifier Accuracy 68.75\n",
      "Iteration 250 | Train Loss  0.1064 | Classifier Accuracy 96.88\n",
      "Iteration 300 | Train Loss  0.0490 | Classifier Accuracy 100.00\n",
      "Iteration 350 | Train Loss  0.1819 | Classifier Accuracy 96.88\n",
      "Iteration 400 | Train Loss  0.1464 | Classifier Accuracy 96.88\n",
      "Iteration 450 | Train Loss  0.3466 | Classifier Accuracy 78.12\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.2290 | Accuracy 90.98 \n",
      "Valid Loss Mean 0.2340 | Accuracy 90.41 \n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.3599 | Classifier Accuracy 87.50\n",
      "Iteration  50 | Train Loss  0.1331 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.2396 | Classifier Accuracy 84.38\n",
      "Iteration 150 | Train Loss  0.1812 | Classifier Accuracy 87.50\n",
      "Iteration 200 | Train Loss  0.2117 | Classifier Accuracy 93.75\n",
      "Iteration 250 | Train Loss  0.3113 | Classifier Accuracy 84.38\n",
      "Iteration 300 | Train Loss  0.1977 | Classifier Accuracy 90.62\n",
      "Iteration 350 | Train Loss  0.0802 | Classifier Accuracy 100.00\n",
      "Iteration 400 | Train Loss  0.2034 | Classifier Accuracy 90.62\n",
      "Iteration 450 | Train Loss  0.1561 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 5 s\n",
      "Train Loss Mean 0.2188 | Accuracy 91.64 \n",
      "Valid Loss Mean 0.1939 | Accuracy 92.46 \n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.2853 | Classifier Accuracy 90.62\n",
      "Iteration  50 | Train Loss  0.1255 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.1511 | Classifier Accuracy 96.88\n",
      "Iteration 150 | Train Loss  0.1835 | Classifier Accuracy 93.75\n",
      "Iteration 200 | Train Loss  0.2976 | Classifier Accuracy 84.38\n",
      "Iteration 250 | Train Loss  0.2201 | Classifier Accuracy 90.62\n",
      "Iteration 300 | Train Loss  0.1451 | Classifier Accuracy 93.75\n",
      "Iteration 350 | Train Loss  0.1728 | Classifier Accuracy 96.88\n",
      "Iteration 400 | Train Loss  0.2581 | Classifier Accuracy 87.50\n",
      "Iteration 450 | Train Loss  0.3252 | Classifier Accuracy 84.38\n",
      "\n",
      "[Summary] Elapsed time : 1 m 5 s\n",
      "Train Loss Mean 0.2084 | Accuracy 91.97 \n",
      "Valid Loss Mean 0.1906 | Accuracy 92.46 \n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.0810 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.1225 | Classifier Accuracy 100.00\n",
      "Iteration 100 | Train Loss  0.2023 | Classifier Accuracy 93.75\n",
      "Iteration 150 | Train Loss  0.2915 | Classifier Accuracy 84.38\n",
      "Iteration 200 | Train Loss  0.3913 | Classifier Accuracy 87.50\n",
      "Iteration 250 | Train Loss  0.1855 | Classifier Accuracy 96.88\n",
      "Iteration 300 | Train Loss  0.4077 | Classifier Accuracy 81.25\n",
      "Iteration 350 | Train Loss  0.4554 | Classifier Accuracy 75.00\n",
      "Iteration 400 | Train Loss  0.1785 | Classifier Accuracy 93.75\n",
      "Iteration 450 | Train Loss  0.2297 | Classifier Accuracy 87.50\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.2048 | Accuracy 92.00 \n",
      "Valid Loss Mean 0.1831 | Accuracy 92.23 \n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.0932 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1052 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.0614 | Classifier Accuracy 96.88\n",
      "Iteration 150 | Train Loss  0.1772 | Classifier Accuracy 93.75\n",
      "Iteration 200 | Train Loss  0.1717 | Classifier Accuracy 93.75\n",
      "Iteration 250 | Train Loss  0.1973 | Classifier Accuracy 90.62\n",
      "Iteration 300 | Train Loss  0.2837 | Classifier Accuracy 90.62\n",
      "Iteration 350 | Train Loss  0.2418 | Classifier Accuracy 90.62\n",
      "Iteration 400 | Train Loss  0.1206 | Classifier Accuracy 93.75\n",
      "Iteration 450 | Train Loss  0.2035 | Classifier Accuracy 87.50\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.1936 | Accuracy 92.44 \n",
      "Valid Loss Mean 0.1909 | Accuracy 91.73 \n",
      "\n",
      " ====================== epoch 12 ======================\n",
      "Iteration   0 | Train Loss  0.1849 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.0932 | Classifier Accuracy 100.00\n",
      "Iteration 100 | Train Loss  0.0788 | Classifier Accuracy 100.00\n",
      "Iteration 150 | Train Loss  0.2618 | Classifier Accuracy 84.38\n",
      "Iteration 200 | Train Loss  0.0295 | Classifier Accuracy 100.00\n",
      "Iteration 250 | Train Loss  0.5030 | Classifier Accuracy 75.00\n",
      "Iteration 300 | Train Loss  0.2194 | Classifier Accuracy 90.62\n",
      "Iteration 350 | Train Loss  0.1463 | Classifier Accuracy 96.88\n",
      "Iteration 400 | Train Loss  0.3261 | Classifier Accuracy 90.62\n",
      "Iteration 450 | Train Loss  0.2256 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.1922 | Accuracy 92.57 \n",
      "Valid Loss Mean 0.2449 | Accuracy 89.13 \n",
      "\n",
      " ====================== epoch 13 ======================\n",
      "Iteration   0 | Train Loss  0.2199 | Classifier Accuracy 90.62\n",
      "Iteration  50 | Train Loss  0.2079 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.0789 | Classifier Accuracy 100.00\n",
      "Iteration 150 | Train Loss  0.1103 | Classifier Accuracy 96.88\n",
      "Iteration 200 | Train Loss  0.1638 | Classifier Accuracy 93.75\n",
      "Iteration 250 | Train Loss  0.3634 | Classifier Accuracy 90.62\n",
      "Iteration 300 | Train Loss  0.1264 | Classifier Accuracy 93.75\n",
      "Iteration 350 | Train Loss  0.3623 | Classifier Accuracy 84.38\n",
      "Iteration 400 | Train Loss  0.2228 | Classifier Accuracy 90.62\n",
      "Iteration 450 | Train Loss  0.2007 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 5 s\n",
      "Train Loss Mean 0.1844 | Accuracy 92.99 \n",
      "Valid Loss Mean 0.2055 | Accuracy 91.65 \n",
      "\n",
      " ====================== epoch 14 ======================\n",
      "Iteration   0 | Train Loss  0.1103 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.3196 | Classifier Accuracy 87.50\n",
      "Iteration 100 | Train Loss  0.0904 | Classifier Accuracy 100.00\n",
      "Iteration 150 | Train Loss  0.2720 | Classifier Accuracy 87.50\n",
      "Iteration 200 | Train Loss  0.0991 | Classifier Accuracy 96.88\n",
      "Iteration 250 | Train Loss  0.0705 | Classifier Accuracy 100.00\n",
      "Iteration 300 | Train Loss  0.0627 | Classifier Accuracy 100.00\n",
      "Iteration 350 | Train Loss  0.1101 | Classifier Accuracy 93.75\n",
      "Iteration 400 | Train Loss  0.2137 | Classifier Accuracy 84.38\n",
      "Iteration 450 | Train Loss  0.4210 | Classifier Accuracy 81.25\n",
      "\n",
      "[Summary] Elapsed time : 1 m 6 s\n",
      "Train Loss Mean 0.1713 | Accuracy 93.48 \n",
      "Valid Loss Mean 0.1895 | Accuracy 92.04 \n",
      "\n",
      " ====================== epoch 15 ======================\n",
      "Iteration   0 | Train Loss  0.0743 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.2708 | Classifier Accuracy 90.62\n",
      "Iteration 100 | Train Loss  0.2366 | Classifier Accuracy 87.50\n",
      "Iteration 150 | Train Loss  0.1675 | Classifier Accuracy 93.75\n",
      "Iteration 200 | Train Loss  0.1626 | Classifier Accuracy 93.75\n",
      "Iteration 250 | Train Loss  0.2543 | Classifier Accuracy 90.62\n",
      "Iteration 300 | Train Loss  0.2198 | Classifier Accuracy 90.62\n",
      "Iteration 350 | Train Loss  0.1384 | Classifier Accuracy 96.88\n",
      "Iteration 400 | Train Loss  0.0749 | Classifier Accuracy 100.00\n",
      "Iteration 450 | Train Loss  0.0802 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 5 s\n",
      "Train Loss Mean 0.1633 | Accuracy 93.64 \n",
      "Valid Loss Mean 0.2324 | Accuracy 90.65 \n",
      "\n",
      "EARLY STOPPING!!\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.RAdam(age_model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=T_max)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "\n",
    "    # train\n",
    "    age_model.train()\n",
    "    for i, (images, targets) in enumerate(age_train) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = age_model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 50 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    age_model.eval()\n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    for i, (images, targets) in enumerate(age_valid) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = age_model(images)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "\n",
    "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        age_best_model = age_model\n",
    "        path = './age_model/'\n",
    "        torch.save(age_best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    lr_sched.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e82b1-b9c3-42f0-897e-19af9b7f215d",
   "metadata": {},
   "source": [
    "### 마스크 착용 여부 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_best_model.eval()\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "mask_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        scores = mask_best_model(images)\n",
    "        preds = scores.argmax(dim=-1)\n",
    "        mask_predictions.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f517ec71-7f0f-44df-8faf-1b7de4029309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mask_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "08602539-d228-4c17-a3f6-e7a8eac64759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 12599, 1: 1})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(mask_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bde4f-8daf-4786-9374-62fb86f45833",
   "metadata": {},
   "source": [
    "* 마스크는 아직도 문제다..문제.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81500c1-1e60-46ff-9e63-b8c05ff09bf3",
   "metadata": {},
   "source": [
    "### 성별 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e5551462-a408-483e-b040-fff46d83af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_best_model.eval()\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "gender_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        scores = gender_best_model(images)\n",
    "        preds = scores.argmax(dim=-1)\n",
    "        gender_predictions.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36a2b5a9-d291-4a1a-9468-eeac6e7caba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gender_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "450fc99a-50b5-46fd-9ddb-ac5bd7235e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 6210, 1: 6390})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(gender_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c8f75-3ea3-48f8-bf40-0691bfdb32ce",
   "metadata": {},
   "source": [
    "* 성별 비율이 비슷해졌다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7214cbc-cdc6-4eb2-be16-4746b793111e",
   "metadata": {},
   "source": [
    "### 나이 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fbdd678d-6aed-4052-9e1b-ebd8e00cc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_best_model.eval()\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "age_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        scores = age_best_model(images)\n",
    "        preds = scores.argmax(dim=-1)\n",
    "        age_predictions.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f127e450-98dd-48b1-8667-65097b3554b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(age_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f8bcef6b-f516-4db3-a91f-fd4dc4f567c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 7843, 2: 1856, 1: 2901})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(age_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4dd987-498a-49c5-baed-8803cf158953",
   "metadata": {},
   "source": [
    "* 나이도 60대 이상 예측이 아직 적긴하지만 나름 분포와 비슷하게 나온 것 같다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa2ad0-daf6-4755-b067-2f3942c85871",
   "metadata": {},
   "source": [
    "### 최종 클래스 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f1fb527-dd61-4841-b3e6-b97e527e5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "size = len(submission)\n",
    "class_map = np.array([[[0, 1, 2],\n",
    "                       [3, 4, 5]],\n",
    "                      [[6, 7, 8],\n",
    "                       [9, 10, 11]],\n",
    "                      [[12, 13, 14],\n",
    "                       [15, 16, 17]]])\n",
    "for idx in range(size):\n",
    "    i = mask_predictions[idx]\n",
    "    j = gender_predictions[idx]\n",
    "    k = age_predictions[idx]\n",
    "    all_predictions.append(class_map[i][j][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cefb99f9-5db8-4091-aba4-692a0781a219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cdc01aee-aee3-48cc-9c5d-6f9e4144f8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3743, 2: 1122, 3: 4100, 1: 1344, 4: 1557, 5: 733, 8: 1})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "verbal-sample",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission_baseline_pretrained.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0973a2e-0393-4ba7-b392-4b89e054eeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
