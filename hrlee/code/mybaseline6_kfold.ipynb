{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torchvision.models import resnet50, resnext50_32x4d, resnet18, resnext101_32x8d\n",
    "import timm\n",
    "import albumentations as A\n",
    "\n",
    "from torchsummary import summary\n",
    "import torch_optimizer as optim\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from catalyst.data import BalanceClassSampler\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0a315e-6d3c-4fae-8a56-26704e4a6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77238f9-9fa5-45fa-97c9-68cd3724be52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170d98a-18e5-4a7f-91eb-575469d09378",
   "metadata": {},
   "source": [
    "## 1. 데이터셋 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03128f-f827-4668-b213-8ae16d0f08f7",
   "metadata": {},
   "source": [
    "### (1) Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc6a210-a832-4bd7-b68f-466aa2761d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터셋 폴더 경로를 지정해주세요.\n",
    "train_dir = '/opt/ml/input/data/train'\n",
    "trainimage_dir = os.path.join(train_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c412b27-edf7-446e-b965-40a7ffc154d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "0     000001  female  Asian   45  000001_female_Asian_45\n",
       "1     000002  female  Asian   52  000002_female_Asian_52\n",
       "2     000004    male  Asian   54    000004_male_Asian_54\n",
       "3     000005  female  Asian   58  000005_female_Asian_58\n",
       "4     000006  female  Asian   59  000006_female_Asian_59\n",
       "...      ...     ...    ...  ...                     ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "train_df = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87297828-b7a3-421b-9812-59a0abed5f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>mask</th>\n",
       "      <th>wear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask1</td>\n",
       "      <td>Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask2</td>\n",
       "      <td>Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask3</td>\n",
       "      <td>Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask4</td>\n",
       "      <td>Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask5</td>\n",
       "      <td>Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>mask3</td>\n",
       "      <td>Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>mask4</td>\n",
       "      <td>Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>mask5</td>\n",
       "      <td>Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>incorrect_mask</td>\n",
       "      <td>Incorrect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>normal</td>\n",
       "      <td>Not Wear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  gender   race   age                    path            mask  \\\n",
       "0      000001  female  Asian  45.0  000001_female_Asian_45           mask1   \n",
       "1      000001  female  Asian  45.0  000001_female_Asian_45           mask2   \n",
       "2      000001  female  Asian  45.0  000001_female_Asian_45           mask3   \n",
       "3      000001  female  Asian  45.0  000001_female_Asian_45           mask4   \n",
       "4      000001  female  Asian  45.0  000001_female_Asian_45           mask5   \n",
       "...       ...     ...    ...   ...                     ...             ...   \n",
       "18895  006959    male  Asian  19.0    006959_male_Asian_19           mask3   \n",
       "18896  006959    male  Asian  19.0    006959_male_Asian_19           mask4   \n",
       "18897  006959    male  Asian  19.0    006959_male_Asian_19           mask5   \n",
       "18898  006959    male  Asian  19.0    006959_male_Asian_19  incorrect_mask   \n",
       "18899  006959    male  Asian  19.0    006959_male_Asian_19          normal   \n",
       "\n",
       "            wear  \n",
       "0           Wear  \n",
       "1           Wear  \n",
       "2           Wear  \n",
       "3           Wear  \n",
       "4           Wear  \n",
       "...          ...  \n",
       "18895       Wear  \n",
       "18896       Wear  \n",
       "18897       Wear  \n",
       "18898  Incorrect  \n",
       "18899   Not Wear  \n",
       "\n",
       "[18900 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = ['mask1', 'mask2', 'mask3', 'mask4', 'mask5', 'incorrect_mask', 'normal']\n",
    "wears = ['Wear', 'Wear', 'Wear', 'Wear', 'Wear', 'Incorrect', 'Not Wear']\n",
    "mask_df = pd.DataFrame()\n",
    "for person in train_df.values:\n",
    "    for mask, wear in zip(masks, wears):\n",
    "        mask_df = mask_df.append(pd.Series(np.append(person, (mask, wear))), ignore_index=True)\n",
    "mask_df.columns = np.append(train_df.columns.values, ('mask', 'wear'))\n",
    "mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e94ec1-2d04-4170-8847-ca7baff2a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask_df = mask_df.sample(frac=1).reset_index(drop=True)\n",
    "#mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5a057fc-08f4-45d7-9bca-2914fb413e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>mask</th>\n",
       "      <th>wear</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask1</td>\n",
       "      <td>Wear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask2</td>\n",
       "      <td>Wear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask3</td>\n",
       "      <td>Wear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask4</td>\n",
       "      <td>Wear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask5</td>\n",
       "      <td>Wear</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>mask3</td>\n",
       "      <td>Wear</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>mask4</td>\n",
       "      <td>Wear</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>mask5</td>\n",
       "      <td>Wear</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>incorrect_mask</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>normal</td>\n",
       "      <td>Not Wear</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  gender   race   age                    path            mask  \\\n",
       "0      000001  female  Asian  45.0  000001_female_Asian_45           mask1   \n",
       "1      000001  female  Asian  45.0  000001_female_Asian_45           mask2   \n",
       "2      000001  female  Asian  45.0  000001_female_Asian_45           mask3   \n",
       "3      000001  female  Asian  45.0  000001_female_Asian_45           mask4   \n",
       "4      000001  female  Asian  45.0  000001_female_Asian_45           mask5   \n",
       "...       ...     ...    ...   ...                     ...             ...   \n",
       "18895  006959    male  Asian  19.0    006959_male_Asian_19           mask3   \n",
       "18896  006959    male  Asian  19.0    006959_male_Asian_19           mask4   \n",
       "18897  006959    male  Asian  19.0    006959_male_Asian_19           mask5   \n",
       "18898  006959    male  Asian  19.0    006959_male_Asian_19  incorrect_mask   \n",
       "18899  006959    male  Asian  19.0    006959_male_Asian_19          normal   \n",
       "\n",
       "            wear  label  \n",
       "0           Wear      4  \n",
       "1           Wear      4  \n",
       "2           Wear      4  \n",
       "3           Wear      4  \n",
       "4           Wear      4  \n",
       "...          ...    ...  \n",
       "18895       Wear      0  \n",
       "18896       Wear      0  \n",
       "18897       Wear      0  \n",
       "18898  Incorrect      6  \n",
       "18899   Not Wear     12  \n",
       "\n",
       "[18900 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_df = pd.DataFrame()\n",
    "for idx, person in mask_df.iterrows():\n",
    "    gender = person['gender']\n",
    "    gender = 0 if gender=='male' else 1\n",
    "\n",
    "    age = person['age']\n",
    "    if age >= 60.0:\n",
    "        age = 2\n",
    "    elif age >= 30.0:\n",
    "        age = 1\n",
    "    else:\n",
    "        age = 0\n",
    "\n",
    "    mask = person['wear']\n",
    "    if mask == 'Wear':\n",
    "        mask = 0\n",
    "    elif mask == 'Incorrect':\n",
    "        mask = 1\n",
    "    else:\n",
    "        mask = 2\n",
    "\n",
    "    label = 6*mask + 3*gender + age\n",
    "    labeled_df = labeled_df.append(pd.Series(np.append(person, label)), ignore_index=True)\n",
    "labeled_df.columns = np.append(mask_df.columns.values, 'label')\n",
    "labeled_df = labeled_df.astype({'label': int})\n",
    "labeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c56debf3-50ef-4d40-8c80-831cfc855dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c639b38-03dd-487a-8e7f-61d538349675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, path, labeled_df, transform):\n",
    "        super(TrainDataset).__init__()\n",
    "        self.path = path\n",
    "        self.labeled_df = labeled_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        full_path = os.path.join(self.path, self.labeled_df.iloc[idx]['path'])\n",
    "        img_list = glob.glob(full_path + '/*')\n",
    "        file_name = self.labeled_df.iloc[idx]['mask']\n",
    "        try:\n",
    "            image = Image.open(os.path.join(full_path, file_name+'.jpg'))\n",
    "        except:\n",
    "            try:\n",
    "                image = Image.open(os.path.join(full_path, file_name+'.png'))\n",
    "            except:\n",
    "                image = Image.open(os.path.join(full_path, file_name+'.jpeg'))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labeled_df.iloc[idx]['label']\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labeled_df)\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.labeled_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc18ba-d777-47d7-92c4-12aeb62c9a7c",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ffc9345-3bce-47e3-ac79-fe34046e753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0015\n",
    "betas = (0.9, 0.999)\n",
    "weight_decay = 1e-4\n",
    "T_max = 50\n",
    "batch_size = 32\n",
    "weight = torch.tensor([1., 1., 2.] * 6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "062a0a93-08c3-4a5f-b6b8-9fd1bc8cccdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---------------------- Fold 1 --------------------------------------------\n",
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  4.8291 | Classifier Accuracy 0.00\n",
      "Iteration  50 | Train Loss  0.0929 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 12 s\n",
      "Train Loss Mean 1.0367 | Accuracy 71.97 | F1-Score 0.6765\n",
      "Valid Loss Mean 1.9128 | Accuracy 70.40 | F1-Score 0.5372\n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.1026 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0578 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0683 | Accuracy 97.61 | F1-Score 0.9673\n",
      "Valid Loss Mean 1.2965 | Accuracy 77.21 | F1-Score 0.5960\n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.0456 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0045 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0360 | Accuracy 98.48 | F1-Score 0.9790\n",
      "Valid Loss Mean 0.4710 | Accuracy 87.76 | F1-Score 0.6968\n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.0031 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.1612 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0565 | Accuracy 98.12 | F1-Score 0.9746\n",
      "Valid Loss Mean 0.8325 | Accuracy 77.36 | F1-Score 0.5409\n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.0460 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0009 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0231 | Accuracy 99.22 | F1-Score 0.9904\n",
      "Valid Loss Mean 0.4439 | Accuracy 89.23 | F1-Score 0.7632\n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.0417 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0553 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0499 | Accuracy 98.67 | F1-Score 0.9765\n",
      "Valid Loss Mean 2.5573 | Accuracy 87.61 | F1-Score 0.7468\n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.0006 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0021 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0669 | Accuracy 98.25 | F1-Score 0.9811\n",
      "Valid Loss Mean 0.6879 | Accuracy 82.06 | F1-Score 0.6473\n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.0345 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0056 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0618 | Accuracy 98.44 | F1-Score 0.9752\n",
      "Valid Loss Mean 0.7051 | Accuracy 84.61 | F1-Score 0.6100\n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.0124 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0264 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0404 | Accuracy 98.67 | F1-Score 0.9771\n",
      "Valid Loss Mean 0.4672 | Accuracy 90.23 | F1-Score 0.7609\n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.0012 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.1791 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0398 | Accuracy 98.94 | F1-Score 0.9850\n",
      "Valid Loss Mean 1.4472 | Accuracy 87.39 | F1-Score 0.7915\n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.0479 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1131 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.1065 | Accuracy 97.33 | F1-Score 0.9550\n",
      "Valid Loss Mean 0.5687 | Accuracy 86.74 | F1-Score 0.7315\n",
      "\n",
      " ====================== epoch 12 ======================\n",
      "Iteration   0 | Train Loss  0.0156 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0291 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0508 | Accuracy 98.90 | F1-Score 0.9859\n",
      "Valid Loss Mean 0.4790 | Accuracy 88.73 | F1-Score 0.7679\n",
      "\n",
      " ====================== epoch 13 ======================\n",
      "Iteration   0 | Train Loss  0.0057 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0006 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0193 | Accuracy 99.31 | F1-Score 0.9871\n",
      "Valid Loss Mean 0.3587 | Accuracy 92.65 | F1-Score 0.8126\n",
      "\n",
      " ====================== epoch 14 ======================\n",
      "Iteration   0 | Train Loss  0.0005 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0056 | Accuracy 99.77 | F1-Score 0.9976\n",
      "Valid Loss Mean 0.3824 | Accuracy 91.44 | F1-Score 0.8042\n",
      "\n",
      " ====================== epoch 15 ======================\n",
      "Iteration   0 | Train Loss  0.0002 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0047 | Accuracy 99.82 | F1-Score 0.9981\n",
      "Valid Loss Mean 0.3304 | Accuracy 94.85 | F1-Score 0.8496\n",
      "\n",
      " ====================== epoch 16 ======================\n",
      "Iteration   0 | Train Loss  0.0001 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0013 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0138 | Accuracy 99.63 | F1-Score 0.9938\n",
      "Valid Loss Mean 0.4097 | Accuracy 91.12 | F1-Score 0.7907\n",
      "\n",
      " ====================== epoch 17 ======================\n",
      "Iteration   0 | Train Loss  0.0010 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0012 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0006 | Accuracy 100.00 | F1-Score 1.0000\n",
      "Valid Loss Mean 0.3614 | Accuracy 93.17 | F1-Score 0.8256\n",
      "\n",
      " ====================== epoch 18 ======================\n",
      "Iteration   0 | Train Loss  0.0002 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0005 | Accuracy 100.00 | F1-Score 1.0000\n",
      "Valid Loss Mean 0.3860 | Accuracy 93.54 | F1-Score 0.8486\n",
      "\n",
      " ====================== epoch 19 ======================\n",
      "Iteration   0 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0020 | Accuracy 99.95 | F1-Score 0.9984\n",
      "Valid Loss Mean 0.3544 | Accuracy 93.67 | F1-Score 0.8506\n",
      "\n",
      " ====================== epoch 20 ======================\n",
      "Iteration   0 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0001 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0072 | Accuracy 99.91 | F1-Score 0.9984\n",
      "Valid Loss Mean 0.3212 | Accuracy 94.12 | F1-Score 0.8437\n",
      "\n",
      " ====================== epoch 21 ======================\n",
      "Iteration   0 | Train Loss  0.0042 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0009 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0084 | Accuracy 99.82 | F1-Score 0.9974\n",
      "Valid Loss Mean 0.3808 | Accuracy 92.75 | F1-Score 0.8351\n",
      "\n",
      " ====================== epoch 22 ======================\n",
      "Iteration   0 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0019 | Accuracy 99.95 | F1-Score 0.9993\n",
      "Valid Loss Mean 0.3773 | Accuracy 93.93 | F1-Score 0.8508\n",
      "\n",
      " ====================== epoch 23 ======================\n",
      "Iteration   0 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0005 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0093 | Accuracy 99.68 | F1-Score 0.9936\n",
      "Valid Loss Mean 0.3108 | Accuracy 92.96 | F1-Score 0.8350\n",
      "\n",
      " ====================== epoch 24 ======================\n",
      "Iteration   0 | Train Loss  0.0005 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0002 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0092 | Accuracy 99.82 | F1-Score 0.9973\n",
      "Valid Loss Mean 0.3225 | Accuracy 93.38 | F1-Score 0.8591\n",
      "\n",
      " ====================== epoch 25 ======================\n",
      "Iteration   0 | Train Loss  0.0001 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0001 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0048 | Accuracy 99.77 | F1-Score 0.9952\n",
      "Valid Loss Mean 0.3098 | Accuracy 93.93 | F1-Score 0.8464\n",
      "\n",
      " ====================== epoch 26 ======================\n",
      "Iteration   0 | Train Loss  0.0007 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0005 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0010 | Accuracy 100.00 | F1-Score 1.0000\n",
      "Valid Loss Mean 0.3422 | Accuracy 94.83 | F1-Score 0.8574\n",
      "\n",
      " ====================== epoch 27 ======================\n",
      "Iteration   0 | Train Loss  0.0002 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0010 | Accuracy 99.95 | F1-Score 0.9997\n",
      "Valid Loss Mean 0.3506 | Accuracy 93.91 | F1-Score 0.8726\n",
      "\n",
      " ====================== epoch 28 ======================\n",
      "Iteration   0 | Train Loss  0.0017 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0002 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0015 | Accuracy 99.91 | F1-Score 0.9991\n",
      "Valid Loss Mean 0.3564 | Accuracy 93.75 | F1-Score 0.8631\n",
      "\n",
      " ====================== epoch 29 ======================\n",
      "Iteration   0 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0002 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0034 | Accuracy 99.91 | F1-Score 0.9982\n",
      "Valid Loss Mean 0.3106 | Accuracy 95.77 | F1-Score 0.8883\n",
      "\n",
      " ====================== epoch 30 ======================\n",
      "Iteration   0 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0004 | Accuracy 100.00 | F1-Score 1.0000\n",
      "Valid Loss Mean 0.3198 | Accuracy 95.17 | F1-Score 0.8561\n",
      "\n",
      " ---------------------- Fold 2 --------------------------------------------\n",
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  3.6614 | Classifier Accuracy 9.38\n",
      "Iteration  50 | Train Loss  0.0585 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 12 s\n",
      "Train Loss Mean 0.9674 | Accuracy 72.98 | F1-Score 0.6864\n",
      "Valid Loss Mean 0.5826 | Accuracy 79.86 | F1-Score 0.6439\n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.1985 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1631 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0833 | Accuracy 97.38 | F1-Score 0.9702\n",
      "Valid Loss Mean 0.4015 | Accuracy 88.87 | F1-Score 0.7703\n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.0844 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.0180 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0724 | Accuracy 97.70 | F1-Score 0.9709\n",
      "Valid Loss Mean 0.4208 | Accuracy 89.81 | F1-Score 0.7824\n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.1603 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.0568 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0419 | Accuracy 98.62 | F1-Score 0.9791\n",
      "Valid Loss Mean 0.4724 | Accuracy 88.89 | F1-Score 0.7710\n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.1047 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.0104 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0559 | Accuracy 98.35 | F1-Score 0.9754\n",
      "Valid Loss Mean 0.3166 | Accuracy 91.52 | F1-Score 0.7707\n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.0067 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0016 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0333 | Accuracy 98.81 | F1-Score 0.9805\n",
      "Valid Loss Mean 0.2342 | Accuracy 94.80 | F1-Score 0.8413\n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.0120 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.2677 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0499 | Accuracy 98.76 | F1-Score 0.9816\n",
      "Valid Loss Mean 0.3689 | Accuracy 93.57 | F1-Score 0.7906\n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.0256 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1266 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0621 | Accuracy 98.67 | F1-Score 0.9808\n",
      "Valid Loss Mean 0.4398 | Accuracy 88.55 | F1-Score 0.7073\n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.0372 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0236 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0884 | Accuracy 97.29 | F1-Score 0.9622\n",
      "Valid Loss Mean 0.3732 | Accuracy 93.12 | F1-Score 0.8098\n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.0026 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0032 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0280 | Accuracy 98.99 | F1-Score 0.9844\n",
      "Valid Loss Mean 0.3309 | Accuracy 93.70 | F1-Score 0.8246\n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.0011 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0001 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0226 | Accuracy 99.36 | F1-Score 0.9906\n",
      "Valid Loss Mean 0.6570 | Accuracy 85.53 | F1-Score 0.7214\n",
      "\n",
      "EARLY STOPPING!!\n",
      " ---------------------- Fold 3 --------------------------------------------\n",
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  3.4382 | Classifier Accuracy 12.50\n",
      "Iteration  50 | Train Loss  0.1784 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 0 m 12 s\n",
      "Train Loss Mean 0.8904 | Accuracy 73.71 | F1-Score 0.6870\n",
      "Valid Loss Mean 0.5650 | Accuracy 77.36 | F1-Score 0.6973\n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.1963 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.0332 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0601 | Accuracy 98.25 | F1-Score 0.9776\n",
      "Valid Loss Mean 0.4369 | Accuracy 88.45 | F1-Score 0.7772\n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.0852 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0394 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0479 | Accuracy 98.35 | F1-Score 0.9815\n",
      "Valid Loss Mean 0.2865 | Accuracy 89.92 | F1-Score 0.7922\n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.0141 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0171 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0275 | Accuracy 99.26 | F1-Score 0.9899\n",
      "Valid Loss Mean 0.4273 | Accuracy 89.13 | F1-Score 0.7999\n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.0056 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0100 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0276 | Accuracy 98.99 | F1-Score 0.9839\n",
      "Valid Loss Mean 0.6576 | Accuracy 87.79 | F1-Score 0.7446\n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.0899 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1156 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0697 | Accuracy 98.02 | F1-Score 0.9731\n",
      "Valid Loss Mean 0.4101 | Accuracy 89.63 | F1-Score 0.8057\n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.1700 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.0298 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0741 | Accuracy 97.75 | F1-Score 0.9645\n",
      "Valid Loss Mean 0.5003 | Accuracy 91.18 | F1-Score 0.8491\n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.1535 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0837 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0695 | Accuracy 98.25 | F1-Score 0.9751\n",
      "Valid Loss Mean 0.6494 | Accuracy 88.10 | F1-Score 0.7951\n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.0312 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0004 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0778 | Accuracy 97.89 | F1-Score 0.9704\n",
      "Valid Loss Mean 0.4810 | Accuracy 88.81 | F1-Score 0.8251\n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.0298 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.3065 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0679 | Accuracy 98.12 | F1-Score 0.9757\n",
      "Valid Loss Mean 0.4813 | Accuracy 89.08 | F1-Score 0.8033\n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.0196 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0024 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0449 | Accuracy 98.99 | F1-Score 0.9838\n",
      "Valid Loss Mean 0.4288 | Accuracy 89.97 | F1-Score 0.8326\n",
      "\n",
      " ====================== epoch 12 ======================\n",
      "Iteration   0 | Train Loss  0.0966 | Classifier Accuracy 90.62\n",
      "Iteration  50 | Train Loss  0.0069 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0469 | Accuracy 98.85 | F1-Score 0.9842\n",
      "Valid Loss Mean 0.8340 | Accuracy 89.26 | F1-Score 0.8240\n",
      "\n",
      "EARLY STOPPING!!\n",
      " ---------------------- Fold 4 --------------------------------------------\n",
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  4.0380 | Classifier Accuracy 3.12\n",
      "Iteration  50 | Train Loss  0.3877 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 0 m 12 s\n",
      "Train Loss Mean 1.1314 | Accuracy 69.85 | F1-Score 0.6470\n",
      "Valid Loss Mean 2.5900 | Accuracy 67.38 | F1-Score 0.5896\n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.0652 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0861 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0649 | Accuracy 98.02 | F1-Score 0.9742\n",
      "Valid Loss Mean 0.3645 | Accuracy 89.02 | F1-Score 0.7295\n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.0085 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0369 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 14 s\n",
      "Train Loss Mean 0.0306 | Accuracy 98.99 | F1-Score 0.9850\n",
      "Valid Loss Mean 0.4069 | Accuracy 91.91 | F1-Score 0.8083\n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.0028 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0178 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0391 | Accuracy 98.94 | F1-Score 0.9838\n",
      "Valid Loss Mean 4.1101 | Accuracy 86.19 | F1-Score 0.6961\n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.1001 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0550 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0622 | Accuracy 97.89 | F1-Score 0.9701\n",
      "Valid Loss Mean 0.6816 | Accuracy 90.55 | F1-Score 0.7250\n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.0018 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0225 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0400 | Accuracy 98.76 | F1-Score 0.9800\n",
      "Valid Loss Mean 8.1174 | Accuracy 94.04 | F1-Score 0.7819\n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.0028 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0164 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0473 | Accuracy 98.58 | F1-Score 0.9781\n",
      "Valid Loss Mean 2.5495 | Accuracy 92.99 | F1-Score 0.7914\n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.0061 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0028 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0741 | Accuracy 97.75 | F1-Score 0.9660\n",
      "Valid Loss Mean 0.4611 | Accuracy 91.05 | F1-Score 0.8011\n",
      "\n",
      "EARLY STOPPING!!\n",
      " ---------------------- Fold 5 --------------------------------------------\n",
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  4.5799 | Classifier Accuracy 0.00\n",
      "Iteration  50 | Train Loss  0.4795 | Classifier Accuracy 84.38\n",
      "\n",
      "[Summary] Elapsed time : 0 m 12 s\n",
      "Train Loss Mean 1.0023 | Accuracy 72.93 | F1-Score 0.6876\n",
      "Valid Loss Mean 0.6850 | Accuracy 82.41 | F1-Score 0.6056\n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.1583 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1059 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0782 | Accuracy 97.61 | F1-Score 0.9670\n",
      "Valid Loss Mean 0.7094 | Accuracy 84.82 | F1-Score 0.6410\n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.1113 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0041 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0565 | Accuracy 97.98 | F1-Score 0.9686\n",
      "Valid Loss Mean 0.6579 | Accuracy 86.08 | F1-Score 0.6644\n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.0110 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0064 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0528 | Accuracy 98.53 | F1-Score 0.9812\n",
      "Valid Loss Mean 0.6258 | Accuracy 87.26 | F1-Score 0.6669\n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.0291 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.2632 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0432 | Accuracy 98.85 | F1-Score 0.9797\n",
      "Valid Loss Mean 0.6225 | Accuracy 87.37 | F1-Score 0.6305\n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.0067 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0046 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0696 | Accuracy 97.89 | F1-Score 0.9738\n",
      "Valid Loss Mean 0.6359 | Accuracy 88.03 | F1-Score 0.6221\n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.1201 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.0071 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0815 | Accuracy 97.56 | F1-Score 0.9662\n",
      "Valid Loss Mean 0.8205 | Accuracy 84.53 | F1-Score 0.6299\n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.0049 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0062 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0447 | Accuracy 98.58 | F1-Score 0.9805\n",
      "Valid Loss Mean 0.6198 | Accuracy 87.61 | F1-Score 0.6631\n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.0969 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0021 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 14 s\n",
      "Train Loss Mean 0.0151 | Accuracy 99.59 | F1-Score 0.9951\n",
      "Valid Loss Mean 0.5788 | Accuracy 89.99 | F1-Score 0.7007\n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.0001 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0010 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0076 | Accuracy 99.82 | F1-Score 0.9969\n",
      "Valid Loss Mean 0.6394 | Accuracy 88.03 | F1-Score 0.7074\n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.0008 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0018 | Accuracy 99.86 | F1-Score 0.9985\n",
      "Valid Loss Mean 0.6169 | Accuracy 91.49 | F1-Score 0.7390\n",
      "\n",
      " ====================== epoch 12 ======================\n",
      "Iteration   0 | Train Loss  0.0000 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0029 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0083 | Accuracy 99.68 | F1-Score 0.9964\n",
      "Valid Loss Mean 0.6478 | Accuracy 92.17 | F1-Score 0.7474\n",
      "\n",
      " ====================== epoch 13 ======================\n",
      "Iteration   0 | Train Loss  0.0010 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0011 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0274 | Accuracy 99.22 | F1-Score 0.9894\n",
      "Valid Loss Mean 0.7906 | Accuracy 84.90 | F1-Score 0.6764\n",
      "\n",
      " ====================== epoch 14 ======================\n",
      "Iteration   0 | Train Loss  0.0043 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.1108 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.1358 | Accuracy 97.47 | F1-Score 0.9666\n",
      "Valid Loss Mean 1.1038 | Accuracy 81.67 | F1-Score 0.6202\n",
      "\n",
      " ====================== epoch 15 ======================\n",
      "Iteration   0 | Train Loss  0.0149 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0275 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0933 | Accuracy 97.70 | F1-Score 0.9667\n",
      "Valid Loss Mean 0.7918 | Accuracy 87.00 | F1-Score 0.7098\n",
      "\n",
      " ====================== epoch 16 ======================\n",
      "Iteration   0 | Train Loss  0.0096 | Classifier Accuracy 100.00\n",
      "Iteration  50 | Train Loss  0.0004 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0104 | Accuracy 99.72 | F1-Score 0.9963\n",
      "Valid Loss Mean 0.7862 | Accuracy 89.99 | F1-Score 0.6983\n",
      "\n",
      " ====================== epoch 17 ======================\n",
      "Iteration   0 | Train Loss  0.0257 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0150 | Classifier Accuracy 100.00\n",
      "\n",
      "[Summary] Elapsed time : 0 m 13 s\n",
      "Train Loss Mean 0.0179 | Accuracy 99.45 | F1-Score 0.9898\n",
      "Valid Loss Mean 0.7725 | Accuracy 89.73 | F1-Score 0.6743\n",
      "\n",
      "EARLY STOPPING!!\n"
     ]
    }
   ],
   "source": [
    "NUM_FINETUNE_CLASSES = 18\n",
    "\n",
    "num_epochs = 30\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "n_splits = 5\n",
    "kfold = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "best_models = {}\n",
    "fold_results = {}\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_df)):\n",
    "    print(f' ---------------------- Fold %d --------------------------------------------' % (fold+1) )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train = labeled_df.iloc[train_ids]\n",
    "    valid = labeled_df.iloc[test_ids]\n",
    "    \n",
    "    train_data = TrainDataset(trainimage_dir, train, transform)\n",
    "    valid_data = TrainDataset(trainimage_dir, valid, transform)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=4, sampler=ImbalancedDatasetSampler(train_data))\n",
    "    valid_loader = DataLoader(valid_data, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "\n",
    "    # Create model\n",
    "    model = timm.create_model('efficientnet_b3', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.RAdam(model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "    lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=T_max)\n",
    "\n",
    "    # early stopping\n",
    "    valid_early_stop = 0\n",
    "    valid_best_f1score = 0.0\n",
    "    since = time.time()\n",
    "\n",
    "    for e in range(num_epochs) :\n",
    "        print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        train_epoch_f1 = 0\n",
    "        n_iter = 0\n",
    "        train_loss_list = []\n",
    "        train_acc_list = []\n",
    "        for i, (images, targets) in enumerate(train_loader) : \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            scores = model(images)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "            loss = F.cross_entropy(scores, targets, weight=weight)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            correct = sum(targets == preds).cpu()\n",
    "            acc=(correct/images.shape[0] * 100)\n",
    "            train_epoch_f1 += f1_score(preds.cpu().numpy(), targets.cpu().numpy(), average='macro')\n",
    "            n_iter += 1\n",
    "\n",
    "            train_loss_list.append(loss)\n",
    "            train_acc_list.append(acc)\n",
    "\n",
    "            if i % 50 == 0 :\n",
    "                print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "        train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "        train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "        train_epoch_f1 = train_epoch_f1/n_iter\n",
    "\n",
    "        epoch_time = time.time() - since\n",
    "        since = time.time()\n",
    "\n",
    "        print('')\n",
    "        print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "        print(f'Train Loss Mean %.4f | Accuracy %2.2f | F1-Score %2.4f' % (train_mean_loss, train_mean_acc, train_epoch_f1) )\n",
    "\n",
    "        # validation \n",
    "        model.eval()\n",
    "        valid_epoch_f1 = 0\n",
    "        n_iter = 0\n",
    "        valid_loss_list = []\n",
    "        valid_acc_list = []\n",
    "        for i, (images, targets) in enumerate(valid_loader) : \n",
    "            optimizer.zero_grad()\n",
    "            images = images.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                scores = model(images)\n",
    "                loss = F.cross_entropy(scores, targets)\n",
    "                _, preds = scores.max(dim=1)\n",
    "                valid_epoch_f1 += f1_score(preds.cpu().numpy(), targets.cpu().numpy(), average='macro')\n",
    "                n_iter += 1\n",
    "\n",
    "            correct = sum(targets == preds).cpu()\n",
    "            acc=(correct/images.shape[0] * 100)\n",
    "\n",
    "            valid_loss_list.append(loss)\n",
    "            valid_acc_list.append(acc)\n",
    "\n",
    "        valid_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "        valid_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "        valid_epoch_f1 = valid_epoch_f1/n_iter\n",
    "\n",
    "        print(f'Valid Loss Mean %.4f | Accuracy %2.2f | F1-Score %2.4f' % (valid_mean_loss, valid_mean_acc, valid_epoch_f1) )\n",
    "        print('')\n",
    "        \n",
    "        if valid_epoch_f1 > valid_best_f1score:\n",
    "            valid_best_f1score = valid_epoch_f1\n",
    "            valid_early_stop = 0\n",
    "            # new best model save (valid 기준)\n",
    "            best_model = model\n",
    "            best_models[fold] = best_model\n",
    "            # 저장\n",
    "            path = './model/'\n",
    "            torch.save(best_model.state_dict(), f'{path}fold{fold}model{valid_epoch_f1:2.2f}_epoch_{e}.pth')\n",
    "            # update fold result\n",
    "            fold_results[fold] = {\"train_mean_acc\" : train_mean_acc, \n",
    "                                  \"train_mean_loss\" : train_mean_loss, \n",
    "                                  \"train_mean_f1\" : train_epoch_f1,\n",
    "                                  \"valid_mean_acc\" : valid_mean_acc, \n",
    "                                  \"valid_mean_loss\" : valid_mean_loss,\n",
    "                                  \"valid_mean_f1\" : valid_epoch_f1, \n",
    "                                  \"epoch\" : e}\n",
    "\n",
    "        else:\n",
    "            # early stopping    \n",
    "            valid_early_stop += 1\n",
    "            if valid_early_stop >= EARLY_STOPPING_EPOCH:  # patience\n",
    "                print(\"EARLY STOPPING!!\")\n",
    "                break\n",
    "\n",
    "        lr_sched.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "febf6ceb-2e2d-47b7-8efb-25d1ac4c5359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'train_mean_acc': 99.90808823529412,\n",
       "  'train_mean_loss': 0.003409989967481659,\n",
       "  'train_mean_f1': 0.9982292217586336,\n",
       "  'valid_mean_acc': 95.7720588235294,\n",
       "  'valid_mean_loss': 0.31055936438582754,\n",
       "  'valid_mean_f1': 0.888338689175266,\n",
       "  'epoch': 28},\n",
       " 1: {'train_mean_acc': 98.80514705882354,\n",
       "  'train_mean_loss': 0.033337570589056294,\n",
       "  'train_mean_f1': 0.9805279315042602,\n",
       "  'valid_mean_acc': 94.80041997572955,\n",
       "  'valid_mean_loss': 0.23419583986020265,\n",
       "  'valid_mean_f1': 0.8412755734633216,\n",
       "  'epoch': 5},\n",
       " 2: {'train_mean_acc': 97.74816176470588,\n",
       "  'train_mean_loss': 0.07414228328750194,\n",
       "  'train_mean_f1': 0.964452932193574,\n",
       "  'valid_mean_acc': 91.17647058823529,\n",
       "  'valid_mean_loss': 0.5003492908880991,\n",
       "  'valid_mean_f1': 0.8490902100838601,\n",
       "  'epoch': 6},\n",
       " 3: {'train_mean_acc': 98.98897058823529,\n",
       "  'train_mean_loss': 0.03061463130186038,\n",
       "  'train_mean_f1': 0.9849594557749638,\n",
       "  'valid_mean_acc': 91.91176470588235,\n",
       "  'valid_mean_loss': 0.40688596939777627,\n",
       "  'valid_mean_f1': 0.8083209561417367,\n",
       "  'epoch': 2},\n",
       " 4: {'train_mean_acc': 99.6783088235294,\n",
       "  'train_mean_loss': 0.008312570014715996,\n",
       "  'train_mean_f1': 0.9963882522706051,\n",
       "  'valid_mean_acc': 92.17436981201172,\n",
       "  'valid_mean_loss': 0.6477877120761311,\n",
       "  'valid_mean_f1': 0.7474309268972086,\n",
       "  'epoch': 11}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00149092-ec53-4a79-8f2c-93fe8bbe1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        super(TestDataset).__init__()\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "testimage_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(testimage_dir, img_id) for img_id in submission.ImageID]\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "test_dataset = TestDataset(image_paths, test_transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74666c89-1e68-489c-bba8-92d51ff02f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 18s, sys: 25.6 s, total: 24min 44s\n",
      "Wall time: 24min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scores_result = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    torch.cuda.empty_cache()\n",
    "    model = best_models[fold]\n",
    "    #path = './model/'\n",
    "    #valid_mean_f1 = fold_results[fold]['valid_mean_f1']\n",
    "    #e = fold_results[fold]['epoch']\n",
    "    #mypath = f'{path}fold{fold}model{valid_mean_f1:2.2f}_epoch_{e}.pth'\n",
    "    #checkpoint = torch.load(mypath)\n",
    "    #model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    #predictions = []\n",
    "    score_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images in test_loader:\n",
    "            images = images.to(device)\n",
    "            scores = model(images)\n",
    "            _, preds = scores.max(dim=1)\n",
    "            \n",
    "            #predictions.extend(preds.detach().cpu().numpy())\n",
    "            score_list.extend(scores.detach().cpu().numpy())\n",
    "    scores_result.append(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1ede51d-53ed-4106-a01c-35c3d4c9ea3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores_result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "387b42f5-a02e-4458-abc9-b4153f3f5657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 12600, 18])\n"
     ]
    }
   ],
   "source": [
    "myresult = torch.tensor(scores_result)\n",
    "print(myresult.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ab53ee4-5300-4929-932e-73a1f1f6341c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.0693,  2.2004, -0.3666, -4.8301, -8.7945,  0.7341, -3.0786,  4.5599,\n",
       "        -1.8860, -2.8918, -7.9333, -1.6631,  0.4027, 17.7520,  1.6697, -1.3640,\n",
       "         5.0236, -0.3574])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myresult[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17613c00-a36f-4cec-a2ca-8168c06b6c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 12600, 18])\n"
     ]
    }
   ],
   "source": [
    "myresult = F.softmax(myresult, dim=2)\n",
    "print(myresult.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e5371c7-82c9-410a-8ce5-95032d361766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.3350e-10, 1.7620e-07, 1.3526e-08, 1.5584e-10, 2.9578e-12, 4.0663e-08,\n",
       "        8.9815e-10, 1.8652e-06, 2.9602e-09, 1.0826e-09, 6.9980e-12, 3.6992e-09,\n",
       "        2.9192e-08, 9.9999e-01, 1.0364e-07, 4.9887e-09, 2.9655e-06, 1.3652e-08])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myresult[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1d5dade-270b-467f-8f36-35150f2e0f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12600, 18])\n"
     ]
    }
   ],
   "source": [
    "myresult = torch.sum(myresult, dim=0)\n",
    "print(myresult.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d765870a-e251-465f-9c90-f9532bc2230d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12600])\n"
     ]
    }
   ],
   "source": [
    "_, all_predictions = myresult.max(dim=1)\n",
    "print(all_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0fcf4c3e-845f-45ae-969a-d814fdffaa29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13,  1, 13,  ..., 10,  1,  7])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "25129d26-f951-4691-85b8-2404a24335eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions = all_predictions.cpu().numpy()\n",
    "len(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "08602539-d228-4c17-a3f6-e7a8eac64759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({13: 624,\n",
       "         1: 4211,\n",
       "         7: 828,\n",
       "         4: 3890,\n",
       "         3: 151,\n",
       "         0: 610,\n",
       "         16: 1020,\n",
       "         10: 1093,\n",
       "         12: 151,\n",
       "         6: 17,\n",
       "         15: 5})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "verbal-sample",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission_efficientnet_kfold.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0973a2e-0393-4ba7-b392-4b89e054eeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
