{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f77238f9-9fa5-45fa-97c9-68cd3724be52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170d98a-18e5-4a7f-91eb-575469d09378",
   "metadata": {},
   "source": [
    "## 1. 데이터셋 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03128f-f827-4668-b213-8ae16d0f08f7",
   "metadata": {},
   "source": [
    "### (1) Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc6a210-a832-4bd7-b68f-466aa2761d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터셋 폴더 경로를 지정해주세요.\n",
    "train_dir = '/opt/ml/input/data/train'\n",
    "trainimage_dir = os.path.join(train_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c412b27-edf7-446e-b965-40a7ffc154d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "0     000001  female  Asian   45  000001_female_Asian_45\n",
       "1     000002  female  Asian   52  000002_female_Asian_52\n",
       "2     000004    male  Asian   54    000004_male_Asian_54\n",
       "3     000005  female  Asian   58  000005_female_Asian_58\n",
       "4     000006  female  Asian   59  000006_female_Asian_59\n",
       "...      ...     ...    ...  ...                     ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "train_df = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87297828-b7a3-421b-9812-59a0abed5f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45.0</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>mask5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>mask3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>mask4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>mask5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>incorrect_mask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  gender   race   age                    path            mask\n",
       "0      000001  female  Asian  45.0  000001_female_Asian_45           mask1\n",
       "1      000001  female  Asian  45.0  000001_female_Asian_45           mask2\n",
       "2      000001  female  Asian  45.0  000001_female_Asian_45           mask3\n",
       "3      000001  female  Asian  45.0  000001_female_Asian_45           mask4\n",
       "4      000001  female  Asian  45.0  000001_female_Asian_45           mask5\n",
       "...       ...     ...    ...   ...                     ...             ...\n",
       "18895  006959    male  Asian  19.0    006959_male_Asian_19           mask3\n",
       "18896  006959    male  Asian  19.0    006959_male_Asian_19           mask4\n",
       "18897  006959    male  Asian  19.0    006959_male_Asian_19           mask5\n",
       "18898  006959    male  Asian  19.0    006959_male_Asian_19  incorrect_mask\n",
       "18899  006959    male  Asian  19.0    006959_male_Asian_19          normal\n",
       "\n",
       "[18900 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = ['mask1', 'mask2', 'mask3', 'mask4', 'mask5', 'incorrect_mask', 'normal']\n",
    "mask_df = pd.DataFrame()\n",
    "for person in train_df.values:\n",
    "    for mask in masks:\n",
    "        mask_df = mask_df.append(pd.Series(np.append(person, mask)), ignore_index=True)\n",
    "mask_df.columns = np.append(train_df.columns.values, 'mask')\n",
    "mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53e94ec1-2d04-4170-8847-ca7baff2a987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001224</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>24.0</td>\n",
       "      <td>001224_female_Asian_24</td>\n",
       "      <td>mask4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>006093</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>006093_male_Asian_19</td>\n",
       "      <td>mask5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005047</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>26.0</td>\n",
       "      <td>005047_female_Asian_26</td>\n",
       "      <td>mask1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001174</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>25.0</td>\n",
       "      <td>001174_male_Asian_25</td>\n",
       "      <td>mask5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003409</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>56.0</td>\n",
       "      <td>003409_female_Asian_56</td>\n",
       "      <td>mask3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>001657</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>18.0</td>\n",
       "      <td>001657_female_Asian_18</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>003755</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52.0</td>\n",
       "      <td>003755_male_Asian_52</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>003553</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>47.0</td>\n",
       "      <td>003553_female_Asian_47</td>\n",
       "      <td>mask2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>001639</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>18.0</td>\n",
       "      <td>001639_male_Asian_18</td>\n",
       "      <td>mask1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>003172</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.0</td>\n",
       "      <td>003172_female_Asian_19</td>\n",
       "      <td>incorrect_mask</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  gender   race   age                    path            mask\n",
       "0      001224  female  Asian  24.0  001224_female_Asian_24           mask4\n",
       "1      006093    male  Asian  19.0    006093_male_Asian_19           mask5\n",
       "2      005047  female  Asian  26.0  005047_female_Asian_26           mask1\n",
       "3      001174    male  Asian  25.0    001174_male_Asian_25           mask5\n",
       "4      003409  female  Asian  56.0  003409_female_Asian_56           mask3\n",
       "...       ...     ...    ...   ...                     ...             ...\n",
       "18895  001657  female  Asian  18.0  001657_female_Asian_18          normal\n",
       "18896  003755    male  Asian  52.0    003755_male_Asian_52          normal\n",
       "18897  003553  female  Asian  47.0  003553_female_Asian_47           mask2\n",
       "18898  001639    male  Asian  18.0    001639_male_Asian_18           mask1\n",
       "18899  003172  female  Asian  19.0  003172_female_Asian_19  incorrect_mask\n",
       "\n",
       "[18900 rows x 6 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_df = mask_df.sample(frac=1).reset_index(drop=True)\n",
    "mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac641793-bbb2-4913-9f5f-802eab0014e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set dim : (15120, 6)\n",
      "Valid Set dim : (3780, 6)\n"
     ]
    }
   ],
   "source": [
    "train, valid = train_test_split(mask_df, test_size=0.2)\n",
    "print(f'Train Set dim : (%d, %d)' % (train.shape))\n",
    "print(f'Valid Set dim : (%d, %d)' % (valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c56debf3-50ef-4d40-8c80-831cfc855dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "87fa1617-a096-49f4-84e0-df0c17e2a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderDataset(Dataset):\n",
    "    def __init__(self, path, mask_df, transform):\n",
    "        super(GenderDataset).__init__()\n",
    "        self.path = path\n",
    "        self.mask_df = mask_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        full_path = os.path.join(self.path, self.mask_df.iloc[idx]['path'])\n",
    "        img_list = glob.glob(full_path + '/*')\n",
    "        file_name = self.mask_df.iloc[idx]['mask']\n",
    "        for img_name in img_list:\n",
    "            if img_name.startswith(file_name):\n",
    "                break\n",
    "        image = Image.open(os.path.join(full_path, img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.mask_df.iloc[idx]['gender']\n",
    "        label = 0 if label=='male' else 1\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea279a85-e1a8-496b-af9f-92d31ef9b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_train_data = GenderDataset(trainimage_dir, train, transform)\n",
    "gender_valid_data = GenderDataset(trainimage_dir, valid, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "363c8983-ac4f-4cf7-a022-85bc18477512",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_train = DataLoader(gender_train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "gender_valid = DataLoader(gender_valid_data, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "23c20e3f-bd45-49b9-88aa-d666c3eb2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, path, mask_df, transform):\n",
    "        self.path = path\n",
    "        self.mask_df = mask_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        full_path = os.path.join(self.path, self.mask_df.iloc[idx]['path'])\n",
    "        img_list = glob.glob(full_path + '/*')\n",
    "        file_name = self.mask_df.iloc[idx]['mask']\n",
    "        for img_name in img_list:\n",
    "            if img_name.startswith(file_name):\n",
    "                break\n",
    "        image = Image.open(os.path.join(full_path, img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.mask_df.iloc[idx]['age']\n",
    "        if label >= 60.0:\n",
    "            label = 2\n",
    "        elif label >= 30.0:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "89a53049-c2b8-4208-b771-fdf55f667c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_train_data = AgeDataset(trainimage_dir, train, transform)\n",
    "age_valid_data = AgeDataset(trainimage_dir, valid, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "828112b0-5d23-4026-85f6-c46899c3c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_train = DataLoader(age_train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "age_valid = DataLoader(age_valid_data, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "df28340b-779c-4cb7-860b-f8d537c6e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, path, mask_df, transform):\n",
    "        self.path = path\n",
    "        self.mask_df = mask_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        full_path = os.path.join(self.path, self.mask_df.iloc[idx]['path'])\n",
    "        img_list = glob.glob(full_path + '/*')\n",
    "        file_name = self.mask_df.iloc[idx]['mask']\n",
    "        for img_name in img_list:\n",
    "            if img_name.startswith(file_name):\n",
    "                break\n",
    "        image = Image.open(os.path.join(full_path, img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.mask_df.iloc[idx]['mask']\n",
    "        if label.startswith('mask'):\n",
    "            label = 0\n",
    "        elif label.startswith('incorrect'):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 2\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mask_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "24d518fa-091e-48bb-929e-51e38d7e39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train_data = MaskDataset(trainimage_dir, train, transform)\n",
    "mask_valid_data = MaskDataset(trainimage_dir, valid, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f6f6cb31-33d5-4382-8967-919d6c03b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = DataLoader(mask_train_data, batch_size=32, shuffle=True)\n",
    "mask_valid = DataLoader(mask_valid_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "### (2) Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d6c8f3ab-f585-4dd8-a261-d7ec57eac464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4630f279-a6d1-4e5f-bafe-650e74586582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "testimage_dir = os.path.join(test_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8e633ea7-6b01-4192-8b22-023ca2299de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(testimage_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "test_dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73381a1b-a63b-4e79-97ee-f3a42e689588",
   "metadata": {},
   "source": [
    "## 2. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d1171a50-fa3f-4fea-adef-0e330cfb262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "efeaf1bd-df17-4bea-870c-7447438a1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f17b9b65-bbc6-4e40-8cbf-d34e91cd5c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2de8698f-961f-45f4-be88-c8af758fc216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 64, 127, 95]          23,296\n",
      "       BatchNorm2d-2          [-1, 64, 127, 95]             128\n",
      "              ReLU-3          [-1, 64, 127, 95]               0\n",
      " AdaptiveAvgPool2d-4             [-1, 64, 1, 1]               0\n",
      "           Dropout-5                   [-1, 64]               0\n",
      "            Linear-6                   [-1, 32]           2,080\n",
      "              ReLU-7                   [-1, 32]               0\n",
      "            Linear-8                    [-1, 3]              99\n",
      "================================================================\n",
      "Total params: 25,603\n",
      "Trainable params: 25,603\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.25\n",
      "Forward/backward pass size (MB): 17.67\n",
      "Params size (MB): 0.10\n",
      "Estimated Total Size (MB): 20.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 512, 384))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b3e7b-3bf2-4bc2-ac3e-18d6ac5ce01c",
   "metadata": {},
   "source": [
    "### 마스크 착용 여부 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d75eed4b-a194-4cad-9298-4b082a193806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_model = MyModel(num_classes=3)\n",
    "mask_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0219f0-4fad-49ec-8fe0-7a8507b09a23",
   "metadata": {},
   "source": [
    "### 성별 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "11bf0059-0e2a-499e-aab6-377e059877c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=32, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_model = MyModel(num_classes=2)\n",
    "gender_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971f9a1-25cb-49a7-b4b4-6f354980555a",
   "metadata": {},
   "source": [
    "### 나이 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d90fdc5c-c098-4f2e-97e2-62e0bb71993f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_model = MyModel(num_classes=3)\n",
    "age_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef342b-e96c-4169-93c5-455cc67abb17",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f016b-18b2-43a5-b956-4ca8abcf7d07",
   "metadata": {},
   "source": [
    "### 마스크 착용 여부 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "23053052-b145-47d3-8db7-3033ff72dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  1.0962 | Classifier Accuracy 34.38\n",
      "Iteration  50 | Train Loss  0.8587 | Classifier Accuracy 68.75\n",
      "Iteration 100 | Train Loss  0.9022 | Classifier Accuracy 68.75\n",
      "Iteration 150 | Train Loss  0.9320 | Classifier Accuracy 65.62\n",
      "Iteration 200 | Train Loss  0.7116 | Classifier Accuracy 78.12\n",
      "Iteration 250 | Train Loss  0.8411 | Classifier Accuracy 71.88\n",
      "Iteration 300 | Train Loss  0.6701 | Classifier Accuracy 81.25\n",
      "Iteration 350 | Train Loss  0.7773 | Classifier Accuracy 71.88\n",
      "Iteration 400 | Train Loss  0.9211 | Classifier Accuracy 65.62\n",
      "Iteration 450 | Train Loss  0.8691 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 44 s\n",
      "Train Loss Mean 0.8161 | Accuracy 71.07 \n",
      "Valid Loss Mean 0.8053 | Accuracy 70.77 \n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.9341 | Classifier Accuracy 62.50\n",
      "Iteration  50 | Train Loss  0.7782 | Classifier Accuracy 71.88\n",
      "Iteration 100 | Train Loss  0.9097 | Classifier Accuracy 65.62\n",
      "Iteration 150 | Train Loss  0.7948 | Classifier Accuracy 71.88\n",
      "Iteration 200 | Train Loss  0.7942 | Classifier Accuracy 71.88\n",
      "Iteration 250 | Train Loss  0.9001 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.7454 | Classifier Accuracy 75.00\n",
      "Iteration 350 | Train Loss  0.9104 | Classifier Accuracy 65.62\n",
      "Iteration 400 | Train Loss  0.8494 | Classifier Accuracy 68.75\n",
      "Iteration 450 | Train Loss  0.6504 | Classifier Accuracy 81.25\n",
      "\n",
      "[Summary] Elapsed time : 2 m 3 s\n",
      "Train Loss Mean 0.8061 | Accuracy 71.39 \n",
      "Valid Loss Mean 0.8049 | Accuracy 70.77 \n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.8468 | Classifier Accuracy 68.75\n",
      "Iteration  50 | Train Loss  0.7172 | Classifier Accuracy 78.12\n",
      "Iteration 100 | Train Loss  0.9958 | Classifier Accuracy 59.38\n",
      "Iteration 150 | Train Loss  1.0314 | Classifier Accuracy 56.25\n",
      "Iteration 200 | Train Loss  0.9285 | Classifier Accuracy 62.50\n",
      "Iteration 250 | Train Loss  0.6695 | Classifier Accuracy 78.12\n",
      "Iteration 300 | Train Loss  0.8569 | Classifier Accuracy 68.75\n",
      "Iteration 350 | Train Loss  0.7338 | Classifier Accuracy 75.00\n",
      "Iteration 400 | Train Loss  0.5949 | Classifier Accuracy 84.38\n",
      "Iteration 450 | Train Loss  0.6142 | Classifier Accuracy 84.38\n",
      "\n",
      "[Summary] Elapsed time : 2 m 3 s\n",
      "Train Loss Mean 0.8038 | Accuracy 71.39 \n",
      "Valid Loss Mean 0.8086 | Accuracy 70.77 \n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.9020 | Classifier Accuracy 62.50\n",
      "Iteration  50 | Train Loss  0.8055 | Classifier Accuracy 68.75\n",
      "Iteration 100 | Train Loss  0.5519 | Classifier Accuracy 87.50\n",
      "Iteration 150 | Train Loss  0.9222 | Classifier Accuracy 65.62\n",
      "Iteration 200 | Train Loss  0.8175 | Classifier Accuracy 71.88\n",
      "Iteration 250 | Train Loss  0.8309 | Classifier Accuracy 68.75\n",
      "Iteration 300 | Train Loss  0.8953 | Classifier Accuracy 65.62\n",
      "Iteration 350 | Train Loss  1.0888 | Classifier Accuracy 53.12\n",
      "Iteration 400 | Train Loss  0.8295 | Classifier Accuracy 68.75\n",
      "Iteration 450 | Train Loss  0.8714 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 2 m 8 s\n",
      "Train Loss Mean 0.8018 | Accuracy 71.39 \n",
      "Valid Loss Mean 0.8052 | Accuracy 70.77 \n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.6712 | Classifier Accuracy 78.12\n",
      "Iteration  50 | Train Loss  0.9027 | Classifier Accuracy 65.62\n",
      "Iteration 100 | Train Loss  0.8214 | Classifier Accuracy 68.75\n",
      "Iteration 150 | Train Loss  0.6457 | Classifier Accuracy 81.25\n",
      "Iteration 200 | Train Loss  0.9343 | Classifier Accuracy 62.50\n",
      "Iteration 250 | Train Loss  0.7525 | Classifier Accuracy 75.00\n",
      "Iteration 300 | Train Loss  0.8739 | Classifier Accuracy 68.75\n",
      "Iteration 350 | Train Loss  0.8225 | Classifier Accuracy 68.75\n",
      "Iteration 400 | Train Loss  0.6858 | Classifier Accuracy 78.12\n",
      "Iteration 450 | Train Loss  0.7701 | Classifier Accuracy 71.88\n",
      "\n",
      "[Summary] Elapsed time : 2 m 7 s\n",
      "Train Loss Mean 0.8005 | Accuracy 71.39 \n",
      "Valid Loss Mean 0.8009 | Accuracy 70.77 \n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.9044 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.6964 | Classifier Accuracy 78.12\n",
      "Iteration 100 | Train Loss  0.7080 | Classifier Accuracy 78.12\n",
      "Iteration 150 | Train Loss  0.7505 | Classifier Accuracy 75.00\n",
      "Iteration 200 | Train Loss  0.6460 | Classifier Accuracy 81.25\n",
      "Iteration 250 | Train Loss  0.9566 | Classifier Accuracy 62.50\n",
      "Iteration 300 | Train Loss  0.4895 | Classifier Accuracy 90.62\n",
      "Iteration 350 | Train Loss  0.8052 | Classifier Accuracy 71.88\n",
      "Iteration 400 | Train Loss  0.8466 | Classifier Accuracy 68.75\n",
      "Iteration 450 | Train Loss  0.6906 | Classifier Accuracy 78.12\n",
      "\n",
      "[Summary] Elapsed time : 2 m 9 s\n",
      "Train Loss Mean 0.7992 | Accuracy 71.39 \n",
      "Valid Loss Mean 0.7986 | Accuracy 70.77 \n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.8875 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.7387 | Classifier Accuracy 75.00\n",
      "Iteration 100 | Train Loss  0.8051 | Classifier Accuracy 71.88\n",
      "Iteration 150 | Train Loss  1.0014 | Classifier Accuracy 59.38\n",
      "Iteration 200 | Train Loss  1.0419 | Classifier Accuracy 56.25\n",
      "Iteration 250 | Train Loss  0.5887 | Classifier Accuracy 84.38\n",
      "Iteration 300 | Train Loss  0.9419 | Classifier Accuracy 62.50\n",
      "Iteration 350 | Train Loss  0.6001 | Classifier Accuracy 84.38\n",
      "Iteration 400 | Train Loss  0.6894 | Classifier Accuracy 78.12\n",
      "Iteration 450 | Train Loss  0.7807 | Classifier Accuracy 71.88\n",
      "\n",
      "[Summary] Elapsed time : 2 m 7 s\n",
      "Train Loss Mean 0.7980 | Accuracy 71.39 \n",
      "Valid Loss Mean 0.7997 | Accuracy 70.77 \n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.5946 | Classifier Accuracy 84.38\n",
      "Iteration  50 | Train Loss  0.7484 | Classifier Accuracy 75.00\n",
      "Iteration 100 | Train Loss  0.8355 | Classifier Accuracy 68.75\n",
      "Iteration 150 | Train Loss  0.7642 | Classifier Accuracy 75.00\n",
      "Iteration 200 | Train Loss  0.7375 | Classifier Accuracy 75.00\n",
      "Iteration 250 | Train Loss  0.8928 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.7796 | Classifier Accuracy 71.88\n",
      "Iteration 350 | Train Loss  0.9177 | Classifier Accuracy 65.62\n",
      "Iteration 400 | Train Loss  0.8892 | Classifier Accuracy 65.62\n",
      "Iteration 450 | Train Loss  0.7751 | Classifier Accuracy 71.88\n",
      "\n",
      "[Summary] Elapsed time : 2 m 7 s\n",
      "Train Loss Mean 0.7997 | Accuracy 71.39 \n",
      "Valid Loss Mean 0.8025 | Accuracy 70.77 \n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.7575 | Classifier Accuracy 75.00\n",
      "Iteration  50 | Train Loss  0.7434 | Classifier Accuracy 75.00\n",
      "Iteration 100 | Train Loss  0.6553 | Classifier Accuracy 81.25\n",
      "Iteration 150 | Train Loss  0.7523 | Classifier Accuracy 75.00\n",
      "Iteration 200 | Train Loss  0.9902 | Classifier Accuracy 59.38\n",
      "Iteration 250 | Train Loss  0.9739 | Classifier Accuracy 59.38\n",
      "Iteration 300 | Train Loss  0.8989 | Classifier Accuracy 65.62\n",
      "Iteration 350 | Train Loss  0.8615 | Classifier Accuracy 68.75\n",
      "Iteration 400 | Train Loss  0.9925 | Classifier Accuracy 59.38\n",
      "Iteration 450 | Train Loss  0.9405 | Classifier Accuracy 62.50\n",
      "\n",
      "[Summary] Elapsed time : 2 m 6 s\n",
      "Train Loss Mean 0.7984 | Accuracy 71.39 \n",
      "Valid Loss Mean 0.8017 | Accuracy 70.77 \n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.8853 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.6478 | Classifier Accuracy 81.25\n",
      "Iteration 100 | Train Loss  0.8292 | Classifier Accuracy 68.75\n",
      "Iteration 150 | Train Loss  0.9280 | Classifier Accuracy 62.50\n",
      "Iteration 200 | Train Loss  0.7836 | Classifier Accuracy 71.88\n",
      "Iteration 250 | Train Loss  0.7948 | Classifier Accuracy 71.88\n",
      "Iteration 300 | Train Loss  0.8316 | Classifier Accuracy 68.75\n",
      "Iteration 350 | Train Loss  0.6806 | Classifier Accuracy 78.12\n",
      "Iteration 400 | Train Loss  0.8987 | Classifier Accuracy 65.62\n",
      "Iteration 450 | Train Loss  0.6768 | Classifier Accuracy 78.12\n",
      "\n",
      "[Summary] Elapsed time : 2 m 4 s\n",
      "Train Loss Mean 0.7973 | Accuracy 71.39 \n",
      "Valid Loss Mean 0.8011 | Accuracy 70.77 \n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.9456 | Classifier Accuracy 62.50\n",
      "Iteration  50 | Train Loss  0.9300 | Classifier Accuracy 62.50\n",
      "Iteration 100 | Train Loss  0.8469 | Classifier Accuracy 68.75\n",
      "Iteration 150 | Train Loss  0.9087 | Classifier Accuracy 65.62\n",
      "Iteration 200 | Train Loss  0.7844 | Classifier Accuracy 71.88\n",
      "Iteration 250 | Train Loss  0.9924 | Classifier Accuracy 59.38\n",
      "Iteration 300 | Train Loss  0.7380 | Classifier Accuracy 75.00\n",
      "Iteration 350 | Train Loss  0.6826 | Classifier Accuracy 78.12\n",
      "Iteration 400 | Train Loss  0.6362 | Classifier Accuracy 81.25\n",
      "Iteration 450 | Train Loss  0.9108 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 2 m 7 s\n",
      "Train Loss Mean 0.7983 | Accuracy 71.39 \n",
      "Valid Loss Mean 0.8028 | Accuracy 70.77 \n",
      "\n",
      "EARLY STOPPING!!\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(mask_model.parameters(), lr=1e-3)\n",
    "lr_sched = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "mask_model.train()\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "\n",
    "    # train\n",
    "    for i, (images, targets) in enumerate(mask_train) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = mask_model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 50 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    for i, (images, targets) in enumerate(mask_valid) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = mask_model(images)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "\n",
    "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        best_model = mask_model\n",
    "        path = './mask_model/'\n",
    "        torch.save(best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    lr_sched.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601c94d-8113-4e75-a024-d027369d9df0",
   "metadata": {},
   "source": [
    "### 성별 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5215c74b-e926-4239-a572-cae4e8bd7278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  0.6924 | Classifier Accuracy 53.12\n",
      "Iteration  50 | Train Loss  0.6987 | Classifier Accuracy 59.38\n",
      "Iteration 100 | Train Loss  0.5753 | Classifier Accuracy 78.12\n",
      "Iteration 150 | Train Loss  0.6142 | Classifier Accuracy 68.75\n",
      "Iteration 200 | Train Loss  0.6173 | Classifier Accuracy 68.75\n",
      "Iteration 250 | Train Loss  0.6409 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.6745 | Classifier Accuracy 53.12\n",
      "Iteration 350 | Train Loss  0.6861 | Classifier Accuracy 59.38\n",
      "Iteration 400 | Train Loss  0.5974 | Classifier Accuracy 68.75\n",
      "Iteration 450 | Train Loss  0.6200 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 8 s\n",
      "Train Loss Mean 0.6585 | Accuracy 61.36 \n",
      "Valid Loss Mean 0.6621 | Accuracy 60.14 \n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.6700 | Classifier Accuracy 53.12\n",
      "Iteration  50 | Train Loss  0.6859 | Classifier Accuracy 62.50\n",
      "Iteration 100 | Train Loss  0.6198 | Classifier Accuracy 71.88\n",
      "Iteration 150 | Train Loss  0.7723 | Classifier Accuracy 46.88\n",
      "Iteration 200 | Train Loss  0.6486 | Classifier Accuracy 65.62\n",
      "Iteration 250 | Train Loss  0.6862 | Classifier Accuracy 50.00\n",
      "Iteration 300 | Train Loss  0.6727 | Classifier Accuracy 62.50\n",
      "Iteration 350 | Train Loss  0.6579 | Classifier Accuracy 65.62\n",
      "Iteration 400 | Train Loss  0.6073 | Classifier Accuracy 65.62\n",
      "Iteration 450 | Train Loss  0.6733 | Classifier Accuracy 68.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 24 s\n",
      "Train Loss Mean 0.6515 | Accuracy 61.71 \n",
      "Valid Loss Mean 0.6501 | Accuracy 60.43 \n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.5810 | Classifier Accuracy 68.75\n",
      "Iteration  50 | Train Loss  0.6937 | Classifier Accuracy 53.12\n",
      "Iteration 100 | Train Loss  0.7384 | Classifier Accuracy 46.88\n",
      "Iteration 150 | Train Loss  0.7215 | Classifier Accuracy 50.00\n",
      "Iteration 200 | Train Loss  0.7414 | Classifier Accuracy 59.38\n",
      "Iteration 250 | Train Loss  0.7057 | Classifier Accuracy 53.12\n",
      "Iteration 300 | Train Loss  0.6022 | Classifier Accuracy 68.75\n",
      "Iteration 350 | Train Loss  0.6693 | Classifier Accuracy 62.50\n",
      "Iteration 400 | Train Loss  0.6030 | Classifier Accuracy 68.75\n",
      "Iteration 450 | Train Loss  0.5893 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 25 s\n",
      "Train Loss Mean 0.6453 | Accuracy 62.13 \n",
      "Valid Loss Mean 0.6476 | Accuracy 61.06 \n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.7162 | Classifier Accuracy 56.25\n",
      "Iteration  50 | Train Loss  0.5838 | Classifier Accuracy 68.75\n",
      "Iteration 100 | Train Loss  0.5933 | Classifier Accuracy 75.00\n",
      "Iteration 150 | Train Loss  0.6455 | Classifier Accuracy 53.12\n",
      "Iteration 200 | Train Loss  0.6094 | Classifier Accuracy 65.62\n",
      "Iteration 250 | Train Loss  0.5203 | Classifier Accuracy 78.12\n",
      "Iteration 300 | Train Loss  0.5900 | Classifier Accuracy 68.75\n",
      "Iteration 350 | Train Loss  0.6675 | Classifier Accuracy 56.25\n",
      "Iteration 400 | Train Loss  0.5874 | Classifier Accuracy 65.62\n",
      "Iteration 450 | Train Loss  0.6270 | Classifier Accuracy 68.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 26 s\n",
      "Train Loss Mean 0.6418 | Accuracy 62.54 \n",
      "Valid Loss Mean 0.6393 | Accuracy 61.79 \n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.5768 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.7032 | Classifier Accuracy 62.50\n",
      "Iteration 100 | Train Loss  0.6189 | Classifier Accuracy 53.12\n",
      "Iteration 150 | Train Loss  0.6415 | Classifier Accuracy 68.75\n",
      "Iteration 200 | Train Loss  0.6561 | Classifier Accuracy 65.62\n",
      "Iteration 250 | Train Loss  0.6015 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.6341 | Classifier Accuracy 62.50\n",
      "Iteration 350 | Train Loss  0.5428 | Classifier Accuracy 75.00\n",
      "Iteration 400 | Train Loss  0.6811 | Classifier Accuracy 53.12\n",
      "Iteration 450 | Train Loss  0.5874 | Classifier Accuracy 62.50\n",
      "\n",
      "[Summary] Elapsed time : 1 m 24 s\n",
      "Train Loss Mean 0.6358 | Accuracy 63.23 \n",
      "Valid Loss Mean 0.6423 | Accuracy 61.63 \n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.5549 | Classifier Accuracy 71.88\n",
      "Iteration  50 | Train Loss  0.7436 | Classifier Accuracy 59.38\n",
      "Iteration 100 | Train Loss  0.7023 | Classifier Accuracy 71.88\n",
      "Iteration 150 | Train Loss  0.6223 | Classifier Accuracy 62.50\n",
      "Iteration 200 | Train Loss  0.5500 | Classifier Accuracy 81.25\n",
      "Iteration 250 | Train Loss  0.5638 | Classifier Accuracy 71.88\n",
      "Iteration 300 | Train Loss  0.7124 | Classifier Accuracy 59.38\n",
      "Iteration 350 | Train Loss  0.6649 | Classifier Accuracy 56.25\n",
      "Iteration 400 | Train Loss  0.6112 | Classifier Accuracy 65.62\n",
      "Iteration 450 | Train Loss  0.5554 | Classifier Accuracy 71.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 23 s\n",
      "Train Loss Mean 0.6281 | Accuracy 63.95 \n",
      "Valid Loss Mean 0.6337 | Accuracy 62.82 \n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.6386 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.6681 | Classifier Accuracy 53.12\n",
      "Iteration 100 | Train Loss  0.6624 | Classifier Accuracy 56.25\n",
      "Iteration 150 | Train Loss  0.7484 | Classifier Accuracy 40.62\n",
      "Iteration 200 | Train Loss  0.6152 | Classifier Accuracy 68.75\n",
      "Iteration 250 | Train Loss  0.6385 | Classifier Accuracy 62.50\n",
      "Iteration 300 | Train Loss  0.7608 | Classifier Accuracy 40.62\n",
      "Iteration 350 | Train Loss  0.5627 | Classifier Accuracy 65.62\n",
      "Iteration 400 | Train Loss  0.6541 | Classifier Accuracy 56.25\n",
      "Iteration 450 | Train Loss  0.6743 | Classifier Accuracy 56.25\n",
      "\n",
      "[Summary] Elapsed time : 1 m 24 s\n",
      "Train Loss Mean 0.6266 | Accuracy 64.46 \n",
      "Valid Loss Mean 0.6338 | Accuracy 62.74 \n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.6454 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.5640 | Classifier Accuracy 75.00\n",
      "Iteration 100 | Train Loss  0.5988 | Classifier Accuracy 68.75\n",
      "Iteration 150 | Train Loss  0.5753 | Classifier Accuracy 71.88\n",
      "Iteration 200 | Train Loss  0.5778 | Classifier Accuracy 75.00\n",
      "Iteration 250 | Train Loss  0.5652 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.5240 | Classifier Accuracy 81.25\n",
      "Iteration 350 | Train Loss  0.6535 | Classifier Accuracy 68.75\n",
      "Iteration 400 | Train Loss  0.6353 | Classifier Accuracy 65.62\n",
      "Iteration 450 | Train Loss  0.5499 | Classifier Accuracy 75.00\n",
      "\n",
      "[Summary] Elapsed time : 1 m 24 s\n",
      "Train Loss Mean 0.6245 | Accuracy 64.30 \n",
      "Valid Loss Mean 0.6323 | Accuracy 62.29 \n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.6086 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.6861 | Classifier Accuracy 56.25\n",
      "Iteration 100 | Train Loss  0.6661 | Classifier Accuracy 59.38\n",
      "Iteration 150 | Train Loss  0.7313 | Classifier Accuracy 62.50\n",
      "Iteration 200 | Train Loss  0.7171 | Classifier Accuracy 50.00\n",
      "Iteration 250 | Train Loss  0.6447 | Classifier Accuracy 62.50\n",
      "Iteration 300 | Train Loss  0.6544 | Classifier Accuracy 65.62\n",
      "Iteration 350 | Train Loss  0.6167 | Classifier Accuracy 65.62\n",
      "Iteration 400 | Train Loss  0.6423 | Classifier Accuracy 75.00\n",
      "Iteration 450 | Train Loss  0.5534 | Classifier Accuracy 81.25\n",
      "\n",
      "[Summary] Elapsed time : 1 m 25 s\n",
      "Train Loss Mean 0.6215 | Accuracy 65.17 \n",
      "Valid Loss Mean 0.6345 | Accuracy 62.82 \n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.6626 | Classifier Accuracy 59.38\n",
      "Iteration  50 | Train Loss  0.5789 | Classifier Accuracy 59.38\n",
      "Iteration 100 | Train Loss  0.6636 | Classifier Accuracy 59.38\n",
      "Iteration 150 | Train Loss  0.6336 | Classifier Accuracy 65.62\n",
      "Iteration 200 | Train Loss  0.5707 | Classifier Accuracy 65.62\n",
      "Iteration 250 | Train Loss  0.6875 | Classifier Accuracy 59.38\n",
      "Iteration 300 | Train Loss  0.5257 | Classifier Accuracy 75.00\n",
      "Iteration 350 | Train Loss  0.5842 | Classifier Accuracy 65.62\n",
      "Iteration 400 | Train Loss  0.6024 | Classifier Accuracy 65.62\n",
      "Iteration 450 | Train Loss  0.5272 | Classifier Accuracy 87.50\n",
      "\n",
      "[Summary] Elapsed time : 1 m 22 s\n",
      "Train Loss Mean 0.6226 | Accuracy 65.06 \n",
      "Valid Loss Mean 0.6300 | Accuracy 62.95 \n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.5602 | Classifier Accuracy 68.75\n",
      "Iteration  50 | Train Loss  0.6060 | Classifier Accuracy 62.50\n",
      "Iteration 100 | Train Loss  0.6912 | Classifier Accuracy 53.12\n",
      "Iteration 150 | Train Loss  0.5459 | Classifier Accuracy 71.88\n",
      "Iteration 200 | Train Loss  0.6901 | Classifier Accuracy 62.50\n",
      "Iteration 250 | Train Loss  0.5539 | Classifier Accuracy 75.00\n",
      "Iteration 300 | Train Loss  0.7032 | Classifier Accuracy 53.12\n",
      "Iteration 350 | Train Loss  0.5620 | Classifier Accuracy 84.38\n",
      "Iteration 400 | Train Loss  0.6754 | Classifier Accuracy 59.38\n",
      "Iteration 450 | Train Loss  0.5367 | Classifier Accuracy 75.00\n",
      "\n",
      "[Summary] Elapsed time : 1 m 23 s\n",
      "Train Loss Mean 0.6224 | Accuracy 64.57 \n",
      "Valid Loss Mean 0.6258 | Accuracy 64.00 \n",
      "\n",
      " ====================== epoch 12 ======================\n",
      "Iteration   0 | Train Loss  0.6366 | Classifier Accuracy 59.38\n",
      "Iteration  50 | Train Loss  0.5945 | Classifier Accuracy 68.75\n",
      "Iteration 100 | Train Loss  0.5957 | Classifier Accuracy 65.62\n",
      "Iteration 150 | Train Loss  0.7063 | Classifier Accuracy 53.12\n",
      "Iteration 200 | Train Loss  0.7343 | Classifier Accuracy 53.12\n",
      "Iteration 250 | Train Loss  0.5005 | Classifier Accuracy 84.38\n",
      "Iteration 300 | Train Loss  0.5377 | Classifier Accuracy 71.88\n",
      "Iteration 350 | Train Loss  0.6395 | Classifier Accuracy 53.12\n",
      "Iteration 400 | Train Loss  0.6397 | Classifier Accuracy 68.75\n",
      "Iteration 450 | Train Loss  0.5698 | Classifier Accuracy 68.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 24 s\n",
      "Train Loss Mean 0.6175 | Accuracy 65.35 \n",
      "Valid Loss Mean 0.6272 | Accuracy 63.29 \n",
      "\n",
      " ====================== epoch 13 ======================\n",
      "Iteration   0 | Train Loss  0.6448 | Classifier Accuracy 53.12\n",
      "Iteration  50 | Train Loss  0.7437 | Classifier Accuracy 53.12\n",
      "Iteration 100 | Train Loss  0.6816 | Classifier Accuracy 62.50\n",
      "Iteration 150 | Train Loss  0.5749 | Classifier Accuracy 68.75\n",
      "Iteration 200 | Train Loss  0.7160 | Classifier Accuracy 65.62\n",
      "Iteration 250 | Train Loss  0.6508 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.5791 | Classifier Accuracy 68.75\n",
      "Iteration 350 | Train Loss  0.5854 | Classifier Accuracy 65.62\n",
      "Iteration 400 | Train Loss  0.6773 | Classifier Accuracy 65.62\n",
      "Iteration 450 | Train Loss  0.6434 | Classifier Accuracy 53.12\n",
      "\n",
      "[Summary] Elapsed time : 1 m 23 s\n",
      "Train Loss Mean 0.6200 | Accuracy 64.94 \n",
      "Valid Loss Mean 0.6296 | Accuracy 63.84 \n",
      "\n",
      " ====================== epoch 14 ======================\n",
      "Iteration   0 | Train Loss  0.6133 | Classifier Accuracy 75.00\n",
      "Iteration  50 | Train Loss  0.6120 | Classifier Accuracy 75.00\n",
      "Iteration 100 | Train Loss  0.5663 | Classifier Accuracy 78.12\n",
      "Iteration 150 | Train Loss  0.5728 | Classifier Accuracy 71.88\n",
      "Iteration 200 | Train Loss  0.6616 | Classifier Accuracy 62.50\n",
      "Iteration 250 | Train Loss  0.6239 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.5978 | Classifier Accuracy 59.38\n",
      "Iteration 350 | Train Loss  0.6271 | Classifier Accuracy 75.00\n",
      "Iteration 400 | Train Loss  0.5496 | Classifier Accuracy 71.88\n",
      "Iteration 450 | Train Loss  0.5043 | Classifier Accuracy 71.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 24 s\n",
      "Train Loss Mean 0.6209 | Accuracy 65.19 \n",
      "Valid Loss Mean 0.6254 | Accuracy 63.03 \n",
      "\n",
      " ====================== epoch 15 ======================\n",
      "Iteration   0 | Train Loss  0.6028 | Classifier Accuracy 62.50\n",
      "Iteration  50 | Train Loss  0.7637 | Classifier Accuracy 62.50\n",
      "Iteration 100 | Train Loss  0.6495 | Classifier Accuracy 56.25\n",
      "Iteration 150 | Train Loss  0.5860 | Classifier Accuracy 62.50\n",
      "Iteration 200 | Train Loss  0.6341 | Classifier Accuracy 62.50\n",
      "Iteration 250 | Train Loss  0.6367 | Classifier Accuracy 71.88\n",
      "Iteration 300 | Train Loss  0.7062 | Classifier Accuracy 59.38\n",
      "Iteration 350 | Train Loss  0.7107 | Classifier Accuracy 50.00\n",
      "Iteration 400 | Train Loss  0.5331 | Classifier Accuracy 75.00\n",
      "Iteration 450 | Train Loss  0.6007 | Classifier Accuracy 68.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 24 s\n",
      "Train Loss Mean 0.6227 | Accuracy 65.09 \n",
      "Valid Loss Mean 0.6262 | Accuracy 63.73 \n",
      "\n",
      " ====================== epoch 16 ======================\n",
      "Iteration   0 | Train Loss  0.6057 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.5289 | Classifier Accuracy 84.38\n",
      "Iteration 100 | Train Loss  0.7367 | Classifier Accuracy 59.38\n",
      "Iteration 150 | Train Loss  0.6156 | Classifier Accuracy 59.38\n",
      "Iteration 200 | Train Loss  0.6933 | Classifier Accuracy 53.12\n",
      "Iteration 250 | Train Loss  0.6238 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.6080 | Classifier Accuracy 62.50\n",
      "Iteration 350 | Train Loss  0.5636 | Classifier Accuracy 75.00\n",
      "Iteration 400 | Train Loss  0.6286 | Classifier Accuracy 68.75\n",
      "Iteration 450 | Train Loss  0.5656 | Classifier Accuracy 81.25\n",
      "\n",
      "[Summary] Elapsed time : 1 m 23 s\n",
      "Train Loss Mean 0.6186 | Accuracy 65.08 \n",
      "Valid Loss Mean 0.6270 | Accuracy 63.03 \n",
      "\n",
      " ====================== epoch 17 ======================\n",
      "Iteration   0 | Train Loss  0.6407 | Classifier Accuracy 50.00\n",
      "Iteration  50 | Train Loss  0.6861 | Classifier Accuracy 59.38\n",
      "Iteration 100 | Train Loss  0.6518 | Classifier Accuracy 65.62\n",
      "Iteration 150 | Train Loss  0.7736 | Classifier Accuracy 40.62\n",
      "Iteration 200 | Train Loss  0.7214 | Classifier Accuracy 53.12\n",
      "Iteration 250 | Train Loss  0.5984 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.6726 | Classifier Accuracy 56.25\n",
      "Iteration 350 | Train Loss  0.6198 | Classifier Accuracy 62.50\n",
      "Iteration 400 | Train Loss  0.6358 | Classifier Accuracy 62.50\n",
      "Iteration 450 | Train Loss  0.6609 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 23 s\n",
      "Train Loss Mean 0.6191 | Accuracy 65.10 \n",
      "Valid Loss Mean 0.6299 | Accuracy 63.58 \n",
      "\n",
      " ====================== epoch 18 ======================\n",
      "Iteration   0 | Train Loss  0.6325 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.6288 | Classifier Accuracy 59.38\n",
      "Iteration 100 | Train Loss  0.6677 | Classifier Accuracy 53.12\n",
      "Iteration 150 | Train Loss  0.6932 | Classifier Accuracy 56.25\n",
      "Iteration 200 | Train Loss  0.6189 | Classifier Accuracy 65.62\n",
      "Iteration 250 | Train Loss  0.5879 | Classifier Accuracy 68.75\n",
      "Iteration 300 | Train Loss  0.6676 | Classifier Accuracy 50.00\n",
      "Iteration 350 | Train Loss  0.6689 | Classifier Accuracy 56.25\n",
      "Iteration 400 | Train Loss  0.6297 | Classifier Accuracy 59.38\n",
      "Iteration 450 | Train Loss  0.6594 | Classifier Accuracy 56.25\n",
      "\n",
      "[Summary] Elapsed time : 1 m 22 s\n",
      "Train Loss Mean 0.6228 | Accuracy 64.69 \n",
      "Valid Loss Mean 0.6228 | Accuracy 64.73 \n",
      "\n",
      " ====================== epoch 19 ======================\n",
      "Iteration   0 | Train Loss  0.5839 | Classifier Accuracy 78.12\n",
      "Iteration  50 | Train Loss  0.5915 | Classifier Accuracy 62.50\n",
      "Iteration 100 | Train Loss  0.7650 | Classifier Accuracy 56.25\n",
      "Iteration 150 | Train Loss  0.6417 | Classifier Accuracy 50.00\n",
      "Iteration 200 | Train Loss  0.5781 | Classifier Accuracy 75.00\n",
      "Iteration 250 | Train Loss  0.5917 | Classifier Accuracy 62.50\n",
      "Iteration 300 | Train Loss  0.6206 | Classifier Accuracy 59.38\n",
      "Iteration 350 | Train Loss  0.6315 | Classifier Accuracy 68.75\n",
      "Iteration 400 | Train Loss  0.6461 | Classifier Accuracy 62.50\n",
      "Iteration 450 | Train Loss  0.4931 | Classifier Accuracy 78.12\n",
      "\n",
      "[Summary] Elapsed time : 1 m 22 s\n",
      "Train Loss Mean 0.6207 | Accuracy 64.50 \n",
      "Valid Loss Mean 0.6245 | Accuracy 63.60 \n",
      "\n",
      " ====================== epoch 20 ======================\n",
      "Iteration   0 | Train Loss  0.6744 | Classifier Accuracy 59.38\n",
      "Iteration  50 | Train Loss  0.6277 | Classifier Accuracy 62.50\n",
      "Iteration 100 | Train Loss  0.6142 | Classifier Accuracy 68.75\n",
      "Iteration 150 | Train Loss  0.5607 | Classifier Accuracy 68.75\n",
      "Iteration 200 | Train Loss  0.6464 | Classifier Accuracy 53.12\n",
      "Iteration 250 | Train Loss  0.5792 | Classifier Accuracy 68.75\n",
      "Iteration 300 | Train Loss  0.6537 | Classifier Accuracy 59.38\n",
      "Iteration 350 | Train Loss  0.6080 | Classifier Accuracy 62.50\n",
      "Iteration 400 | Train Loss  0.6233 | Classifier Accuracy 65.62\n",
      "Iteration 450 | Train Loss  0.5783 | Classifier Accuracy 68.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 23 s\n",
      "Train Loss Mean 0.6207 | Accuracy 64.99 \n",
      "Valid Loss Mean 0.6288 | Accuracy 62.55 \n",
      "\n",
      " ====================== epoch 21 ======================\n",
      "Iteration   0 | Train Loss  0.6470 | Classifier Accuracy 59.38\n",
      "Iteration  50 | Train Loss  0.5646 | Classifier Accuracy 84.38\n",
      "Iteration 100 | Train Loss  0.5709 | Classifier Accuracy 68.75\n",
      "Iteration 150 | Train Loss  0.6161 | Classifier Accuracy 78.12\n",
      "Iteration 200 | Train Loss  0.6853 | Classifier Accuracy 50.00\n",
      "Iteration 250 | Train Loss  0.6636 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.5477 | Classifier Accuracy 75.00\n",
      "Iteration 350 | Train Loss  0.6653 | Classifier Accuracy 53.12\n",
      "Iteration 400 | Train Loss  0.5535 | Classifier Accuracy 68.75\n",
      "Iteration 450 | Train Loss  0.7003 | Classifier Accuracy 56.25\n",
      "\n",
      "[Summary] Elapsed time : 1 m 23 s\n",
      "Train Loss Mean 0.6203 | Accuracy 65.31 \n",
      "Valid Loss Mean 0.6317 | Accuracy 63.55 \n",
      "\n",
      " ====================== epoch 22 ======================\n",
      "Iteration   0 | Train Loss  0.5810 | Classifier Accuracy 62.50\n",
      "Iteration  50 | Train Loss  0.5574 | Classifier Accuracy 71.88\n",
      "Iteration 100 | Train Loss  0.5767 | Classifier Accuracy 68.75\n",
      "Iteration 150 | Train Loss  0.6124 | Classifier Accuracy 65.62\n",
      "Iteration 200 | Train Loss  0.7341 | Classifier Accuracy 53.12\n",
      "Iteration 250 | Train Loss  0.7617 | Classifier Accuracy 62.50\n",
      "Iteration 300 | Train Loss  0.5593 | Classifier Accuracy 78.12\n",
      "Iteration 350 | Train Loss  0.5655 | Classifier Accuracy 71.88\n",
      "Iteration 400 | Train Loss  0.5491 | Classifier Accuracy 75.00\n",
      "Iteration 450 | Train Loss  0.5563 | Classifier Accuracy 81.25\n",
      "\n",
      "[Summary] Elapsed time : 1 m 22 s\n",
      "Train Loss Mean 0.6175 | Accuracy 65.45 \n",
      "Valid Loss Mean 0.6306 | Accuracy 62.68 \n",
      "\n",
      " ====================== epoch 23 ======================\n",
      "Iteration   0 | Train Loss  0.5832 | Classifier Accuracy 59.38\n",
      "Iteration  50 | Train Loss  0.5896 | Classifier Accuracy 68.75\n",
      "Iteration 100 | Train Loss  0.6560 | Classifier Accuracy 53.12\n",
      "Iteration 150 | Train Loss  0.6503 | Classifier Accuracy 65.62\n",
      "Iteration 200 | Train Loss  0.6593 | Classifier Accuracy 62.50\n",
      "Iteration 250 | Train Loss  0.7411 | Classifier Accuracy 46.88\n",
      "Iteration 300 | Train Loss  0.6660 | Classifier Accuracy 71.88\n",
      "Iteration 350 | Train Loss  0.6597 | Classifier Accuracy 56.25\n",
      "Iteration 400 | Train Loss  0.6391 | Classifier Accuracy 62.50\n",
      "Iteration 450 | Train Loss  0.5894 | Classifier Accuracy 62.50\n",
      "\n",
      "[Summary] Elapsed time : 1 m 24 s\n",
      "Train Loss Mean 0.6183 | Accuracy 65.66 \n",
      "Valid Loss Mean 0.6365 | Accuracy 63.26 \n",
      "\n",
      "EARLY STOPPING!!\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(gender_model.parameters(), lr=1e-3)\n",
    "lr_sched = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "gender_model.train()\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "\n",
    "    # train\n",
    "    for i, (images, targets) in enumerate(gender_train) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = gender_model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 50 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    for i, (images, targets) in enumerate(gender_valid) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = gender_model(images)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "\n",
    "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        gender_best_model = gender_model\n",
    "        path = './gender_model/'\n",
    "        torch.save(best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    lr_sched.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a1662-4a6a-4088-b3e7-b0557ba6408e",
   "metadata": {},
   "source": [
    "### 나이 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ec5a4a4b-51b4-4817-8bd9-b89ea55ac896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  1.1152 | Classifier Accuracy 28.12\n",
      "Iteration  50 | Train Loss  0.7981 | Classifier Accuracy 65.62\n",
      "Iteration 100 | Train Loss  0.9373 | Classifier Accuracy 53.12\n",
      "Iteration 150 | Train Loss  0.7393 | Classifier Accuracy 65.62\n",
      "Iteration 200 | Train Loss  0.9226 | Classifier Accuracy 50.00\n",
      "Iteration 250 | Train Loss  0.9060 | Classifier Accuracy 50.00\n",
      "Iteration 300 | Train Loss  0.8675 | Classifier Accuracy 56.25\n",
      "Iteration 350 | Train Loss  0.7638 | Classifier Accuracy 59.38\n",
      "Iteration 400 | Train Loss  0.9923 | Classifier Accuracy 56.25\n",
      "Iteration 450 | Train Loss  0.6774 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 7 s\n",
      "Train Loss Mean 0.8460 | Accuracy 59.35 \n",
      "Valid Loss Mean 0.8052 | Accuracy 63.21 \n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.6856 | Classifier Accuracy 68.75\n",
      "Iteration  50 | Train Loss  0.8595 | Classifier Accuracy 56.25\n",
      "Iteration 100 | Train Loss  0.7628 | Classifier Accuracy 71.88\n",
      "Iteration 150 | Train Loss  0.7399 | Classifier Accuracy 68.75\n",
      "Iteration 200 | Train Loss  0.7441 | Classifier Accuracy 75.00\n",
      "Iteration 250 | Train Loss  0.8035 | Classifier Accuracy 50.00\n",
      "Iteration 300 | Train Loss  0.8564 | Classifier Accuracy 56.25\n",
      "Iteration 350 | Train Loss  0.6629 | Classifier Accuracy 68.75\n",
      "Iteration 400 | Train Loss  0.7197 | Classifier Accuracy 59.38\n",
      "Iteration 450 | Train Loss  0.7590 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 23 s\n",
      "Train Loss Mean 0.8061 | Accuracy 62.83 \n",
      "Valid Loss Mean 0.7909 | Accuracy 64.44 \n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.7825 | Classifier Accuracy 56.25\n",
      "Iteration  50 | Train Loss  0.9100 | Classifier Accuracy 62.50\n",
      "Iteration 100 | Train Loss  0.9488 | Classifier Accuracy 53.12\n",
      "Iteration 150 | Train Loss  0.6804 | Classifier Accuracy 68.75\n",
      "Iteration 200 | Train Loss  1.0354 | Classifier Accuracy 59.38\n",
      "Iteration 250 | Train Loss  0.7844 | Classifier Accuracy 68.75\n",
      "Iteration 300 | Train Loss  0.7676 | Classifier Accuracy 56.25\n",
      "Iteration 350 | Train Loss  0.7163 | Classifier Accuracy 71.88\n",
      "Iteration 400 | Train Loss  0.7635 | Classifier Accuracy 59.38\n",
      "Iteration 450 | Train Loss  0.6604 | Classifier Accuracy 71.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 26 s\n",
      "Train Loss Mean 0.7936 | Accuracy 64.22 \n",
      "Valid Loss Mean 0.7705 | Accuracy 65.18 \n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.7971 | Classifier Accuracy 56.25\n",
      "Iteration  50 | Train Loss  0.9488 | Classifier Accuracy 53.12\n",
      "Iteration 100 | Train Loss  0.8396 | Classifier Accuracy 53.12\n",
      "Iteration 150 | Train Loss  1.1212 | Classifier Accuracy 50.00\n",
      "Iteration 200 | Train Loss  0.8291 | Classifier Accuracy 68.75\n",
      "Iteration 250 | Train Loss  0.5904 | Classifier Accuracy 75.00\n",
      "Iteration 300 | Train Loss  0.7661 | Classifier Accuracy 71.88\n",
      "Iteration 350 | Train Loss  0.8844 | Classifier Accuracy 56.25\n",
      "Iteration 400 | Train Loss  0.7675 | Classifier Accuracy 68.75\n",
      "Iteration 450 | Train Loss  0.5827 | Classifier Accuracy 71.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 25 s\n",
      "Train Loss Mean 0.7808 | Accuracy 65.01 \n",
      "Valid Loss Mean 0.7689 | Accuracy 65.31 \n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.6728 | Classifier Accuracy 75.00\n",
      "Iteration  50 | Train Loss  0.6578 | Classifier Accuracy 78.12\n",
      "Iteration 100 | Train Loss  0.6237 | Classifier Accuracy 84.38\n",
      "Iteration 150 | Train Loss  0.5547 | Classifier Accuracy 84.38\n",
      "Iteration 200 | Train Loss  0.6599 | Classifier Accuracy 68.75\n",
      "Iteration 250 | Train Loss  0.7648 | Classifier Accuracy 75.00\n",
      "Iteration 300 | Train Loss  0.8415 | Classifier Accuracy 59.38\n",
      "Iteration 350 | Train Loss  0.7362 | Classifier Accuracy 68.75\n",
      "Iteration 400 | Train Loss  0.6454 | Classifier Accuracy 78.12\n",
      "Iteration 450 | Train Loss  0.7884 | Classifier Accuracy 56.25\n",
      "\n",
      "[Summary] Elapsed time : 1 m 24 s\n",
      "Train Loss Mean 0.7741 | Accuracy 65.37 \n",
      "Valid Loss Mean 0.7594 | Accuracy 66.26 \n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.6605 | Classifier Accuracy 75.00\n",
      "Iteration  50 | Train Loss  0.7606 | Classifier Accuracy 65.62\n",
      "Iteration 100 | Train Loss  0.9334 | Classifier Accuracy 59.38\n",
      "Iteration 150 | Train Loss  0.8163 | Classifier Accuracy 56.25\n",
      "Iteration 200 | Train Loss  0.8970 | Classifier Accuracy 62.50\n",
      "Iteration 250 | Train Loss  0.7495 | Classifier Accuracy 62.50\n",
      "Iteration 300 | Train Loss  0.7661 | Classifier Accuracy 71.88\n",
      "Iteration 350 | Train Loss  0.8650 | Classifier Accuracy 65.62\n",
      "Iteration 400 | Train Loss  0.9303 | Classifier Accuracy 56.25\n",
      "Iteration 450 | Train Loss  0.8450 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 25 s\n",
      "Train Loss Mean 0.7628 | Accuracy 66.56 \n",
      "Valid Loss Mean 0.7522 | Accuracy 65.97 \n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.5346 | Classifier Accuracy 81.25\n",
      "Iteration  50 | Train Loss  0.6475 | Classifier Accuracy 71.88\n",
      "Iteration 100 | Train Loss  0.6823 | Classifier Accuracy 65.62\n",
      "Iteration 150 | Train Loss  0.8356 | Classifier Accuracy 68.75\n",
      "Iteration 200 | Train Loss  0.7464 | Classifier Accuracy 59.38\n",
      "Iteration 250 | Train Loss  0.5875 | Classifier Accuracy 81.25\n",
      "Iteration 300 | Train Loss  0.8065 | Classifier Accuracy 68.75\n",
      "Iteration 350 | Train Loss  0.7023 | Classifier Accuracy 78.12\n",
      "Iteration 400 | Train Loss  0.6427 | Classifier Accuracy 71.88\n",
      "Iteration 450 | Train Loss  0.8665 | Classifier Accuracy 62.50\n",
      "\n",
      "[Summary] Elapsed time : 1 m 25 s\n",
      "Train Loss Mean 0.7556 | Accuracy 66.68 \n",
      "Valid Loss Mean 0.7439 | Accuracy 66.96 \n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.6793 | Classifier Accuracy 68.75\n",
      "Iteration  50 | Train Loss  0.8313 | Classifier Accuracy 59.38\n",
      "Iteration 100 | Train Loss  0.6976 | Classifier Accuracy 62.50\n",
      "Iteration 150 | Train Loss  0.8603 | Classifier Accuracy 56.25\n",
      "Iteration 200 | Train Loss  0.6422 | Classifier Accuracy 71.88\n",
      "Iteration 250 | Train Loss  0.8501 | Classifier Accuracy 68.75\n",
      "Iteration 300 | Train Loss  0.6034 | Classifier Accuracy 78.12\n",
      "Iteration 350 | Train Loss  0.8682 | Classifier Accuracy 56.25\n",
      "Iteration 400 | Train Loss  0.5767 | Classifier Accuracy 81.25\n",
      "Iteration 450 | Train Loss  0.8013 | Classifier Accuracy 62.50\n",
      "\n",
      "[Summary] Elapsed time : 1 m 25 s\n",
      "Train Loss Mean 0.7569 | Accuracy 66.77 \n",
      "Valid Loss Mean 0.7478 | Accuracy 66.36 \n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.7143 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.7885 | Classifier Accuracy 59.38\n",
      "Iteration 100 | Train Loss  0.6872 | Classifier Accuracy 68.75\n",
      "Iteration 150 | Train Loss  0.6840 | Classifier Accuracy 71.88\n",
      "Iteration 200 | Train Loss  0.6005 | Classifier Accuracy 75.00\n",
      "Iteration 250 | Train Loss  1.0408 | Classifier Accuracy 50.00\n",
      "Iteration 300 | Train Loss  0.8944 | Classifier Accuracy 62.50\n",
      "Iteration 350 | Train Loss  0.6474 | Classifier Accuracy 75.00\n",
      "Iteration 400 | Train Loss  0.9201 | Classifier Accuracy 62.50\n",
      "Iteration 450 | Train Loss  0.8634 | Classifier Accuracy 62.50\n",
      "\n",
      "[Summary] Elapsed time : 1 m 26 s\n",
      "Train Loss Mean 0.7536 | Accuracy 66.98 \n",
      "Valid Loss Mean 0.7502 | Accuracy 67.38 \n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.7814 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.6683 | Classifier Accuracy 75.00\n",
      "Iteration 100 | Train Loss  0.6956 | Classifier Accuracy 68.75\n",
      "Iteration 150 | Train Loss  0.6891 | Classifier Accuracy 71.88\n",
      "Iteration 200 | Train Loss  0.5043 | Classifier Accuracy 87.50\n",
      "Iteration 250 | Train Loss  0.7241 | Classifier Accuracy 68.75\n",
      "Iteration 300 | Train Loss  0.6411 | Classifier Accuracy 75.00\n",
      "Iteration 350 | Train Loss  0.6410 | Classifier Accuracy 75.00\n",
      "Iteration 400 | Train Loss  0.5883 | Classifier Accuracy 75.00\n",
      "Iteration 450 | Train Loss  0.7529 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 25 s\n",
      "Train Loss Mean 0.7497 | Accuracy 67.09 \n",
      "Valid Loss Mean 0.7492 | Accuracy 66.91 \n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.6748 | Classifier Accuracy 68.75\n",
      "Iteration  50 | Train Loss  0.6694 | Classifier Accuracy 75.00\n",
      "Iteration 100 | Train Loss  0.8827 | Classifier Accuracy 53.12\n",
      "Iteration 150 | Train Loss  0.6870 | Classifier Accuracy 68.75\n",
      "Iteration 200 | Train Loss  0.7203 | Classifier Accuracy 65.62\n",
      "Iteration 250 | Train Loss  0.5687 | Classifier Accuracy 75.00\n",
      "Iteration 300 | Train Loss  0.8396 | Classifier Accuracy 59.38\n",
      "Iteration 350 | Train Loss  0.7054 | Classifier Accuracy 75.00\n",
      "Iteration 400 | Train Loss  0.5020 | Classifier Accuracy 75.00\n",
      "Iteration 450 | Train Loss  0.9103 | Classifier Accuracy 53.12\n",
      "\n",
      "[Summary] Elapsed time : 1 m 26 s\n",
      "Train Loss Mean 0.7526 | Accuracy 67.29 \n",
      "Valid Loss Mean 0.7461 | Accuracy 66.91 \n",
      "\n",
      " ====================== epoch 12 ======================\n",
      "Iteration   0 | Train Loss  0.8700 | Classifier Accuracy 59.38\n",
      "Iteration  50 | Train Loss  0.7755 | Classifier Accuracy 65.62\n",
      "Iteration 100 | Train Loss  0.6965 | Classifier Accuracy 65.62\n",
      "Iteration 150 | Train Loss  0.6863 | Classifier Accuracy 65.62\n",
      "Iteration 200 | Train Loss  0.6173 | Classifier Accuracy 75.00\n",
      "Iteration 250 | Train Loss  0.6838 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.8326 | Classifier Accuracy 59.38\n",
      "Iteration 350 | Train Loss  0.8891 | Classifier Accuracy 59.38\n",
      "Iteration 400 | Train Loss  0.7271 | Classifier Accuracy 68.75\n",
      "Iteration 450 | Train Loss  0.7458 | Classifier Accuracy 75.00\n",
      "\n",
      "[Summary] Elapsed time : 1 m 25 s\n",
      "Train Loss Mean 0.7527 | Accuracy 66.92 \n",
      "Valid Loss Mean 0.7422 | Accuracy 67.86 \n",
      "\n",
      " ====================== epoch 13 ======================\n",
      "Iteration   0 | Train Loss  0.8710 | Classifier Accuracy 53.12\n",
      "Iteration  50 | Train Loss  0.8301 | Classifier Accuracy 62.50\n",
      "Iteration 100 | Train Loss  1.0781 | Classifier Accuracy 56.25\n",
      "Iteration 150 | Train Loss  0.6772 | Classifier Accuracy 71.88\n",
      "Iteration 200 | Train Loss  0.8074 | Classifier Accuracy 56.25\n",
      "Iteration 250 | Train Loss  0.7717 | Classifier Accuracy 56.25\n",
      "Iteration 300 | Train Loss  0.5845 | Classifier Accuracy 68.75\n",
      "Iteration 350 | Train Loss  0.7972 | Classifier Accuracy 71.88\n",
      "Iteration 400 | Train Loss  0.6824 | Classifier Accuracy 71.88\n",
      "Iteration 450 | Train Loss  0.8428 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 26 s\n",
      "Train Loss Mean 0.7522 | Accuracy 67.15 \n",
      "Valid Loss Mean 0.7536 | Accuracy 67.57 \n",
      "\n",
      " ====================== epoch 14 ======================\n",
      "Iteration   0 | Train Loss  0.7293 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.8307 | Classifier Accuracy 68.75\n",
      "Iteration 100 | Train Loss  0.8870 | Classifier Accuracy 62.50\n",
      "Iteration 150 | Train Loss  0.7710 | Classifier Accuracy 68.75\n",
      "Iteration 200 | Train Loss  0.8662 | Classifier Accuracy 56.25\n",
      "Iteration 250 | Train Loss  0.7614 | Classifier Accuracy 75.00\n",
      "Iteration 300 | Train Loss  0.6541 | Classifier Accuracy 71.88\n",
      "Iteration 350 | Train Loss  0.7881 | Classifier Accuracy 68.75\n",
      "Iteration 400 | Train Loss  0.7413 | Classifier Accuracy 53.12\n",
      "Iteration 450 | Train Loss  0.5067 | Classifier Accuracy 87.50\n",
      "\n",
      "[Summary] Elapsed time : 1 m 26 s\n",
      "Train Loss Mean 0.7512 | Accuracy 67.05 \n",
      "Valid Loss Mean 0.7498 | Accuracy 67.15 \n",
      "\n",
      " ====================== epoch 15 ======================\n",
      "Iteration   0 | Train Loss  0.5612 | Classifier Accuracy 87.50\n",
      "Iteration  50 | Train Loss  0.6358 | Classifier Accuracy 71.88\n",
      "Iteration 100 | Train Loss  0.8925 | Classifier Accuracy 53.12\n",
      "Iteration 150 | Train Loss  0.5613 | Classifier Accuracy 78.12\n",
      "Iteration 200 | Train Loss  0.7579 | Classifier Accuracy 68.75\n",
      "Iteration 250 | Train Loss  0.7272 | Classifier Accuracy 59.38\n",
      "Iteration 300 | Train Loss  0.9262 | Classifier Accuracy 56.25\n",
      "Iteration 350 | Train Loss  0.9208 | Classifier Accuracy 43.75\n",
      "Iteration 400 | Train Loss  1.0164 | Classifier Accuracy 56.25\n",
      "Iteration 450 | Train Loss  0.7433 | Classifier Accuracy 71.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 25 s\n",
      "Train Loss Mean 0.7537 | Accuracy 66.86 \n",
      "Valid Loss Mean 0.7487 | Accuracy 66.70 \n",
      "\n",
      " ====================== epoch 16 ======================\n",
      "Iteration   0 | Train Loss  0.7205 | Classifier Accuracy 68.75\n",
      "Iteration  50 | Train Loss  0.7045 | Classifier Accuracy 71.88\n",
      "Iteration 100 | Train Loss  0.7552 | Classifier Accuracy 62.50\n",
      "Iteration 150 | Train Loss  0.6853 | Classifier Accuracy 65.62\n",
      "Iteration 200 | Train Loss  0.5804 | Classifier Accuracy 84.38\n",
      "Iteration 250 | Train Loss  0.5796 | Classifier Accuracy 75.00\n",
      "Iteration 300 | Train Loss  0.7626 | Classifier Accuracy 65.62\n",
      "Iteration 350 | Train Loss  0.6708 | Classifier Accuracy 71.88\n",
      "Iteration 400 | Train Loss  0.6663 | Classifier Accuracy 81.25\n",
      "Iteration 450 | Train Loss  0.7262 | Classifier Accuracy 68.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 23 s\n",
      "Train Loss Mean 0.7554 | Accuracy 66.71 \n",
      "Valid Loss Mean 0.7412 | Accuracy 67.31 \n",
      "\n",
      " ====================== epoch 17 ======================\n",
      "Iteration   0 | Train Loss  0.8095 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.6700 | Classifier Accuracy 65.62\n",
      "Iteration 100 | Train Loss  0.8388 | Classifier Accuracy 56.25\n",
      "Iteration 150 | Train Loss  0.9671 | Classifier Accuracy 53.12\n",
      "Iteration 200 | Train Loss  0.6775 | Classifier Accuracy 75.00\n",
      "Iteration 250 | Train Loss  0.7326 | Classifier Accuracy 71.88\n",
      "Iteration 300 | Train Loss  0.8476 | Classifier Accuracy 68.75\n",
      "Iteration 350 | Train Loss  0.6005 | Classifier Accuracy 78.12\n",
      "Iteration 400 | Train Loss  0.7836 | Classifier Accuracy 56.25\n",
      "Iteration 450 | Train Loss  0.6684 | Classifier Accuracy 78.12\n",
      "\n",
      "[Summary] Elapsed time : 1 m 23 s\n",
      "Train Loss Mean 0.7512 | Accuracy 67.07 \n",
      "Valid Loss Mean 0.7508 | Accuracy 66.83 \n",
      "\n",
      " ====================== epoch 18 ======================\n",
      "Iteration   0 | Train Loss  0.6942 | Classifier Accuracy 71.88\n",
      "Iteration  50 | Train Loss  0.9420 | Classifier Accuracy 65.62\n",
      "Iteration 100 | Train Loss  0.7294 | Classifier Accuracy 68.75\n",
      "Iteration 150 | Train Loss  0.8560 | Classifier Accuracy 46.88\n",
      "Iteration 200 | Train Loss  0.9022 | Classifier Accuracy 65.62\n",
      "Iteration 250 | Train Loss  1.0269 | Classifier Accuracy 50.00\n",
      "Iteration 300 | Train Loss  0.5685 | Classifier Accuracy 81.25\n",
      "Iteration 350 | Train Loss  0.9931 | Classifier Accuracy 53.12\n",
      "Iteration 400 | Train Loss  0.9237 | Classifier Accuracy 56.25\n",
      "Iteration 450 | Train Loss  0.6384 | Classifier Accuracy 75.00\n",
      "\n",
      "[Summary] Elapsed time : 1 m 25 s\n",
      "Train Loss Mean 0.7518 | Accuracy 67.10 \n",
      "Valid Loss Mean 0.7441 | Accuracy 68.04 \n",
      "\n",
      " ====================== epoch 19 ======================\n",
      "Iteration   0 | Train Loss  0.6956 | Classifier Accuracy 59.38\n",
      "Iteration  50 | Train Loss  0.8989 | Classifier Accuracy 53.12\n",
      "Iteration 100 | Train Loss  0.6279 | Classifier Accuracy 68.75\n",
      "Iteration 150 | Train Loss  0.6497 | Classifier Accuracy 81.25\n",
      "Iteration 200 | Train Loss  0.6244 | Classifier Accuracy 68.75\n",
      "Iteration 250 | Train Loss  0.6324 | Classifier Accuracy 71.88\n",
      "Iteration 300 | Train Loss  0.9299 | Classifier Accuracy 50.00\n",
      "Iteration 350 | Train Loss  0.6908 | Classifier Accuracy 78.12\n",
      "Iteration 400 | Train Loss  0.6119 | Classifier Accuracy 71.88\n",
      "Iteration 450 | Train Loss  0.8409 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 24 s\n",
      "Train Loss Mean 0.7502 | Accuracy 67.27 \n",
      "Valid Loss Mean 0.7403 | Accuracy 66.81 \n",
      "\n",
      " ====================== epoch 20 ======================\n",
      "Iteration   0 | Train Loss  0.9101 | Classifier Accuracy 59.38\n",
      "Iteration  50 | Train Loss  0.8207 | Classifier Accuracy 65.62\n",
      "Iteration 100 | Train Loss  0.8371 | Classifier Accuracy 65.62\n",
      "Iteration 150 | Train Loss  0.6276 | Classifier Accuracy 75.00\n",
      "Iteration 200 | Train Loss  0.8830 | Classifier Accuracy 68.75\n",
      "Iteration 250 | Train Loss  1.0878 | Classifier Accuracy 53.12\n",
      "Iteration 300 | Train Loss  0.8128 | Classifier Accuracy 68.75\n",
      "Iteration 350 | Train Loss  0.8195 | Classifier Accuracy 59.38\n",
      "Iteration 400 | Train Loss  0.8692 | Classifier Accuracy 43.75\n",
      "Iteration 450 | Train Loss  0.6872 | Classifier Accuracy 75.00\n",
      "\n",
      "[Summary] Elapsed time : 1 m 26 s\n",
      "Train Loss Mean 0.7505 | Accuracy 67.14 \n",
      "Valid Loss Mean 0.7367 | Accuracy 67.46 \n",
      "\n",
      " ====================== epoch 21 ======================\n",
      "Iteration   0 | Train Loss  0.8341 | Classifier Accuracy 50.00\n",
      "Iteration  50 | Train Loss  0.7485 | Classifier Accuracy 65.62\n",
      "Iteration 100 | Train Loss  0.6969 | Classifier Accuracy 75.00\n",
      "Iteration 150 | Train Loss  0.7177 | Classifier Accuracy 78.12\n",
      "Iteration 200 | Train Loss  0.5892 | Classifier Accuracy 81.25\n",
      "Iteration 250 | Train Loss  0.6950 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.7953 | Classifier Accuracy 59.38\n",
      "Iteration 350 | Train Loss  0.6829 | Classifier Accuracy 75.00\n",
      "Iteration 400 | Train Loss  0.7931 | Classifier Accuracy 59.38\n",
      "Iteration 450 | Train Loss  0.7035 | Classifier Accuracy 68.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 24 s\n",
      "Train Loss Mean 0.7481 | Accuracy 67.63 \n",
      "Valid Loss Mean 0.7397 | Accuracy 68.15 \n",
      "\n",
      " ====================== epoch 22 ======================\n",
      "Iteration   0 | Train Loss  0.8254 | Classifier Accuracy 71.88\n",
      "Iteration  50 | Train Loss  0.6012 | Classifier Accuracy 71.88\n",
      "Iteration 100 | Train Loss  0.7678 | Classifier Accuracy 59.38\n",
      "Iteration 150 | Train Loss  0.7945 | Classifier Accuracy 56.25\n",
      "Iteration 200 | Train Loss  0.9939 | Classifier Accuracy 46.88\n",
      "Iteration 250 | Train Loss  0.8312 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.6754 | Classifier Accuracy 75.00\n",
      "Iteration 350 | Train Loss  0.4857 | Classifier Accuracy 87.50\n",
      "Iteration 400 | Train Loss  0.8875 | Classifier Accuracy 65.62\n",
      "Iteration 450 | Train Loss  0.5599 | Classifier Accuracy 81.25\n",
      "\n",
      "[Summary] Elapsed time : 1 m 24 s\n",
      "Train Loss Mean 0.7496 | Accuracy 67.42 \n",
      "Valid Loss Mean 0.7443 | Accuracy 66.96 \n",
      "\n",
      " ====================== epoch 23 ======================\n",
      "Iteration   0 | Train Loss  0.7615 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.7703 | Classifier Accuracy 68.75\n",
      "Iteration 100 | Train Loss  0.6975 | Classifier Accuracy 59.38\n",
      "Iteration 150 | Train Loss  0.4690 | Classifier Accuracy 87.50\n",
      "Iteration 200 | Train Loss  0.6891 | Classifier Accuracy 75.00\n",
      "Iteration 250 | Train Loss  0.9730 | Classifier Accuracy 50.00\n",
      "Iteration 300 | Train Loss  0.7238 | Classifier Accuracy 59.38\n",
      "Iteration 350 | Train Loss  0.7206 | Classifier Accuracy 68.75\n",
      "Iteration 400 | Train Loss  0.9989 | Classifier Accuracy 46.88\n",
      "Iteration 450 | Train Loss  0.7066 | Classifier Accuracy 62.50\n",
      "\n",
      "[Summary] Elapsed time : 1 m 24 s\n",
      "Train Loss Mean 0.7507 | Accuracy 66.99 \n",
      "Valid Loss Mean 0.7385 | Accuracy 68.09 \n",
      "\n",
      " ====================== epoch 24 ======================\n",
      "Iteration   0 | Train Loss  0.5893 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.7422 | Classifier Accuracy 68.75\n",
      "Iteration 100 | Train Loss  0.4809 | Classifier Accuracy 87.50\n",
      "Iteration 150 | Train Loss  0.7427 | Classifier Accuracy 68.75\n",
      "Iteration 200 | Train Loss  0.6291 | Classifier Accuracy 78.12\n",
      "Iteration 250 | Train Loss  0.7241 | Classifier Accuracy 71.88\n",
      "Iteration 300 | Train Loss  0.6676 | Classifier Accuracy 71.88\n",
      "Iteration 350 | Train Loss  0.6667 | Classifier Accuracy 65.62\n",
      "Iteration 400 | Train Loss  0.8795 | Classifier Accuracy 62.50\n",
      "Iteration 450 | Train Loss  0.7169 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 25 s\n",
      "Train Loss Mean 0.7465 | Accuracy 67.75 \n",
      "Valid Loss Mean 0.7392 | Accuracy 67.49 \n",
      "\n",
      " ====================== epoch 25 ======================\n",
      "Iteration   0 | Train Loss  0.9500 | Classifier Accuracy 53.12\n",
      "Iteration  50 | Train Loss  0.8292 | Classifier Accuracy 53.12\n",
      "Iteration 100 | Train Loss  0.6055 | Classifier Accuracy 62.50\n",
      "Iteration 150 | Train Loss  0.6147 | Classifier Accuracy 68.75\n",
      "Iteration 200 | Train Loss  0.7485 | Classifier Accuracy 78.12\n",
      "Iteration 250 | Train Loss  0.8706 | Classifier Accuracy 65.62\n",
      "Iteration 300 | Train Loss  0.8675 | Classifier Accuracy 65.62\n",
      "Iteration 350 | Train Loss  0.6493 | Classifier Accuracy 71.88\n",
      "Iteration 400 | Train Loss  0.6429 | Classifier Accuracy 75.00\n",
      "Iteration 450 | Train Loss  0.6790 | Classifier Accuracy 68.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 25 s\n",
      "Train Loss Mean 0.7480 | Accuracy 67.54 \n",
      "Valid Loss Mean 0.7453 | Accuracy 66.91 \n",
      "\n",
      "EARLY STOPPING!!\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(age_model.parameters(), lr=1e-3)\n",
    "lr_sched = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "age_model.train()\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "\n",
    "    # train\n",
    "    for i, (images, targets) in enumerate(age_train) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = age_model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 50 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    for i, (images, targets) in enumerate(age_valid) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = age_model(images)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/32 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "\n",
    "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        age_best_model = age_model\n",
    "        path = './age_model/'\n",
    "        torch.save(best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    lr_sched.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e82b1-b9c3-42f0-897e-19af9b7f215d",
   "metadata": {},
   "source": [
    "### 마스크 착용 여부 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "mask_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        scores = best_model(images)\n",
    "        preds = scores.argmax(dim=-1)\n",
    "        mask_predictions.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f517ec71-7f0f-44df-8faf-1b7de4029309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mask_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "08602539-d228-4c17-a3f6-e7a8eac64759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 12600})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(mask_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232e96e-1ffe-4c8a-a760-ba31035b6caa",
   "metadata": {},
   "source": [
    "* 마스크는 그냥 Wear 로 예측하도록 학습된 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81500c1-1e60-46ff-9e63-b8c05ff09bf3",
   "metadata": {},
   "source": [
    "### 성별 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e5551462-a408-483e-b040-fff46d83af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_best_model.eval()\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "gender_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        scores = gender_best_model(images)\n",
    "        preds = scores.argmax(dim=-1)\n",
    "        gender_predictions.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "36a2b5a9-d291-4a1a-9468-eeac6e7caba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gender_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "450fc99a-50b5-46fd-9ddb-ac5bd7235e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 11160, 0: 1440})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(gender_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d483ed8-1524-4edf-8258-96cf00eac3bf",
   "metadata": {},
   "source": [
    "* 성별은 학습 데이터셋의 여성/남성 비율 차이에 비해 너무 여성으로만 예측한 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7214cbc-cdc6-4eb2-be16-4746b793111e",
   "metadata": {},
   "source": [
    "### 나이 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fbdd678d-6aed-4052-9e1b-ebd8e00cc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_best_model.eval()\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "age_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        scores = age_best_model(images)\n",
    "        preds = scores.argmax(dim=-1)\n",
    "        age_predictions.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f127e450-98dd-48b1-8667-65097b3554b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(age_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f8bcef6b-f516-4db3-a91f-fd4dc4f567c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 8775, 0: 3825})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(age_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863b80b-a5c5-4349-962f-d01c19ad8a7c",
   "metadata": {},
   "source": [
    "* 60대 이상은 예측되지 않았다.\n",
    "* 이상하게 30대 이하가 많이 나올 줄 알았는데 30대 이상 60대 미만이 많이 나왔다.\n",
    "* 범위 조절을 잘못했을 가능성도 있을 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa2ad0-daf6-4755-b067-2f3942c85871",
   "metadata": {},
   "source": [
    "### 최종 클래스 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4f1fb527-dd61-4841-b3e6-b97e527e5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "size = len(submission)\n",
    "class_map = np.array([[[0, 1, 2],\n",
    "                       [3, 4, 5]],\n",
    "                      [[6, 7, 8],\n",
    "                       [9, 10, 11]],\n",
    "                      [[12, 13, 14],\n",
    "                       [15, 16, 17]]])\n",
    "for idx in range(size):\n",
    "    i = mask_predictions[idx]\n",
    "    j = gender_predictions[idx]\n",
    "    k = age_predictions[idx]\n",
    "    all_predictions.append(class_map[i][j][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cefb99f9-5db8-4091-aba4-692a0781a219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cdc01aee-aee3-48cc-9c5d-6f9e4144f8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 8382, 3: 2778, 0: 1047, 1: 393})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(all_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2341d3-b450-47ca-a527-516df3c24fea",
   "metadata": {},
   "source": [
    "* 충격적이다... 4개 클래스만 나오다니..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "verbal-sample",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission_baseline.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0973a2e-0393-4ba7-b392-4b89e054eeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
