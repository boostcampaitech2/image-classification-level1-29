{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os, glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "from torchsummary import summary\n",
    "import torch_optimizer as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using device: cuda:0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 학습 데이터셋 폴더 경로를 지정해주세요.\n",
    "train_dir = '/opt/ml/input/data/train'\n",
    "trainimage_dir = os.path.join(train_dir, 'images')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "train_df = pd.read_csv(os.path.join(train_dir, 'train.csv'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "masks = ['mask1', 'mask2', 'mask3', 'mask4', 'mask5', 'incorrect_mask', 'normal']\n",
    "wears = ['Wear', 'Wear', 'Wear', 'Wear', 'Wear', 'Incorrect', 'Not Wear']\n",
    "mask_df = pd.DataFrame()\n",
    "for person in train_df.values:\n",
    "    for mask, wear in zip(masks, wears):\n",
    "        mask_df = mask_df.append(pd.Series(np.append(person, (mask, wear))), ignore_index=True)\n",
    "mask_df.columns = np.append(train_df.columns.values, ('mask', 'wear'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\n",
    "mask_df = mask_df.sample(frac=1).reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train, valid = train_test_split(mask_df, test_size=0.2, stratify=mask_df['wear'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(384),\n",
    "    Resize((224, 224), Image.BILINEAR),\n",
    "    \n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class GenderDataset(Dataset):\n",
    "    def __init__(self, path, mask_df, transform):\n",
    "        super(GenderDataset).__init__()\n",
    "        self.path = path\n",
    "        self.mask_df = mask_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        full_path = os.path.join(self.path, self.mask_df.iloc[idx]['path'])\n",
    "        img_list = glob.glob(full_path + '/*')\n",
    "        file_name = self.mask_df.iloc[idx]['mask']\n",
    "        for img_name in img_list:\n",
    "            if img_name.startswith(file_name):\n",
    "                break\n",
    "        image = Image.open(os.path.join(full_path, img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.mask_df.iloc[idx]['gender']\n",
    "        label = 0 if label=='male' else 1\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mask_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "gender_train_data = GenderDataset(trainimage_dir, train, transform)\n",
    "gender_valid_data = GenderDataset(trainimage_dir, valid, transform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "batch_size = 128"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "gender_train = DataLoader(gender_train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "gender_valid = DataLoader(gender_valid_data, batch_size=batch_size, shuffle=True, num_workers=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, path, mask_df, transform):\n",
    "        super(AgeDataset).__init__()\n",
    "        self.path = path\n",
    "        self.mask_df = mask_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        full_path = os.path.join(self.path, self.mask_df.iloc[idx]['path'])\n",
    "        img_list = glob.glob(full_path + '/*')\n",
    "        file_name = self.mask_df.iloc[idx]['mask']\n",
    "        for img_name in img_list:\n",
    "            if img_name.startswith(file_name):\n",
    "                break\n",
    "        image = Image.open(os.path.join(full_path, img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.mask_df.iloc[idx]['age']\n",
    "        if label >= 60.0:\n",
    "            label = 2\n",
    "        elif label >= 30.0:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mask_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "age_train_data = AgeDataset(trainimage_dir, train, transform)\n",
    "age_valid_data = AgeDataset(trainimage_dir, valid, transform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "age_train = DataLoader(age_train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "age_valid = DataLoader(age_valid_data, batch_size=batch_size, shuffle=True, num_workers=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, path, mask_df, transform):\n",
    "        super(MaskDataset).__init__()\n",
    "        self.path = path\n",
    "        self.mask_df = mask_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        full_path = os.path.join(self.path, self.mask_df.iloc[idx]['path'])\n",
    "        img_list = glob.glob(full_path + '/*')\n",
    "        file_name = self.mask_df.iloc[idx]['mask']\n",
    "        for img_name in img_list:\n",
    "            if img_name.startswith(file_name):\n",
    "                break\n",
    "        image = Image.open(os.path.join(full_path, img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.mask_df.iloc[idx]['mask']\n",
    "        if label.startswith('mask'):\n",
    "            label = 0\n",
    "        elif label.startswith('incorrect'):\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 2\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mask_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "mask_train_data = MaskDataset(trainimage_dir, train, transform)\n",
    "mask_valid_data = MaskDataset(trainimage_dir, valid, transform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "mask_train = DataLoader(mask_train_data, batch_size=batch_size, shuffle=True)\n",
    "mask_valid = DataLoader(mask_valid_data, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        super(TestDataset).__init__()\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "test_dir = '/opt/ml/input/data/eval'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "testimage_dir = os.path.join(test_dir, 'images')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(testimage_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((224, 224), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    \n",
    "])\n",
    "test_dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "\n",
    "model = resnet50(pretrained=True, progress=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = resnet50(pretrained=True, progress=False)\n",
    "        \n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "mask_model = MyModel(num_classes=3)\n",
    "for param in mask_model.parameters():\n",
    "    param.requires_grad = False # frozen\n",
    "for param in mask_model.model.fc.parameters():\n",
    "    param.requires_grad = True # 마지막 레이어 살리기\n",
    "mask_model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "gender_model = MyModel(num_classes=2)\n",
    "for param in gender_model.parameters():\n",
    "    param.requires_grad = False # frozen\n",
    "for param in gender_model.model.fc.parameters():\n",
    "    param.requires_grad = True # 마지막 레이어 살리기\n",
    "gender_model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "\n",
    "age_model = MyModel(num_classes=3)\n",
    "for param in age_model.parameters():\n",
    "    param.requires_grad = False # frozen\n",
    "for param in age_model.model.fc.parameters():\n",
    "    param.requires_grad = True # 마지막 레이어 살리기\n",
    "age_model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "learning_rate = 0.001\n",
    "betas = (0.9, 0.999)\n",
    "weight_decay = 1e-4\n",
    "T_max = 50"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "num_epochs = 30\n",
    "optimizer = torch.optim.Adam(mask_model.parameters(), lr = 0.01)\n",
    "lr_sched  = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, \n",
    "milestones=[int(num_epochs * 0.5), int(num_epochs * 0.75)], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    \n",
    "\n",
    "    # train\n",
    "    mask_model.train()\n",
    "    for i, (images, targets) in enumerate(mask_train) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = mask_model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_sched.step()\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/128 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 50 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    mask_model.eval()\n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    for i, (images, targets) in enumerate(mask_valid) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            scores = mask_model(images)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/128 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "\n",
    "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        mask_best_model = mask_model\n",
    "        path = './mask_model/'\n",
    "        torch.save(mask_best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " ====================== epoch 1 ======================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration   0 | Train Loss  1.3505 | Classifier Accuracy 10.94\n",
      "Iteration  50 | Train Loss  3.0722 | Classifier Accuracy 72.66\n",
      "Iteration 100 | Train Loss  2.2908 | Classifier Accuracy 70.31\n",
      "\n",
      "[Summary] Elapsed time : 2 m 1 s\n",
      "Train Loss Mean 2.6181 | Accuracy 68.75 \n",
      "Valid Loss Mean 1.6289 | Accuracy 70.31 \n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  1.8898 | Classifier Accuracy 67.97\n",
      "Iteration  50 | Train Loss  0.7677 | Classifier Accuracy 74.22\n",
      "Iteration 100 | Train Loss  0.7868 | Classifier Accuracy 72.66\n",
      "\n",
      "[Summary] Elapsed time : 2 m 31 s\n",
      "Train Loss Mean 0.9757 | Accuracy 70.89 \n",
      "Valid Loss Mean 0.8215 | Accuracy 70.31 \n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.7486 | Classifier Accuracy 75.00\n",
      "Iteration  50 | Train Loss  0.7588 | Classifier Accuracy 72.66\n",
      "Iteration 100 | Train Loss  0.7989 | Classifier Accuracy 71.88\n",
      "\n",
      "[Summary] Elapsed time : 2 m 33 s\n",
      "Train Loss Mean 0.8103 | Accuracy 70.90 \n",
      "Valid Loss Mean 0.8183 | Accuracy 70.31 \n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.7535 | Classifier Accuracy 75.78\n",
      "Iteration  50 | Train Loss  0.8614 | Classifier Accuracy 67.19\n",
      "Iteration 100 | Train Loss  0.8183 | Classifier Accuracy 69.53\n",
      "\n",
      "[Summary] Elapsed time : 2 m 31 s\n",
      "Train Loss Mean 0.8062 | Accuracy 70.90 \n",
      "Valid Loss Mean 0.8141 | Accuracy 70.31 \n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.7257 | Classifier Accuracy 75.00\n",
      "Iteration  50 | Train Loss  0.8273 | Classifier Accuracy 69.53\n",
      "Iteration 100 | Train Loss  0.8735 | Classifier Accuracy 67.19\n",
      "\n",
      "[Summary] Elapsed time : 2 m 30 s\n",
      "Train Loss Mean 0.8037 | Accuracy 70.90 \n",
      "Valid Loss Mean 0.8165 | Accuracy 70.31 \n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.7224 | Classifier Accuracy 75.00\n",
      "Iteration  50 | Train Loss  0.8756 | Classifier Accuracy 67.19\n",
      "Iteration 100 | Train Loss  0.8397 | Classifier Accuracy 69.53\n",
      "\n",
      "[Summary] Elapsed time : 2 m 33 s\n",
      "Train Loss Mean 0.8004 | Accuracy 70.90 \n",
      "Valid Loss Mean 0.8141 | Accuracy 70.31 \n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.8063 | Classifier Accuracy 71.09\n",
      "Iteration  50 | Train Loss  0.8194 | Classifier Accuracy 70.31\n",
      "Iteration 100 | Train Loss  0.6596 | Classifier Accuracy 78.91\n",
      "\n",
      "[Summary] Elapsed time : 2 m 33 s\n",
      "Train Loss Mean 0.8036 | Accuracy 70.90 \n",
      "Valid Loss Mean 0.8178 | Accuracy 70.31 \n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.7452 | Classifier Accuracy 75.00\n",
      "Iteration  50 | Train Loss  0.8267 | Classifier Accuracy 69.53\n",
      "Iteration 100 | Train Loss  0.9093 | Classifier Accuracy 64.84\n",
      "\n",
      "[Summary] Elapsed time : 2 m 19 s\n",
      "Train Loss Mean 0.8008 | Accuracy 70.90 \n",
      "Valid Loss Mean 0.8123 | Accuracy 70.31 \n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.8821 | Classifier Accuracy 65.62\n",
      "Iteration  50 | Train Loss  0.8343 | Classifier Accuracy 70.31\n",
      "Iteration 100 | Train Loss  0.7691 | Classifier Accuracy 73.44\n",
      "\n",
      "[Summary] Elapsed time : 2 m 10 s\n",
      "Train Loss Mean 0.8016 | Accuracy 70.90 \n",
      "Valid Loss Mean 0.8113 | Accuracy 70.31 \n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.7956 | Classifier Accuracy 71.09\n",
      "Iteration  50 | Train Loss  0.8177 | Classifier Accuracy 70.31\n",
      "Iteration 100 | Train Loss  0.8958 | Classifier Accuracy 65.62\n",
      "\n",
      "[Summary] Elapsed time : 2 m 10 s\n",
      "Train Loss Mean 0.8010 | Accuracy 70.90 \n",
      "Valid Loss Mean 0.8116 | Accuracy 70.31 \n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.7220 | Classifier Accuracy 76.56\n",
      "Iteration  50 | Train Loss  0.9389 | Classifier Accuracy 63.28\n",
      "Iteration 100 | Train Loss  0.7031 | Classifier Accuracy 78.12\n",
      "\n",
      "[Summary] Elapsed time : 2 m 14 s\n",
      "Train Loss Mean 0.8016 | Accuracy 70.90 \n",
      "Valid Loss Mean 0.8148 | Accuracy 70.31 \n",
      "\n",
      " ====================== epoch 12 ======================\n",
      "Iteration   0 | Train Loss  0.7420 | Classifier Accuracy 75.78\n",
      "Iteration  50 | Train Loss  0.9063 | Classifier Accuracy 64.84\n",
      "Iteration 100 | Train Loss  0.8303 | Classifier Accuracy 70.31\n",
      "\n",
      "[Summary] Elapsed time : 2 m 13 s\n",
      "Train Loss Mean 0.8004 | Accuracy 70.90 \n",
      "Valid Loss Mean 0.8151 | Accuracy 70.31 \n",
      "\n",
      " ====================== epoch 13 ======================\n",
      "Iteration   0 | Train Loss  0.7568 | Classifier Accuracy 73.44\n",
      "Iteration  50 | Train Loss  0.8390 | Classifier Accuracy 68.75\n",
      "Iteration 100 | Train Loss  0.7878 | Classifier Accuracy 71.88\n",
      "\n",
      "[Summary] Elapsed time : 2 m 12 s\n",
      "Train Loss Mean 0.8006 | Accuracy 70.90 \n",
      "Valid Loss Mean 0.8175 | Accuracy 70.31 \n",
      "\n",
      " ====================== epoch 14 ======================\n",
      "Iteration   0 | Train Loss  0.7736 | Classifier Accuracy 72.66\n",
      "Iteration  50 | Train Loss  0.7396 | Classifier Accuracy 75.00\n",
      "Iteration 100 | Train Loss  0.9802 | Classifier Accuracy 61.72\n",
      "\n",
      "[Summary] Elapsed time : 2 m 14 s\n",
      "Train Loss Mean 0.8020 | Accuracy 70.90 \n",
      "Valid Loss Mean 0.8203 | Accuracy 70.31 \n",
      "\n",
      "EARLY STOPPING!!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "\n",
    "\n",
    "optimizer = optim.RAdam(gender_model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=T_max)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "\n",
    "    # train\n",
    "    gender_model.train()\n",
    "    for i, (images, targets) in enumerate(gender_train) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = gender_model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/128 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 50 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    gender_model.eval()\n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    for i, (images, targets) in enumerate(gender_valid) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = gender_model(images)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/128 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "\n",
    "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        gender_best_model = gender_model\n",
    "        path = '/opt/ml/teamrepo/kbs/gender_model/'\n",
    "        torch.save(gender_best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    lr_sched.step()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  0.2214 | Classifier Accuracy 91.41\n",
      "Iteration  50 | Train Loss  0.1943 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.1788 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 0 m 48 s\n",
      "Train Loss Mean 0.1781 | Accuracy 93.21 \n",
      "Valid Loss Mean 0.1776 | Accuracy 91.82 \n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.1528 | Classifier Accuracy 94.53\n",
      "Iteration  50 | Train Loss  0.1993 | Classifier Accuracy 92.19\n",
      "Iteration 100 | Train Loss  0.1977 | Classifier Accuracy 92.19\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1785 | Accuracy 93.09 \n",
      "Valid Loss Mean 0.1672 | Accuracy 92.42 \n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.1877 | Classifier Accuracy 97.66\n",
      "Iteration  50 | Train Loss  0.1374 | Classifier Accuracy 96.09\n",
      "Iteration 100 | Train Loss  0.2037 | Classifier Accuracy 91.41\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1644 | Accuracy 93.66 \n",
      "Valid Loss Mean 0.1608 | Accuracy 92.68 \n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.1916 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.2240 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.2114 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1647 | Accuracy 93.46 \n",
      "Valid Loss Mean 0.1658 | Accuracy 92.66 \n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.1272 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.1905 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.1265 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.1549 | Accuracy 93.86 \n",
      "Valid Loss Mean 0.1572 | Accuracy 93.31 \n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.1260 | Classifier Accuracy 97.66\n",
      "Iteration  50 | Train Loss  0.1677 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.1903 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1479 | Accuracy 94.20 \n",
      "Valid Loss Mean 0.1475 | Accuracy 93.05 \n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.1137 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1288 | Classifier Accuracy 96.09\n",
      "Iteration 100 | Train Loss  0.1332 | Classifier Accuracy 92.19\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1440 | Accuracy 94.20 \n",
      "Valid Loss Mean 0.1394 | Accuracy 93.44 \n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.1097 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1316 | Classifier Accuracy 96.09\n",
      "Iteration 100 | Train Loss  0.1499 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1418 | Accuracy 94.30 \n",
      "Valid Loss Mean 0.1392 | Accuracy 94.11 \n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.0998 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.2180 | Classifier Accuracy 90.62\n",
      "Iteration 100 | Train Loss  0.1192 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.1368 | Accuracy 94.57 \n",
      "Valid Loss Mean 0.1298 | Accuracy 94.11 \n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.1329 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.1636 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.0801 | Classifier Accuracy 98.44\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1317 | Accuracy 94.83 \n",
      "Valid Loss Mean 0.1276 | Accuracy 94.40 \n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.1109 | Classifier Accuracy 92.97\n",
      "Iteration  50 | Train Loss  0.1746 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.1332 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1282 | Accuracy 94.94 \n",
      "Valid Loss Mean 0.1325 | Accuracy 93.46 \n",
      "\n",
      " ====================== epoch 12 ======================\n",
      "Iteration   0 | Train Loss  0.1413 | Classifier Accuracy 94.53\n",
      "Iteration  50 | Train Loss  0.0922 | Classifier Accuracy 97.66\n",
      "Iteration 100 | Train Loss  0.1350 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1215 | Accuracy 95.31 \n",
      "Valid Loss Mean 0.1219 | Accuracy 94.51 \n",
      "\n",
      " ====================== epoch 13 ======================\n",
      "Iteration   0 | Train Loss  0.1179 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.1430 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.0999 | Classifier Accuracy 98.44\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1208 | Accuracy 95.19 \n",
      "Valid Loss Mean 0.1188 | Accuracy 94.27 \n",
      "\n",
      " ====================== epoch 14 ======================\n",
      "Iteration   0 | Train Loss  0.1557 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.0934 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.0925 | Classifier Accuracy 97.66\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1183 | Accuracy 95.23 \n",
      "Valid Loss Mean 0.1286 | Accuracy 94.01 \n",
      "\n",
      " ====================== epoch 15 ======================\n",
      "Iteration   0 | Train Loss  0.1327 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.1190 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.0874 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1168 | Accuracy 95.34 \n",
      "Valid Loss Mean 0.1361 | Accuracy 93.52 \n",
      "\n",
      " ====================== epoch 16 ======================\n",
      "Iteration   0 | Train Loss  0.1480 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.1059 | Classifier Accuracy 96.09\n",
      "Iteration 100 | Train Loss  0.0750 | Classifier Accuracy 97.66\n",
      "\n",
      "[Summary] Elapsed time : 0 m 59 s\n",
      "Train Loss Mean 0.1120 | Accuracy 95.50 \n",
      "Valid Loss Mean 0.1133 | Accuracy 94.69 \n",
      "\n",
      " ====================== epoch 17 ======================\n",
      "Iteration   0 | Train Loss  0.1207 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.0816 | Classifier Accuracy 97.66\n",
      "Iteration 100 | Train Loss  0.1181 | Classifier Accuracy 97.66\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1089 | Accuracy 95.77 \n",
      "Valid Loss Mean 0.1083 | Accuracy 94.87 \n",
      "\n",
      " ====================== epoch 18 ======================\n",
      "Iteration   0 | Train Loss  0.0865 | Classifier Accuracy 97.66\n",
      "Iteration  50 | Train Loss  0.1169 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1124 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1060 | Accuracy 95.82 \n",
      "Valid Loss Mean 0.1098 | Accuracy 94.51 \n",
      "\n",
      " ====================== epoch 19 ======================\n",
      "Iteration   0 | Train Loss  0.1394 | Classifier Accuracy 94.53\n",
      "Iteration  50 | Train Loss  0.1482 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.1250 | Classifier Accuracy 95.31\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.1032 | Accuracy 95.92 \n",
      "Valid Loss Mean 0.1087 | Accuracy 94.95 \n",
      "\n",
      " ====================== epoch 20 ======================\n",
      "Iteration   0 | Train Loss  0.0937 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0960 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.1635 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.1057 | Accuracy 95.77 \n",
      "Valid Loss Mean 0.1212 | Accuracy 94.30 \n",
      "\n",
      " ====================== epoch 21 ======================\n",
      "Iteration   0 | Train Loss  0.0591 | Classifier Accuracy 99.22\n",
      "Iteration  50 | Train Loss  0.0699 | Classifier Accuracy 98.44\n",
      "Iteration 100 | Train Loss  0.1430 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1006 | Accuracy 95.98 \n",
      "Valid Loss Mean 0.1079 | Accuracy 94.90 \n",
      "\n",
      " ====================== epoch 22 ======================\n",
      "Iteration   0 | Train Loss  0.0969 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.1029 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.0982 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.0994 | Accuracy 96.07 \n",
      "Valid Loss Mean 0.1040 | Accuracy 95.10 \n",
      "\n",
      " ====================== epoch 23 ======================\n",
      "Iteration   0 | Train Loss  0.1320 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.1019 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.1396 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.0971 | Accuracy 96.11 \n",
      "Valid Loss Mean 0.1044 | Accuracy 94.77 \n",
      "\n",
      " ====================== epoch 24 ======================\n",
      "Iteration   0 | Train Loss  0.1799 | Classifier Accuracy 92.19\n",
      "Iteration  50 | Train Loss  0.0926 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1346 | Classifier Accuracy 95.31\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.0973 | Accuracy 96.09 \n",
      "Valid Loss Mean 0.0986 | Accuracy 95.60 \n",
      "\n",
      " ====================== epoch 25 ======================\n",
      "Iteration   0 | Train Loss  0.0676 | Classifier Accuracy 97.66\n",
      "Iteration  50 | Train Loss  0.0786 | Classifier Accuracy 97.66\n",
      "Iteration 100 | Train Loss  0.0739 | Classifier Accuracy 98.44\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.0968 | Accuracy 96.31 \n",
      "Valid Loss Mean 0.1009 | Accuracy 94.84 \n",
      "\n",
      " ====================== epoch 26 ======================\n",
      "Iteration   0 | Train Loss  0.0614 | Classifier Accuracy 98.44\n",
      "Iteration  50 | Train Loss  0.0471 | Classifier Accuracy 100.00\n",
      "Iteration 100 | Train Loss  0.1483 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.0992 | Accuracy 96.03 \n",
      "Valid Loss Mean 0.0984 | Accuracy 95.29 \n",
      "\n",
      " ====================== epoch 27 ======================\n",
      "Iteration   0 | Train Loss  0.0848 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.1325 | Classifier Accuracy 96.09\n",
      "Iteration 100 | Train Loss  0.0920 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.0928 | Accuracy 96.36 \n",
      "Valid Loss Mean 0.1064 | Accuracy 94.56 \n",
      "\n",
      " ====================== epoch 28 ======================\n",
      "Iteration   0 | Train Loss  0.0670 | Classifier Accuracy 98.44\n",
      "Iteration  50 | Train Loss  0.1374 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.1264 | Classifier Accuracy 95.31\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.0931 | Accuracy 96.34 \n",
      "Valid Loss Mean 0.0979 | Accuracy 95.18 \n",
      "\n",
      " ====================== epoch 29 ======================\n",
      "Iteration   0 | Train Loss  0.0759 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.0981 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.0771 | Classifier Accuracy 97.66\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.0895 | Accuracy 96.37 \n",
      "Valid Loss Mean 0.0984 | Accuracy 95.18 \n",
      "\n",
      " ====================== epoch 30 ======================\n",
      "Iteration   0 | Train Loss  0.0986 | Classifier Accuracy 97.66\n",
      "Iteration  50 | Train Loss  0.0987 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.0858 | Classifier Accuracy 97.66\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.0944 | Accuracy 96.11 \n",
      "Valid Loss Mean 0.0947 | Accuracy 95.23 \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "optimizer = optim.RAdam(age_model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=T_max)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "\n",
    "    # train\n",
    "    age_model.train()\n",
    "    for i, (images, targets) in enumerate(age_train) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = age_model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/128 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 50 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    age_model.eval()\n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    for i, (images, targets) in enumerate(age_valid) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = age_model(images)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/128 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "\n",
    "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        age_best_model = age_model\n",
    "        path = '/opt/ml/teamrepo/kbs/age_model/'\n",
    "        torch.save(age_best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    lr_sched.step()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  0.5508 | Classifier Accuracy 82.03\n",
      "Iteration  50 | Train Loss  0.4801 | Classifier Accuracy 83.59\n",
      "Iteration 100 | Train Loss  0.4436 | Classifier Accuracy 84.38\n",
      "\n",
      "[Summary] Elapsed time : 0 m 48 s\n",
      "Train Loss Mean 0.4755 | Accuracy 83.57 \n",
      "Valid Loss Mean 0.4211 | Accuracy 83.12 \n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.3754 | Classifier Accuracy 88.28\n",
      "Iteration  50 | Train Loss  0.3421 | Classifier Accuracy 89.06\n",
      "Iteration 100 | Train Loss  0.3053 | Classifier Accuracy 86.72\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.3821 | Accuracy 85.13 \n",
      "Valid Loss Mean 0.3512 | Accuracy 85.26 \n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.3205 | Classifier Accuracy 89.06\n",
      "Iteration  50 | Train Loss  0.2981 | Classifier Accuracy 88.28\n",
      "Iteration 100 | Train Loss  0.3457 | Classifier Accuracy 88.28\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.3340 | Accuracy 86.25 \n",
      "Valid Loss Mean 0.3108 | Accuracy 87.08 \n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.2863 | Classifier Accuracy 88.28\n",
      "Iteration  50 | Train Loss  0.2887 | Classifier Accuracy 89.06\n",
      "Iteration 100 | Train Loss  0.3031 | Classifier Accuracy 85.16\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.3049 | Accuracy 87.68 \n",
      "Valid Loss Mean 0.3098 | Accuracy 86.22 \n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.3871 | Classifier Accuracy 81.25\n",
      "Iteration  50 | Train Loss  0.3285 | Classifier Accuracy 87.50\n",
      "Iteration 100 | Train Loss  0.2935 | Classifier Accuracy 86.72\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.2826 | Accuracy 88.34 \n",
      "Valid Loss Mean 0.2698 | Accuracy 88.70 \n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.2557 | Classifier Accuracy 91.41\n",
      "Iteration  50 | Train Loss  0.2140 | Classifier Accuracy 92.97\n",
      "Iteration 100 | Train Loss  0.2308 | Classifier Accuracy 92.97\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.2705 | Accuracy 89.10 \n",
      "Valid Loss Mean 0.2524 | Accuracy 88.93 \n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.3051 | Classifier Accuracy 88.28\n",
      "Iteration  50 | Train Loss  0.2666 | Classifier Accuracy 86.72\n",
      "Iteration 100 | Train Loss  0.2364 | Classifier Accuracy 92.19\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.2542 | Accuracy 89.69 \n",
      "Valid Loss Mean 0.2402 | Accuracy 89.43 \n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.2506 | Classifier Accuracy 91.41\n",
      "Iteration  50 | Train Loss  0.2286 | Classifier Accuracy 90.62\n",
      "Iteration 100 | Train Loss  0.1312 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.2453 | Accuracy 89.96 \n",
      "Valid Loss Mean 0.2331 | Accuracy 89.82 \n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.2856 | Classifier Accuracy 90.62\n",
      "Iteration  50 | Train Loss  0.1857 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1964 | Classifier Accuracy 95.31\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.2355 | Accuracy 90.63 \n",
      "Valid Loss Mean 0.2267 | Accuracy 90.16 \n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.2581 | Classifier Accuracy 91.41\n",
      "Iteration  50 | Train Loss  0.2611 | Classifier Accuracy 91.41\n",
      "Iteration 100 | Train Loss  0.1718 | Classifier Accuracy 92.19\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.2291 | Accuracy 90.76 \n",
      "Valid Loss Mean 0.2256 | Accuracy 89.92 \n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.2589 | Classifier Accuracy 88.28\n",
      "Iteration  50 | Train Loss  0.2033 | Classifier Accuracy 92.97\n",
      "Iteration 100 | Train Loss  0.2676 | Classifier Accuracy 86.72\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.2186 | Accuracy 91.03 \n",
      "Valid Loss Mean 0.2204 | Accuracy 90.39 \n",
      "\n",
      " ====================== epoch 12 ======================\n",
      "Iteration   0 | Train Loss  0.2130 | Classifier Accuracy 91.41\n",
      "Iteration  50 | Train Loss  0.1759 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1355 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.2123 | Accuracy 91.71 \n",
      "Valid Loss Mean 0.2095 | Accuracy 90.16 \n",
      "\n",
      " ====================== epoch 13 ======================\n",
      "Iteration   0 | Train Loss  0.2241 | Classifier Accuracy 91.41\n",
      "Iteration  50 | Train Loss  0.1817 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.1917 | Classifier Accuracy 92.19\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.2048 | Accuracy 91.85 \n",
      "Valid Loss Mean 0.1979 | Accuracy 91.98 \n",
      "\n",
      " ====================== epoch 14 ======================\n",
      "Iteration   0 | Train Loss  0.1678 | Classifier Accuracy 94.53\n",
      "Iteration  50 | Train Loss  0.1969 | Classifier Accuracy 90.62\n",
      "Iteration 100 | Train Loss  0.1385 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.2022 | Accuracy 91.99 \n",
      "Valid Loss Mean 0.1944 | Accuracy 91.30 \n",
      "\n",
      " ====================== epoch 15 ======================\n",
      "Iteration   0 | Train Loss  0.1563 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.2597 | Classifier Accuracy 90.62\n",
      "Iteration 100 | Train Loss  0.1342 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1992 | Accuracy 92.06 \n",
      "Valid Loss Mean 0.1869 | Accuracy 92.45 \n",
      "\n",
      " ====================== epoch 16 ======================\n",
      "Iteration   0 | Train Loss  0.1185 | Classifier Accuracy 97.66\n",
      "Iteration  50 | Train Loss  0.1550 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.2046 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1891 | Accuracy 92.39 \n",
      "Valid Loss Mean 0.1873 | Accuracy 92.27 \n",
      "\n",
      " ====================== epoch 17 ======================\n",
      "Iteration   0 | Train Loss  0.2032 | Classifier Accuracy 91.41\n",
      "Iteration  50 | Train Loss  0.1334 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.1585 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1852 | Accuracy 92.73 \n",
      "Valid Loss Mean 0.1808 | Accuracy 91.90 \n",
      "\n",
      " ====================== epoch 18 ======================\n",
      "Iteration   0 | Train Loss  0.1408 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.2036 | Classifier Accuracy 92.19\n",
      "Iteration 100 | Train Loss  0.2532 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1846 | Accuracy 92.60 \n",
      "Valid Loss Mean 0.1797 | Accuracy 92.11 \n",
      "\n",
      " ====================== epoch 19 ======================\n",
      "Iteration   0 | Train Loss  0.1685 | Classifier Accuracy 94.53\n",
      "Iteration  50 | Train Loss  0.2445 | Classifier Accuracy 89.84\n",
      "Iteration 100 | Train Loss  0.2510 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1780 | Accuracy 93.07 \n",
      "Valid Loss Mean 0.1881 | Accuracy 91.43 \n",
      "\n",
      " ====================== epoch 20 ======================\n",
      "Iteration   0 | Train Loss  0.2048 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1386 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.1597 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1704 | Accuracy 93.49 \n",
      "Valid Loss Mean 0.1698 | Accuracy 93.26 \n",
      "\n",
      " ====================== epoch 21 ======================\n",
      "Iteration   0 | Train Loss  0.1640 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.1591 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.1337 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1745 | Accuracy 93.44 \n",
      "Valid Loss Mean 0.1617 | Accuracy 93.39 \n",
      "\n",
      " ====================== epoch 22 ======================\n",
      "Iteration   0 | Train Loss  0.1929 | Classifier Accuracy 92.97\n",
      "Iteration  50 | Train Loss  0.1908 | Classifier Accuracy 92.97\n",
      "Iteration 100 | Train Loss  0.1975 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1690 | Accuracy 93.48 \n",
      "Valid Loss Mean 0.1665 | Accuracy 92.45 \n",
      "\n",
      " ====================== epoch 23 ======================\n",
      "Iteration   0 | Train Loss  0.1639 | Classifier Accuracy 94.53\n",
      "Iteration  50 | Train Loss  0.1621 | Classifier Accuracy 96.09\n",
      "Iteration 100 | Train Loss  0.0959 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.1649 | Accuracy 93.74 \n",
      "Valid Loss Mean 0.1580 | Accuracy 93.49 \n",
      "\n",
      " ====================== epoch 24 ======================\n",
      "Iteration   0 | Train Loss  0.1672 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1462 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1350 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1595 | Accuracy 94.07 \n",
      "Valid Loss Mean 0.1554 | Accuracy 93.52 \n",
      "\n",
      " ====================== epoch 25 ======================\n",
      "Iteration   0 | Train Loss  0.2239 | Classifier Accuracy 92.19\n",
      "Iteration  50 | Train Loss  0.1135 | Classifier Accuracy 96.09\n",
      "Iteration 100 | Train Loss  0.1558 | Classifier Accuracy 95.31\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.1576 | Accuracy 94.06 \n",
      "Valid Loss Mean 0.1530 | Accuracy 93.93 \n",
      "\n",
      " ====================== epoch 26 ======================\n",
      "Iteration   0 | Train Loss  0.1380 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.1689 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1598 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1566 | Accuracy 94.03 \n",
      "Valid Loss Mean 0.1566 | Accuracy 93.85 \n",
      "\n",
      " ====================== epoch 27 ======================\n",
      "Iteration   0 | Train Loss  0.1592 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.1490 | Classifier Accuracy 96.09\n",
      "Iteration 100 | Train Loss  0.1380 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1541 | Accuracy 94.22 \n",
      "Valid Loss Mean 0.1491 | Accuracy 93.91 \n",
      "\n",
      " ====================== epoch 28 ======================\n",
      "Iteration   0 | Train Loss  0.1464 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.1836 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.1672 | Classifier Accuracy 95.31\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1493 | Accuracy 94.56 \n",
      "Valid Loss Mean 0.1525 | Accuracy 93.12 \n",
      "\n",
      " ====================== epoch 29 ======================\n",
      "Iteration   0 | Train Loss  0.1311 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1359 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.1346 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1538 | Accuracy 94.37 \n",
      "Valid Loss Mean 0.1468 | Accuracy 94.17 \n",
      "\n",
      " ====================== epoch 30 ======================\n",
      "Iteration   0 | Train Loss  0.1255 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.1174 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.1554 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1487 | Accuracy 94.45 \n",
      "Valid Loss Mean 0.1432 | Accuracy 94.32 \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "mask_best_model.eval()\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "mask_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        scores = mask_best_model(images)\n",
    "        preds = scores.argmax(dim=-1)\n",
    "        mask_predictions.extend(preds.cpu().numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "from collections import Counter\n",
    "Counter(mask_predictions) #이야 더 확고해졌네\n",
    "#레이블링 관련 알고리즘을 살펴보자. startswith의 대용은?\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({0: 12600})"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "\n",
    "gender_best_model.eval()\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "gender_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        scores = gender_best_model(images)\n",
    "        preds = scores.argmax(dim=-1)\n",
    "        gender_predictions.extend(preds.cpu().numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "from collections import Counter\n",
    "Counter(gender_predictions)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({0: 6565, 1: 6035})"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "age_best_model.eval()\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "age_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        scores = age_best_model(images)\n",
    "        preds = scores.argmax(dim=-1)\n",
    "        age_predictions.extend(preds.cpu().numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "from collections import Counter\n",
    "Counter(age_predictions)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({0: 6158, 1: 5418, 2: 1024})"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "all_predictions = []\n",
    "size = len(submission)\n",
    "class_map = np.array([[[0, 1, 2],\n",
    "                       [3, 4, 5]],\n",
    "                      [[6, 7, 8],\n",
    "                       [9, 10, 11]],\n",
    "                      [[12, 13, 14],\n",
    "                       [15, 16, 17]]])\n",
    "for idx in range(size):\n",
    "    i = mask_predictions[idx]\n",
    "    j = gender_predictions[idx]\n",
    "    k = age_predictions[idx]\n",
    "    all_predictions.append(class_map[i][j][k])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission_baseline_pretrained.csv'), index=False)\n",
    "print('test inference is done!')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}