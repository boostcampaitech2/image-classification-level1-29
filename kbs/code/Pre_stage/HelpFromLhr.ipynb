{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import os, glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "from torchsummary import summary\n",
    "import torch_optimizer as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using device: cuda:0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# 학습 데이터셋 폴더 경로를 지정해주세요.\n",
    "train_dir = '/opt/ml/input/data/train'\n",
    "trainimage_dir = os.path.join(train_dir, 'images')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "train_df = pd.read_csv(os.path.join(train_dir, 'train.csv'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\n",
    "masks = ['mask1', 'mask2', 'mask3', 'mask4', 'mask5', 'incorrect_mask', 'normal']\n",
    "wears = ['Wear', 'Wear', 'Wear', 'Wear', 'Wear', 'Incorrect', 'Not Wear']\n",
    "mask_df = pd.DataFrame()\n",
    "for person in train_df.values:\n",
    "    for mask, wear in zip(masks, wears):\n",
    "        mask_df = mask_df.append(pd.Series(np.append(person, (mask, wear))), ignore_index=True)\n",
    "mask_df.columns = np.append(train_df.columns.values, ('mask', 'wear'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "print(mask_df)\n",
    "mask_df = mask_df.sample(frac=1).reset_index(drop=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "           id  gender   race   age                    path            mask  \\\n",
      "0      000001  female  Asian  45.0  000001_female_Asian_45           mask1   \n",
      "1      000001  female  Asian  45.0  000001_female_Asian_45           mask2   \n",
      "2      000001  female  Asian  45.0  000001_female_Asian_45           mask3   \n",
      "3      000001  female  Asian  45.0  000001_female_Asian_45           mask4   \n",
      "4      000001  female  Asian  45.0  000001_female_Asian_45           mask5   \n",
      "...       ...     ...    ...   ...                     ...             ...   \n",
      "18895  006959    male  Asian  19.0    006959_male_Asian_19           mask3   \n",
      "18896  006959    male  Asian  19.0    006959_male_Asian_19           mask4   \n",
      "18897  006959    male  Asian  19.0    006959_male_Asian_19           mask5   \n",
      "18898  006959    male  Asian  19.0    006959_male_Asian_19  incorrect_mask   \n",
      "18899  006959    male  Asian  19.0    006959_male_Asian_19          normal   \n",
      "\n",
      "            wear  \n",
      "0           Wear  \n",
      "1           Wear  \n",
      "2           Wear  \n",
      "3           Wear  \n",
      "4           Wear  \n",
      "...          ...  \n",
      "18895       Wear  \n",
      "18896       Wear  \n",
      "18897       Wear  \n",
      "18898  Incorrect  \n",
      "18899   Not Wear  \n",
      "\n",
      "[18900 rows x 7 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "train, valid = train_test_split(mask_df, test_size=0.2, stratify=mask_df['wear'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(384),\n",
    "    Resize((224, 224), Image.BILINEAR),\n",
    "    \n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "class GenderDataset(Dataset):\n",
    "    def __init__(self, path, mask_df, transform):\n",
    "        super(GenderDataset).__init__()\n",
    "        self.path = path\n",
    "        self.mask_df = mask_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        full_path = os.path.join(self.path, self.mask_df.iloc[idx]['path'])\n",
    "        img_list = glob.glob(full_path + '/*')\n",
    "        file_name = self.mask_df.iloc[idx]['mask']\n",
    "        for img_name in img_list:\n",
    "            if img_name.startswith(file_name):\n",
    "                break\n",
    "        image = Image.open(os.path.join(full_path, img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.mask_df.iloc[idx]['gender']\n",
    "        label = 0 if label=='male' else 1\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mask_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "gender_train_data = GenderDataset(trainimage_dir, train, transform)\n",
    "gender_valid_data = GenderDataset(trainimage_dir, valid, transform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "batch_size = 128"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "gender_train = DataLoader(gender_train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "gender_valid = DataLoader(gender_valid_data, batch_size=batch_size, shuffle=True, num_workers=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, path, mask_df, transform):\n",
    "        super(AgeDataset).__init__()\n",
    "        self.path = path\n",
    "        self.mask_df = mask_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        full_path = os.path.join(self.path, self.mask_df.iloc[idx]['path'])\n",
    "        img_list = glob.glob(full_path + '/*')\n",
    "        file_name = self.mask_df.iloc[idx]['mask']\n",
    "        for img_name in img_list:\n",
    "            if img_name.startswith(file_name):\n",
    "                break\n",
    "        image = Image.open(os.path.join(full_path, img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.mask_df.iloc[idx]['age']\n",
    "        if label >= 60.0:\n",
    "            label = 2\n",
    "        elif label >= 30.0:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mask_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "age_train_data = AgeDataset(trainimage_dir, train, transform)\n",
    "age_valid_data = AgeDataset(trainimage_dir, valid, transform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "age_train = DataLoader(age_train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "age_valid = DataLoader(age_valid_data, batch_size=batch_size, shuffle=True, num_workers=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, path, mask_df, transform):\n",
    "        super(MaskDataset).__init__()\n",
    "        self.path = path\n",
    "        self.mask_df = mask_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        full_path = os.path.join(self.path, self.mask_df.iloc[idx]['path'])\n",
    "        img_list = glob.glob(full_path + '/*')\n",
    "        file_name = self.mask_df.iloc[idx]['mask']\n",
    "        for img_name in img_list:\n",
    "            if img_name.startswith(file_name):\n",
    "                break\n",
    "        image = Image.open(os.path.join(full_path, img_name))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.mask_df.iloc[idx]['mask']\n",
    "        if 'mask' in label:\n",
    "            label = 0\n",
    "            \n",
    "        elif 'incorrect' in label:\n",
    "            label = 1\n",
    "            \n",
    "        else:\n",
    "            label = 2\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mask_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "mask_train_data = MaskDataset(trainimage_dir, train, transform)\n",
    "mask_valid_data = MaskDataset(trainimage_dir, valid, transform)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "mask_train = DataLoader(mask_train_data, batch_size=batch_size, shuffle=True)\n",
    "mask_valid = DataLoader(mask_valid_data, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        super(TestDataset).__init__()\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "test_dir = '/opt/ml/input/data/eval'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "testimage_dir = os.path.join(test_dir, 'images')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(testimage_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((224, 224), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    \n",
    "])\n",
    "test_dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "\n",
    "model = resnet50(pretrained=True, progress=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = resnet50(pretrained=True, progress=False)\n",
    "        \n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "mask_model = MyModel(num_classes=3)\n",
    "for param in mask_model.parameters():\n",
    "    param.requires_grad = False # frozen\n",
    "for param in mask_model.model.fc.parameters():\n",
    "    param.requires_grad = True # 마지막 레이어 살리기\n",
    "mask_model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "gender_model = MyModel(num_classes=2)\n",
    "for param in gender_model.parameters():\n",
    "    param.requires_grad = False # frozen\n",
    "for param in gender_model.model.fc.parameters():\n",
    "    param.requires_grad = True # 마지막 레이어 살리기\n",
    "gender_model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "\n",
    "age_model = MyModel(num_classes=3)\n",
    "for param in age_model.parameters():\n",
    "    param.requires_grad = False # frozen\n",
    "for param in age_model.model.fc.parameters():\n",
    "    param.requires_grad = True # 마지막 레이어 살리기\n",
    "age_model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "learning_rate = 0.001\n",
    "betas = (0.9, 0.999)\n",
    "weight_decay = 1e-4\n",
    "T_max = 50"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "num_epochs = 30\n",
    "optimizer = torch.optim.Adam(mask_model.parameters(), lr = 0.01)\n",
    "lr_sched  = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, \n",
    "milestones=[int(num_epochs * 0.5), int(num_epochs * 0.75)], gamma=0.1, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    \n",
    "\n",
    "    # train\n",
    "    mask_model.train()\n",
    "    for i, (images, targets) in enumerate(mask_train) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = mask_model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_sched.step()\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/128 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 50 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    mask_model.eval()\n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    for i, (images, targets) in enumerate(mask_valid) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            scores = mask_model(images)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/128 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "\n",
    "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        mask_best_model = mask_model\n",
    "        path = './mask_model/'\n",
    "        torch.save(mask_best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  0.7714 | Classifier Accuracy 91.41\n",
      "Iteration  50 | Train Loss  0.4281 | Classifier Accuracy 85.94\n",
      "Iteration 100 | Train Loss  0.3486 | Classifier Accuracy 89.06\n",
      "\n",
      "[Summary] Elapsed time : 1 m 45 s\n",
      "Train Loss Mean 0.6656 | Accuracy 83.90 \n",
      "Valid Loss Mean 0.4240 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.4753 | Classifier Accuracy 82.81\n",
      "Iteration  50 | Train Loss  0.3383 | Classifier Accuracy 89.84\n",
      "Iteration 100 | Train Loss  0.4717 | Classifier Accuracy 82.81\n",
      "\n",
      "[Summary] Elapsed time : 2 m 11 s\n",
      "Train Loss Mean 0.4185 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4223 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.3603 | Classifier Accuracy 89.06\n",
      "Iteration  50 | Train Loss  0.4457 | Classifier Accuracy 83.59\n",
      "Iteration 100 | Train Loss  0.4515 | Classifier Accuracy 83.59\n",
      "\n",
      "[Summary] Elapsed time : 2 m 12 s\n",
      "Train Loss Mean 0.4172 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4197 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.4014 | Classifier Accuracy 87.50\n",
      "Iteration  50 | Train Loss  0.4418 | Classifier Accuracy 83.59\n",
      "Iteration 100 | Train Loss  0.4156 | Classifier Accuracy 85.94\n",
      "\n",
      "[Summary] Elapsed time : 2 m 12 s\n",
      "Train Loss Mean 0.4171 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4182 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.4694 | Classifier Accuracy 82.81\n",
      "Iteration  50 | Train Loss  0.3728 | Classifier Accuracy 87.50\n",
      "Iteration 100 | Train Loss  0.4053 | Classifier Accuracy 85.16\n",
      "\n",
      "[Summary] Elapsed time : 2 m 12 s\n",
      "Train Loss Mean 0.4153 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4164 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.4591 | Classifier Accuracy 82.81\n",
      "Iteration  50 | Train Loss  0.4097 | Classifier Accuracy 85.94\n",
      "Iteration 100 | Train Loss  0.4665 | Classifier Accuracy 82.81\n",
      "\n",
      "[Summary] Elapsed time : 2 m 13 s\n",
      "Train Loss Mean 0.4140 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4167 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.4034 | Classifier Accuracy 85.94\n",
      "Iteration  50 | Train Loss  0.4410 | Classifier Accuracy 84.38\n",
      "Iteration 100 | Train Loss  0.4026 | Classifier Accuracy 86.72\n",
      "\n",
      "[Summary] Elapsed time : 2 m 13 s\n",
      "Train Loss Mean 0.4150 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4168 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.5209 | Classifier Accuracy 80.47\n",
      "Iteration  50 | Train Loss  0.4548 | Classifier Accuracy 83.59\n",
      "Iteration 100 | Train Loss  0.3953 | Classifier Accuracy 86.72\n",
      "\n",
      "[Summary] Elapsed time : 2 m 13 s\n",
      "Train Loss Mean 0.4148 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4198 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.3896 | Classifier Accuracy 86.72\n",
      "Iteration  50 | Train Loss  0.4336 | Classifier Accuracy 84.38\n",
      "Iteration 100 | Train Loss  0.4775 | Classifier Accuracy 82.03\n",
      "\n",
      "[Summary] Elapsed time : 2 m 13 s\n",
      "Train Loss Mean 0.4117 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4168 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.4306 | Classifier Accuracy 84.38\n",
      "Iteration  50 | Train Loss  0.3832 | Classifier Accuracy 87.50\n",
      "Iteration 100 | Train Loss  0.4179 | Classifier Accuracy 85.16\n",
      "\n",
      "[Summary] Elapsed time : 2 m 17 s\n",
      "Train Loss Mean 0.4114 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4164 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.3547 | Classifier Accuracy 89.06\n",
      "Iteration  50 | Train Loss  0.4461 | Classifier Accuracy 82.81\n",
      "Iteration 100 | Train Loss  0.3556 | Classifier Accuracy 89.06\n",
      "\n",
      "[Summary] Elapsed time : 2 m 15 s\n",
      "Train Loss Mean 0.4121 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4192 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 12 ======================\n",
      "Iteration   0 | Train Loss  0.4313 | Classifier Accuracy 84.38\n",
      "Iteration  50 | Train Loss  0.4272 | Classifier Accuracy 84.38\n",
      "Iteration 100 | Train Loss  0.4044 | Classifier Accuracy 85.94\n",
      "\n",
      "[Summary] Elapsed time : 2 m 13 s\n",
      "Train Loss Mean 0.4130 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4176 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 13 ======================\n",
      "Iteration   0 | Train Loss  0.4144 | Classifier Accuracy 85.16\n",
      "Iteration  50 | Train Loss  0.4395 | Classifier Accuracy 84.38\n",
      "Iteration 100 | Train Loss  0.4333 | Classifier Accuracy 84.38\n",
      "\n",
      "[Summary] Elapsed time : 2 m 13 s\n",
      "Train Loss Mean 0.4105 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4184 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 14 ======================\n",
      "Iteration   0 | Train Loss  0.3768 | Classifier Accuracy 87.50\n",
      "Iteration  50 | Train Loss  0.5173 | Classifier Accuracy 79.69\n",
      "Iteration 100 | Train Loss  0.4444 | Classifier Accuracy 83.59\n",
      "\n",
      "[Summary] Elapsed time : 2 m 12 s\n",
      "Train Loss Mean 0.4132 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4209 | Accuracy 84.38 \n",
      "\n",
      " ====================== epoch 15 ======================\n",
      "Iteration   0 | Train Loss  0.4854 | Classifier Accuracy 82.03\n",
      "Iteration  50 | Train Loss  0.4376 | Classifier Accuracy 84.38\n",
      "Iteration 100 | Train Loss  0.4058 | Classifier Accuracy 85.94\n",
      "\n",
      "[Summary] Elapsed time : 2 m 12 s\n",
      "Train Loss Mean 0.4133 | Accuracy 85.08 \n",
      "Valid Loss Mean 0.4190 | Accuracy 84.38 \n",
      "\n",
      "EARLY STOPPING!!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "\n",
    "\n",
    "optimizer = optim.RAdam(gender_model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=T_max)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "\n",
    "    # train\n",
    "    gender_model.train()\n",
    "    for i, (images, targets) in enumerate(gender_train) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = gender_model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/128 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 50 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    gender_model.eval()\n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    for i, (images, targets) in enumerate(gender_valid) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = gender_model(images)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/128 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "\n",
    "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        gender_best_model = gender_model\n",
    "        path = '/opt/ml/teamrepo/kbs/code/gender_model/'\n",
    "        torch.save(gender_best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    lr_sched.step()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  0.3074 | Classifier Accuracy 87.50\n",
      "Iteration  50 | Train Loss  0.2670 | Classifier Accuracy 87.50\n",
      "Iteration 100 | Train Loss  0.2751 | Classifier Accuracy 91.41\n",
      "\n",
      "[Summary] Elapsed time : 0 m 48 s\n",
      "Train Loss Mean 0.2673 | Accuracy 90.25 \n",
      "Valid Loss Mean 0.2566 | Accuracy 89.48 \n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.2719 | Classifier Accuracy 88.28\n",
      "Iteration  50 | Train Loss  0.2092 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.1909 | Classifier Accuracy 92.97\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.2286 | Accuracy 91.16 \n",
      "Valid Loss Mean 0.2252 | Accuracy 90.49 \n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.1224 | Classifier Accuracy 98.44\n",
      "Iteration  50 | Train Loss  0.1630 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.2000 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.2024 | Accuracy 92.05 \n",
      "Valid Loss Mean 0.2092 | Accuracy 90.52 \n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.1158 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1862 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.1835 | Classifier Accuracy 90.62\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1910 | Accuracy 92.06 \n",
      "Valid Loss Mean 0.1939 | Accuracy 91.38 \n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.1496 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.2189 | Classifier Accuracy 92.97\n",
      "Iteration 100 | Train Loss  0.2016 | Classifier Accuracy 91.41\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1730 | Accuracy 93.21 \n",
      "Valid Loss Mean 0.1819 | Accuracy 92.27 \n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.1864 | Classifier Accuracy 92.97\n",
      "Iteration  50 | Train Loss  0.1525 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.1901 | Classifier Accuracy 91.41\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1634 | Accuracy 93.32 \n",
      "Valid Loss Mean 0.1775 | Accuracy 92.60 \n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.1144 | Classifier Accuracy 97.66\n",
      "Iteration  50 | Train Loss  0.1548 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1779 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1557 | Accuracy 93.86 \n",
      "Valid Loss Mean 0.1694 | Accuracy 92.66 \n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.1631 | Classifier Accuracy 92.97\n",
      "Iteration  50 | Train Loss  0.1543 | Classifier Accuracy 96.09\n",
      "Iteration 100 | Train Loss  0.1874 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1524 | Accuracy 94.05 \n",
      "Valid Loss Mean 0.1593 | Accuracy 93.26 \n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.2096 | Classifier Accuracy 91.41\n",
      "Iteration  50 | Train Loss  0.1268 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1665 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1451 | Accuracy 94.33 \n",
      "Valid Loss Mean 0.1535 | Accuracy 93.39 \n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.1473 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1434 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.1644 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1419 | Accuracy 94.29 \n",
      "Valid Loss Mean 0.1497 | Accuracy 93.96 \n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.1278 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.1387 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.0796 | Classifier Accuracy 98.44\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.1334 | Accuracy 94.68 \n",
      "Valid Loss Mean 0.1561 | Accuracy 93.26 \n",
      "\n",
      " ====================== epoch 12 ======================\n",
      "Iteration   0 | Train Loss  0.1321 | Classifier Accuracy 94.53\n",
      "Iteration  50 | Train Loss  0.1184 | Classifier Accuracy 96.09\n",
      "Iteration 100 | Train Loss  0.1559 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.1269 | Accuracy 95.10 \n",
      "Valid Loss Mean 0.1486 | Accuracy 93.44 \n",
      "\n",
      " ====================== epoch 13 ======================\n",
      "Iteration   0 | Train Loss  0.1785 | Classifier Accuracy 89.84\n",
      "Iteration  50 | Train Loss  0.0805 | Classifier Accuracy 99.22\n",
      "Iteration 100 | Train Loss  0.1068 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1221 | Accuracy 95.25 \n",
      "Valid Loss Mean 0.1413 | Accuracy 94.19 \n",
      "\n",
      " ====================== epoch 14 ======================\n",
      "Iteration   0 | Train Loss  0.1137 | Classifier Accuracy 98.44\n",
      "Iteration  50 | Train Loss  0.1136 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.1600 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1299 | Accuracy 94.68 \n",
      "Valid Loss Mean 0.1441 | Accuracy 93.65 \n",
      "\n",
      " ====================== epoch 15 ======================\n",
      "Iteration   0 | Train Loss  0.1660 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1237 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1675 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1204 | Accuracy 95.30 \n",
      "Valid Loss Mean 0.1325 | Accuracy 94.53 \n",
      "\n",
      " ====================== epoch 16 ======================\n",
      "Iteration   0 | Train Loss  0.1524 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1733 | Classifier Accuracy 93.75\n",
      "Iteration 100 | Train Loss  0.1914 | Classifier Accuracy 91.41\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1215 | Accuracy 95.21 \n",
      "Valid Loss Mean 0.1354 | Accuracy 94.17 \n",
      "\n",
      " ====================== epoch 17 ======================\n",
      "Iteration   0 | Train Loss  0.0916 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.1352 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.0633 | Classifier Accuracy 98.44\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1157 | Accuracy 95.25 \n",
      "Valid Loss Mean 0.1363 | Accuracy 93.52 \n",
      "\n",
      " ====================== epoch 18 ======================\n",
      "Iteration   0 | Train Loss  0.1404 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.1125 | Classifier Accuracy 96.09\n",
      "Iteration 100 | Train Loss  0.1020 | Classifier Accuracy 95.31\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1160 | Accuracy 95.33 \n",
      "Valid Loss Mean 0.1268 | Accuracy 94.40 \n",
      "\n",
      " ====================== epoch 19 ======================\n",
      "Iteration   0 | Train Loss  0.0882 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1014 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.0724 | Classifier Accuracy 99.22\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1071 | Accuracy 95.94 \n",
      "Valid Loss Mean 0.1258 | Accuracy 94.40 \n",
      "\n",
      " ====================== epoch 20 ======================\n",
      "Iteration   0 | Train Loss  0.0735 | Classifier Accuracy 97.66\n",
      "Iteration  50 | Train Loss  0.1039 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1262 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1077 | Accuracy 95.78 \n",
      "Valid Loss Mean 0.1214 | Accuracy 94.79 \n",
      "\n",
      " ====================== epoch 21 ======================\n",
      "Iteration   0 | Train Loss  0.0536 | Classifier Accuracy 99.22\n",
      "Iteration  50 | Train Loss  0.1311 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.1325 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1037 | Accuracy 96.01 \n",
      "Valid Loss Mean 0.1212 | Accuracy 94.74 \n",
      "\n",
      " ====================== epoch 22 ======================\n",
      "Iteration   0 | Train Loss  0.1129 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.1050 | Classifier Accuracy 96.09\n",
      "Iteration 100 | Train Loss  0.1270 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1048 | Accuracy 95.86 \n",
      "Valid Loss Mean 0.1189 | Accuracy 94.69 \n",
      "\n",
      " ====================== epoch 23 ======================\n",
      "Iteration   0 | Train Loss  0.1311 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.1242 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.1222 | Classifier Accuracy 97.66\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1066 | Accuracy 95.72 \n",
      "Valid Loss Mean 0.1236 | Accuracy 94.45 \n",
      "\n",
      " ====================== epoch 24 ======================\n",
      "Iteration   0 | Train Loss  0.1046 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.1465 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.1374 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.1039 | Accuracy 96.01 \n",
      "Valid Loss Mean 0.1143 | Accuracy 94.82 \n",
      "\n",
      " ====================== epoch 25 ======================\n",
      "Iteration   0 | Train Loss  0.1436 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.1095 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.0839 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.0995 | Accuracy 96.06 \n",
      "Valid Loss Mean 0.1180 | Accuracy 94.48 \n",
      "\n",
      " ====================== epoch 26 ======================\n",
      "Iteration   0 | Train Loss  0.0882 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.0585 | Classifier Accuracy 99.22\n",
      "Iteration 100 | Train Loss  0.0718 | Classifier Accuracy 98.44\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.0973 | Accuracy 96.19 \n",
      "Valid Loss Mean 0.1154 | Accuracy 94.64 \n",
      "\n",
      " ====================== epoch 27 ======================\n",
      "Iteration   0 | Train Loss  0.0760 | Classifier Accuracy 97.66\n",
      "Iteration  50 | Train Loss  0.0738 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.1332 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.0973 | Accuracy 96.19 \n",
      "Valid Loss Mean 0.1098 | Accuracy 94.97 \n",
      "\n",
      " ====================== epoch 28 ======================\n",
      "Iteration   0 | Train Loss  0.1046 | Classifier Accuracy 97.66\n",
      "Iteration  50 | Train Loss  0.0905 | Classifier Accuracy 96.09\n",
      "Iteration 100 | Train Loss  0.1692 | Classifier Accuracy 95.31\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.0951 | Accuracy 96.26 \n",
      "Valid Loss Mean 0.1119 | Accuracy 94.79 \n",
      "\n",
      " ====================== epoch 29 ======================\n",
      "Iteration   0 | Train Loss  0.1227 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.0973 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.0691 | Classifier Accuracy 97.66\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.0922 | Accuracy 96.45 \n",
      "Valid Loss Mean 0.1134 | Accuracy 94.82 \n",
      "\n",
      " ====================== epoch 30 ======================\n",
      "Iteration   0 | Train Loss  0.1605 | Classifier Accuracy 94.53\n",
      "Iteration  50 | Train Loss  0.1039 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.0653 | Classifier Accuracy 98.44\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.0914 | Accuracy 96.65 \n",
      "Valid Loss Mean 0.1077 | Accuracy 95.16 \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "optimizer = optim.RAdam(age_model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=T_max)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "for e in range(num_epochs) :\n",
    "    print(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "\n",
    "    # train\n",
    "    age_model.train()\n",
    "    for i, (images, targets) in enumerate(age_train) : \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = age_model(images)\n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/128 * 100)\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "\n",
    "        if i % 50 == 0 :\n",
    "            print(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    print('')\n",
    "    print(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    print(f'Train Loss Mean %.4f | Accuracy %2.2f ' % (train_mean_loss, train_mean_acc) )\n",
    "\n",
    "    # validation \n",
    "    age_model.eval()\n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    for i, (images, targets) in enumerate(age_valid) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = age_model(images)\n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/128 * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "\n",
    "    print(f'Valid Loss Mean %.4f | Accuracy %2.2f ' % (val_mean_loss, val_mean_acc) )\n",
    "    print('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        age_best_model = age_model\n",
    "        path = '/opt/ml/teamrepo/kbs/code/age_model/'\n",
    "        torch.save(age_best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            print(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    lr_sched.step()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " ====================== epoch 1 ======================\n",
      "Iteration   0 | Train Loss  1.1023 | Classifier Accuracy 38.28\n",
      "Iteration  50 | Train Loss  0.7335 | Classifier Accuracy 76.56\n",
      "Iteration 100 | Train Loss  0.6076 | Classifier Accuracy 80.47\n",
      "\n",
      "[Summary] Elapsed time : 0 m 48 s\n",
      "Train Loss Mean 0.7269 | Accuracy 71.50 \n",
      "Valid Loss Mean 0.5236 | Accuracy 82.99 \n",
      "\n",
      " ====================== epoch 2 ======================\n",
      "Iteration   0 | Train Loss  0.6262 | Classifier Accuracy 78.12\n",
      "Iteration  50 | Train Loss  0.4957 | Classifier Accuracy 81.25\n",
      "Iteration 100 | Train Loss  0.4558 | Classifier Accuracy 82.81\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.4544 | Accuracy 83.84 \n",
      "Valid Loss Mean 0.4015 | Accuracy 83.98 \n",
      "\n",
      " ====================== epoch 3 ======================\n",
      "Iteration   0 | Train Loss  0.3653 | Classifier Accuracy 88.28\n",
      "Iteration  50 | Train Loss  0.3275 | Classifier Accuracy 85.16\n",
      "Iteration 100 | Train Loss  0.3213 | Classifier Accuracy 84.38\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.3702 | Accuracy 85.93 \n",
      "Valid Loss Mean 0.3270 | Accuracy 86.43 \n",
      "\n",
      " ====================== epoch 4 ======================\n",
      "Iteration   0 | Train Loss  0.3026 | Classifier Accuracy 92.19\n",
      "Iteration  50 | Train Loss  0.3125 | Classifier Accuracy 85.94\n",
      "Iteration 100 | Train Loss  0.3093 | Classifier Accuracy 85.94\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.3291 | Accuracy 86.77 \n",
      "Valid Loss Mean 0.2966 | Accuracy 86.30 \n",
      "\n",
      " ====================== epoch 5 ======================\n",
      "Iteration   0 | Train Loss  0.3043 | Classifier Accuracy 85.94\n",
      "Iteration  50 | Train Loss  0.2721 | Classifier Accuracy 89.84\n",
      "Iteration 100 | Train Loss  0.3545 | Classifier Accuracy 83.59\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.3024 | Accuracy 87.63 \n",
      "Valid Loss Mean 0.2729 | Accuracy 88.07 \n",
      "\n",
      " ====================== epoch 6 ======================\n",
      "Iteration   0 | Train Loss  0.2873 | Classifier Accuracy 86.72\n",
      "Iteration  50 | Train Loss  0.2866 | Classifier Accuracy 85.16\n",
      "Iteration 100 | Train Loss  0.3100 | Classifier Accuracy 85.16\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.2843 | Accuracy 88.37 \n",
      "Valid Loss Mean 0.2766 | Accuracy 87.37 \n",
      "\n",
      " ====================== epoch 7 ======================\n",
      "Iteration   0 | Train Loss  0.2246 | Classifier Accuracy 89.84\n",
      "Iteration  50 | Train Loss  0.2828 | Classifier Accuracy 89.06\n",
      "Iteration 100 | Train Loss  0.3186 | Classifier Accuracy 85.94\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.2694 | Accuracy 88.88 \n",
      "Valid Loss Mean 0.2472 | Accuracy 89.61 \n",
      "\n",
      " ====================== epoch 8 ======================\n",
      "Iteration   0 | Train Loss  0.2584 | Classifier Accuracy 92.19\n",
      "Iteration  50 | Train Loss  0.2830 | Classifier Accuracy 90.62\n",
      "Iteration 100 | Train Loss  0.2310 | Classifier Accuracy 91.41\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.2559 | Accuracy 89.65 \n",
      "Valid Loss Mean 0.2398 | Accuracy 88.88 \n",
      "\n",
      " ====================== epoch 9 ======================\n",
      "Iteration   0 | Train Loss  0.3599 | Classifier Accuracy 86.72\n",
      "Iteration  50 | Train Loss  0.2427 | Classifier Accuracy 89.84\n",
      "Iteration 100 | Train Loss  0.1694 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.2436 | Accuracy 90.11 \n",
      "Valid Loss Mean 0.2314 | Accuracy 88.88 \n",
      "\n",
      " ====================== epoch 10 ======================\n",
      "Iteration   0 | Train Loss  0.2902 | Classifier Accuracy 86.72\n",
      "Iteration  50 | Train Loss  0.2693 | Classifier Accuracy 89.84\n",
      "Iteration 100 | Train Loss  0.2658 | Classifier Accuracy 89.84\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.2369 | Accuracy 90.55 \n",
      "Valid Loss Mean 0.2130 | Accuracy 90.29 \n",
      "\n",
      " ====================== epoch 11 ======================\n",
      "Iteration   0 | Train Loss  0.2493 | Classifier Accuracy 89.06\n",
      "Iteration  50 | Train Loss  0.1971 | Classifier Accuracy 91.41\n",
      "Iteration 100 | Train Loss  0.1926 | Classifier Accuracy 92.97\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.2234 | Accuracy 91.15 \n",
      "Valid Loss Mean 0.2099 | Accuracy 91.22 \n",
      "\n",
      " ====================== epoch 12 ======================\n",
      "Iteration   0 | Train Loss  0.2995 | Classifier Accuracy 91.41\n",
      "Iteration  50 | Train Loss  0.3089 | Classifier Accuracy 88.28\n",
      "Iteration 100 | Train Loss  0.2303 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.2254 | Accuracy 90.79 \n",
      "Valid Loss Mean 0.2053 | Accuracy 90.13 \n",
      "\n",
      " ====================== epoch 13 ======================\n",
      "Iteration   0 | Train Loss  0.2002 | Classifier Accuracy 93.75\n",
      "Iteration  50 | Train Loss  0.1684 | Classifier Accuracy 92.97\n",
      "Iteration 100 | Train Loss  0.1380 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.2201 | Accuracy 91.11 \n",
      "Valid Loss Mean 0.1963 | Accuracy 91.48 \n",
      "\n",
      " ====================== epoch 14 ======================\n",
      "Iteration   0 | Train Loss  0.2170 | Classifier Accuracy 95.31\n",
      "Iteration  50 | Train Loss  0.1905 | Classifier Accuracy 92.19\n",
      "Iteration 100 | Train Loss  0.2057 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.2078 | Accuracy 91.68 \n",
      "Valid Loss Mean 0.1947 | Accuracy 91.09 \n",
      "\n",
      " ====================== epoch 15 ======================\n",
      "Iteration   0 | Train Loss  0.1953 | Classifier Accuracy 92.97\n",
      "Iteration  50 | Train Loss  0.2197 | Classifier Accuracy 92.97\n",
      "Iteration 100 | Train Loss  0.1870 | Classifier Accuracy 92.19\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1993 | Accuracy 91.94 \n",
      "Valid Loss Mean 0.1871 | Accuracy 92.40 \n",
      "\n",
      " ====================== epoch 16 ======================\n",
      "Iteration   0 | Train Loss  0.2205 | Classifier Accuracy 92.19\n",
      "Iteration  50 | Train Loss  0.1788 | Classifier Accuracy 90.62\n",
      "Iteration 100 | Train Loss  0.1482 | Classifier Accuracy 95.31\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1968 | Accuracy 92.42 \n",
      "Valid Loss Mean 0.1953 | Accuracy 91.41 \n",
      "\n",
      " ====================== epoch 17 ======================\n",
      "Iteration   0 | Train Loss  0.2098 | Classifier Accuracy 92.19\n",
      "Iteration  50 | Train Loss  0.2149 | Classifier Accuracy 89.84\n",
      "Iteration 100 | Train Loss  0.2231 | Classifier Accuracy 92.19\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1945 | Accuracy 92.19 \n",
      "Valid Loss Mean 0.1815 | Accuracy 92.50 \n",
      "\n",
      " ====================== epoch 18 ======================\n",
      "Iteration   0 | Train Loss  0.1483 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1518 | Classifier Accuracy 92.97\n",
      "Iteration 100 | Train Loss  0.2279 | Classifier Accuracy 92.19\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1859 | Accuracy 92.81 \n",
      "Valid Loss Mean 0.1739 | Accuracy 93.02 \n",
      "\n",
      " ====================== epoch 19 ======================\n",
      "Iteration   0 | Train Loss  0.1199 | Classifier Accuracy 97.66\n",
      "Iteration  50 | Train Loss  0.1424 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1952 | Classifier Accuracy 91.41\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1838 | Accuracy 92.84 \n",
      "Valid Loss Mean 0.1747 | Accuracy 92.97 \n",
      "\n",
      " ====================== epoch 20 ======================\n",
      "Iteration   0 | Train Loss  0.2174 | Classifier Accuracy 91.41\n",
      "Iteration  50 | Train Loss  0.2082 | Classifier Accuracy 92.19\n",
      "Iteration 100 | Train Loss  0.1367 | Classifier Accuracy 94.53\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1835 | Accuracy 92.86 \n",
      "Valid Loss Mean 0.1677 | Accuracy 92.81 \n",
      "\n",
      " ====================== epoch 21 ======================\n",
      "Iteration   0 | Train Loss  0.1203 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1285 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.2014 | Classifier Accuracy 92.19\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1793 | Accuracy 92.92 \n",
      "Valid Loss Mean 0.1733 | Accuracy 92.03 \n",
      "\n",
      " ====================== epoch 22 ======================\n",
      "Iteration   0 | Train Loss  0.1339 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.2536 | Classifier Accuracy 92.19\n",
      "Iteration 100 | Train Loss  0.1885 | Classifier Accuracy 92.97\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1738 | Accuracy 93.26 \n",
      "Valid Loss Mean 0.1734 | Accuracy 91.54 \n",
      "\n",
      " ====================== epoch 23 ======================\n",
      "Iteration   0 | Train Loss  0.1181 | Classifier Accuracy 97.66\n",
      "Iteration  50 | Train Loss  0.1194 | Classifier Accuracy 97.66\n",
      "Iteration 100 | Train Loss  0.1552 | Classifier Accuracy 95.31\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.1677 | Accuracy 93.57 \n",
      "Valid Loss Mean 0.1580 | Accuracy 93.54 \n",
      "\n",
      " ====================== epoch 24 ======================\n",
      "Iteration   0 | Train Loss  0.1822 | Classifier Accuracy 92.19\n",
      "Iteration  50 | Train Loss  0.1788 | Classifier Accuracy 94.53\n",
      "Iteration 100 | Train Loss  0.2272 | Classifier Accuracy 89.06\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1661 | Accuracy 93.48 \n",
      "Valid Loss Mean 0.1588 | Accuracy 93.83 \n",
      "\n",
      " ====================== epoch 25 ======================\n",
      "Iteration   0 | Train Loss  0.1659 | Classifier Accuracy 94.53\n",
      "Iteration  50 | Train Loss  0.1728 | Classifier Accuracy 92.19\n",
      "Iteration 100 | Train Loss  0.1506 | Classifier Accuracy 93.75\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1648 | Accuracy 93.93 \n",
      "Valid Loss Mean 0.1600 | Accuracy 94.11 \n",
      "\n",
      " ====================== epoch 26 ======================\n",
      "Iteration   0 | Train Loss  0.1814 | Classifier Accuracy 96.09\n",
      "Iteration  50 | Train Loss  0.1534 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1470 | Classifier Accuracy 95.31\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1575 | Accuracy 94.16 \n",
      "Valid Loss Mean 0.1618 | Accuracy 92.42 \n",
      "\n",
      " ====================== epoch 27 ======================\n",
      "Iteration   0 | Train Loss  0.1327 | Classifier Accuracy 94.53\n",
      "Iteration  50 | Train Loss  0.1538 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1446 | Classifier Accuracy 96.88\n",
      "\n",
      "[Summary] Elapsed time : 1 m 0 s\n",
      "Train Loss Mean 0.1586 | Accuracy 94.18 \n",
      "Valid Loss Mean 0.1491 | Accuracy 93.88 \n",
      "\n",
      " ====================== epoch 28 ======================\n",
      "Iteration   0 | Train Loss  0.1117 | Classifier Accuracy 98.44\n",
      "Iteration  50 | Train Loss  0.1456 | Classifier Accuracy 95.31\n",
      "Iteration 100 | Train Loss  0.1954 | Classifier Accuracy 92.19\n",
      "\n",
      "[Summary] Elapsed time : 0 m 60 s\n",
      "Train Loss Mean 0.1578 | Accuracy 93.98 \n",
      "Valid Loss Mean 0.1506 | Accuracy 93.54 \n",
      "\n",
      " ====================== epoch 29 ======================\n",
      "Iteration   0 | Train Loss  0.0984 | Classifier Accuracy 97.66\n",
      "Iteration  50 | Train Loss  0.1415 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.2146 | Classifier Accuracy 92.97\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1526 | Accuracy 94.26 \n",
      "Valid Loss Mean 0.1530 | Accuracy 92.94 \n",
      "\n",
      " ====================== epoch 30 ======================\n",
      "Iteration   0 | Train Loss  0.1101 | Classifier Accuracy 96.88\n",
      "Iteration  50 | Train Loss  0.1686 | Classifier Accuracy 96.88\n",
      "Iteration 100 | Train Loss  0.1308 | Classifier Accuracy 96.09\n",
      "\n",
      "[Summary] Elapsed time : 1 m 1 s\n",
      "Train Loss Mean 0.1521 | Accuracy 94.30 \n",
      "Valid Loss Mean 0.1502 | Accuracy 94.09 \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "mask_best_model.eval()\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "mask_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        scores = mask_best_model(images)\n",
    "        preds = scores.argmax(dim=-1)\n",
    "        mask_predictions.extend(preds.cpu().numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "from collections import Counter\n",
    "Counter(mask_predictions) #이야 더 확고해졌네\n",
    "#레이블링 관련 알고리즘을 살펴보자. startswith의 대용은?\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({0: 12600})"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "\n",
    "gender_best_model.eval()\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "gender_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        scores = gender_best_model(images)\n",
    "        preds = scores.argmax(dim=-1)\n",
    "        gender_predictions.extend(preds.cpu().numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "from collections import Counter\n",
    "Counter(gender_predictions)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({0: 4933, 1: 7667})"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "age_best_model.eval()\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "age_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        scores = age_best_model(images)\n",
    "        preds = scores.argmax(dim=-1)\n",
    "        age_predictions.extend(preds.cpu().numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "from collections import Counter\n",
    "Counter(age_predictions)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({0: 2827, 1: 8740, 2: 1033})"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "all_predictions = []\n",
    "size = len(submission)\n",
    "class_map = np.array([[[0, 1, 2],\n",
    "                       [3, 4, 5]],\n",
    "                      [[6, 7, 8],\n",
    "                       [9, 10, 11]],\n",
    "                      [[12, 13, 14],\n",
    "                       [15, 16, 17]]])\n",
    "for idx in range(size):\n",
    "    i = mask_predictions[idx]\n",
    "    j = gender_predictions[idx]\n",
    "    k = age_predictions[idx]\n",
    "    all_predictions.append(class_map[i][j][k])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission_baseline_pretrained.csv'), index=False)\n",
    "print('test inference is done!')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(now)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-08-24 18:25:39.126484\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "print(now)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-08-24 18:25:39.126484\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}