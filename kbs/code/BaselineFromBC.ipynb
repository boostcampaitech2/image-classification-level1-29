{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "import timm\n",
    "from timm.models.layers.classifier import ClassifierHead\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_dir = '/opt/ml/input/data/train'\n",
    "img_dir = f'{data_dir}/images'\n",
    "df_path = f'{data_dir}/train.csv'\n",
    "df = pd.read_csv(df_path)\n",
    "project_name = 'wandb_bc'\n",
    "wandb_dir = './model'\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_img_stats(img_dir, img_ids):\n",
    "    img_info = dict(heights=[], widths=[], means=[], stds=[])\n",
    "    for img_id in tqdm(img_ids):\n",
    "        for path in glob(os.path.join(img_dir, img_id, '*')):\n",
    "            img = np.array(Image.open(path))\n",
    "            h, w, _ = img.shape\n",
    "            img_info['heights'].append(h)\n",
    "            img_info['widths'].append(w)\n",
    "            img_info['means'].append(img.mean(axis=(0,1)))\n",
    "            img_info['stds'].append(img.std(axis=(0,1)))\n",
    "    return img_info"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# img_info = get_img_stats(img_dir, df.path.values)\n",
    "\n",
    "# print(f'RGB Mean: {np.mean(img_info[\"means\"], axis=0) / 255.}')\n",
    "# print(f'RGB Standard Deviation: {np.mean(img_info[\"stds\"], axis=0) / 255.}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mean, std = ( 0.5601,0.5241,0.5014),(0.2332,0.2430,0.2456)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transforms(need=('train', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    \"\"\"\n",
    "    train 혹은 validation의 augmentation 함수를 정의합니다. train은 데이터에 많은 변형을 주어야하지만, validation에는 최소한의 전처리만 주어져야합니다.\n",
    "    \n",
    "    Args:\n",
    "        need: 'train', 혹은 'val' 혹은 둘 다에 대한 augmentation 함수를 얻을 건지에 대한 옵션입니다.\n",
    "        img_size: Augmentation 이후 얻을 이미지 사이즈입니다.\n",
    "        mean: 이미지를 Normalize할 때 사용될 RGB 평균값입니다.\n",
    "        std: 이미지를 Normalize할 때 사용될 RGB 표준편차입니다.\n",
    "\n",
    "    Returns:\n",
    "        transformations: Augmentation 함수들이 저장된 dictionary 입니다. transformations['train']은 train 데이터에 대한 augmentation 함수가 있습니다.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = Compose([\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            GaussNoise(p=0.5),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = Compose([\n",
    "            Resize(img_size[0], img_size[1]),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### 마스크 여부, 성별, 나이를 mapping할 클래스를 생성합니다.\n",
    "\n",
    "class MaskLabels:\n",
    "    mask = 0\n",
    "    incorrect = 1\n",
    "    normal = 2\n",
    "\n",
    "class GenderLabels:\n",
    "    male = 0\n",
    "    female = 1\n",
    "\n",
    "class AgeGroup:\n",
    "    map_label = lambda x: 0 if int(x) < 30 else 1 if int(x) < 60 else 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MaskBaseDataset(data.Dataset):\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    _file_names = {\n",
    "        \"mask1.jpg\": MaskLabels.mask,\n",
    "        \"mask2.jpg\": MaskLabels.mask,\n",
    "        \"mask3.jpg\": MaskLabels.mask,\n",
    "        \"mask4.jpg\": MaskLabels.mask,\n",
    "        \"mask5.jpg\": MaskLabels.mask,\n",
    "        \"incorrect_mask.jpg\": MaskLabels.incorrect,\n",
    "        \"normal.jpg\": MaskLabels.normal\n",
    "    }\n",
    "\n",
    "    image_paths = []\n",
    "    mask_labels = []\n",
    "    gender_labels = []\n",
    "    age_labels = []\n",
    "\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        MaskBaseDataset을 initialize 합니다.\n",
    "\n",
    "        Args:\n",
    "            img_dir: 학습 이미지 폴더의 root directory 입니다.\n",
    "            transform: Augmentation을 하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.transform = transform\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        transform 함수를 설정하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        image의 경로와 각 이미지들의 label을 계산하여 저장해두는 함수입니다.\n",
    "        \"\"\"\n",
    "        profiles = os.listdir(self.img_dir)\n",
    "        for profile in profiles:\n",
    "            for file_name, label in self._file_names.items():\n",
    "                img_path = os.path.join(self.img_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
    "                if os.path.exists(img_path):\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.mask_labels.append(label)\n",
    "\n",
    "                    id, gender, race, age = profile.split(\"_\")\n",
    "                    gender_label = getattr(GenderLabels, gender)\n",
    "                    age_label = AgeGroup.map_label(age)\n",
    "\n",
    "                    self.gender_labels.append(gender_label)\n",
    "                    self.age_labels.append(age_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        데이터를 불러오는 함수입니다. \n",
    "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            index: 불러올 데이터의 인덱스값입니다.\n",
    "        \"\"\"\n",
    "        # 이미지를 불러옵니다.\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # 레이블을 불러옵니다.\n",
    "        mask_label = self.mask_labels[index]\n",
    "        gender_label = self.gender_labels[index]\n",
    "        age_label = self.age_labels[index]\n",
    "        multi_class_label = int(mask_label) * 6 + int(gender_label) * 3 + int(age_label)\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        image_transform = self.transform(image=np.array(image))['image']\n",
    "        return image_transform, multi_class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
    "transform = get_transforms(mean=mean, std=std)\n",
    "\n",
    "dataset = MaskBaseDataset(\n",
    "    img_dir=img_dir\n",
    ")\n",
    "\n",
    "# train dataset과 validation dataset을 8:2 비율로 나눕니다.\n",
    "n_val = int(len(dataset) * 0.2)\n",
    "n_train = len(dataset) - n_val\n",
    "train_dataset, val_dataset = data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# 각 dataset에 augmentation 함수를 설정합니다.\n",
    "train_dataset.dataset.set_transform(transform['train'])\n",
    "val_dataset.dataset.set_transform(transform['val'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_loader = data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    shuffle=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Augmentation으로 이미지를 Normalize했기 때문에, 역으로 다시 Normalize 해주어야합니다.\n",
    "# inv_normalize = transforms.Normalize(\n",
    "#     mean=[-m / s for m, s in zip(mean, std)],\n",
    "#     std=[1 / s for s in std]\n",
    "# )\n",
    "\n",
    "# n_rows, n_cols = 4, 3\n",
    "\n",
    "# fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True, figsize=(16, 24))\n",
    "# for i in range(n_rows*n_cols):\n",
    "#     axes[i%n_rows][i//(n_cols+1)].imshow(inv_normalize(images[i]).permute(1, 2, 0))\n",
    "#     axes[i%n_rows][i//(n_cols+1)].set_title(f'Label: {labels[i]}', color='r')\n",
    "# plt.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#이제 모델링 \n",
    "\n",
    "model_name = \"res2next50\"\n",
    "model = timm.create_model('res2next50', pretrained=True)\n",
    "\n",
    "from timm.models.layers.classifier import ClassifierHead\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self,model_name,pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model=timm.create_model(model_name,pretrained=True)\n",
    "        n_features=self.model.num_features\n",
    "        \n",
    "        self.mask_classifier=ClassifierHead(n_features,3)\n",
    "        self.gender_classifier=ClassifierHead(n_features,2)\n",
    "        self.age_classifier=ClassifierHead(n_features,3)\n",
    "    def forward(self,x):\n",
    "        x=self.model.forward_features(x)\n",
    "        \n",
    "        mask=self.mask_classifier(x).view(x.size(0),3,1,1)\n",
    "        gender=self.gender_classifier(x).view(x.size(0),1,2,1)\n",
    "        age=self.age_classifier(x).view(x.size(0),1,1,3)\n",
    "        return (mask*gender*age).view(x.size(0),-1)\n",
    "        \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "initial_lr = 0.001\n",
    "num_epoch =30\n",
    "batch_size = 50\n",
    "betas = (0.9, 0.999)\n",
    "weight_decay = 1e-4\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.optim as optim\n",
    "from novograd import *\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def initialize_weights(model):\n",
    "    \"\"\"\n",
    "    Initialize all weights using xavier uniform. \n",
    "    For more weight initialization methods, check https://pytorch.org/docs/stable/nn.init.html\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.01)\n",
    "            m.bias.data.zero_()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pprint as p\n",
    "net = MyModel(model_name,pretrained=True)\n",
    "\n",
    "net.to(device)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = NovoGrad(net.parameters(), lr=initial_lr, betas=betas, weight_decay=weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, \n",
    "                                                steps_per_epoch=10, epochs=10,anneal_strategy='linear')\n",
    "num_epochs = 30\n",
    "config={'epochs':num_epochs,'batch_size':batch_size,'learning_rate':initial_lr}\n",
    "wandb.init(project=project_name,config=config,dir=wandb_dir,)\n",
    "valid_early_stop = 0\n",
    "valid_best_loss = float('inf')\n",
    "EARLY_STOPPING_EPOCH = 5\n",
    "since = time.time()\n",
    "\n",
    "final_train_loss = []\n",
    "final_train_acc = []\n",
    "final_valid_loss = []\n",
    "final_valid_acc = []\n",
    "\n",
    "for e in range(num_epochs) :\n",
    "    p.pprint(f' ====================== epoch %d ======================' % (e+1) )\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    epoch_f1 = 0\n",
    "    n_iter = 0\n",
    "\n",
    "    # train\n",
    "    net.train()\n",
    "    for i, (images, targets) in enumerate(train_loader) : \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        scores = net(images)\n",
    "        \n",
    "        _, preds = scores.max(dim=1)\n",
    "\n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/images.shape[0] * 100)\n",
    "        epoch_f1 += f1_score(preds.cpu().numpy(), targets.cpu().numpy(), average='macro')\n",
    "        n_iter += 1\n",
    "\n",
    "        train_loss_list.append(loss)\n",
    "        train_acc_list.append(acc)\n",
    "        \n",
    "        if i % 50 == 0 :\n",
    "            p.pprint(f'Iteration %3.d | Train Loss  %.4f | Classifier Accuracy %2.2f' % (i, loss, acc))\n",
    "\n",
    "    train_mean_loss = np.mean(train_loss_list, dtype=\"float64\")\n",
    "    train_mean_acc = np.mean(train_acc_list, dtype=\"float64\")\n",
    "    wandb.log({'train loss' : train_mean_loss/(i+1), 'train_accuracy' : float(train_mean_acc)/(i+1),'train_f1 score':f1_score(targets.cpu(),preds.cpu(),average='macro')})\n",
    "    final_train_loss.append(train_mean_loss)\n",
    "    final_train_acc.append(train_mean_acc)\n",
    "    \n",
    "    epoch_f1 = epoch_f1/n_iter\n",
    "\n",
    "    epoch_time = time.time() - since\n",
    "    since = time.time()\n",
    "\n",
    "    p.pprint('')\n",
    "    p.pprint(f'[Summary] Elapsed time : %.0f m %.0f s' % (epoch_time // 60, epoch_time % 60))\n",
    "    p.pprint(f'Train Loss Mean %.4f | Accuracy %2.2f | F1-Score %2.4f' % (train_mean_loss, train_mean_acc, epoch_f1) )\n",
    "\n",
    "    # validation \n",
    "    net.eval()\n",
    "    epoch_f1 = 0\n",
    "    n_iter = 0\n",
    "    valid_loss_list = []\n",
    "    valid_acc_list = []\n",
    "    \n",
    "    for i, (images, targets) in enumerate(valid_loader) : \n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores = net(images)  \n",
    "            loss = F.cross_entropy(scores, targets)\n",
    "            _, preds = scores.max(dim=1)\n",
    "            epoch_f1 += f1_score(preds.cpu().numpy(), targets.cpu().numpy(), average='macro')\n",
    "            n_iter += 1\n",
    "\n",
    "        correct = sum(targets == preds).cpu()\n",
    "        acc=(correct/images.shape[0] * 100)\n",
    "\n",
    "        valid_loss_list.append(loss)\n",
    "        valid_acc_list.append(acc)\n",
    "\n",
    "\n",
    "    val_mean_loss = np.mean(valid_loss_list, dtype=\"float64\")\n",
    "    val_mean_acc = np.mean(valid_acc_list, dtype=\"float64\")\n",
    "    wandb.log({'valid_loss' : val_mean_loss/(i+1), 'valid_accuracy' : float(val_mean_acc)/(i+1),'valid_f1 score':f1_score(targets.cpu(),preds.cpu(),average='macro')})\n",
    "\n",
    "    final_valid_loss.append(val_mean_loss)\n",
    "    final_valid_acc.append(val_mean_acc)\n",
    "    \n",
    "    epoch_f1 = epoch_f1/n_iter\n",
    "    \n",
    "    p.pprint(f'Valid Loss Mean %.4f | Accuracy %2.2f | F1-Score %2.4f' % (val_mean_loss, val_mean_acc, epoch_f1) )\n",
    "    p.pprint('')\n",
    "\n",
    "    if val_mean_loss < valid_best_loss:\n",
    "        valid_best_loss = val_mean_loss\n",
    "        valid_early_stop = 0\n",
    "        # new best model save (valid 기준)\n",
    "        best_model = net\n",
    "        path = './model/'\n",
    "        torch.save(best_model.state_dict(), f'{path}model{val_mean_acc:2.2f}_epoch_{e}.pth')\n",
    "    else:\n",
    "        # early stopping    \n",
    "        valid_early_stop += 1\n",
    "        if valid_early_stop >= EARLY_STOPPING_EPOCH:\n",
    "            p.pprint(\"EARLY STOPPING!!\")\n",
    "            break\n",
    "\n",
    "    lr_scheduler.step()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        super(TestDataset).__init__()\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "testimage_dir = os.path.join(test_dir, 'images')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(testimage_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "])\n",
    "test_dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_model.eval()\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in test_loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        scores = best_model(images)\n",
    "        preds = scores.argmax(dim=-1)\n",
    "        all_predictions.extend(preds.cpu().numpy())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter\n",
    "Counter(all_predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission_RES2NEXT_step_weight_init.csv'), index=False)\n",
    "print('test inference is done!')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "print(datetime.now(timezone('asia/seoul')).strftime('%Y-%m-%d %H:%M:%S'))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}