{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "4_DataGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 4 - Data Generation\n",
        "- 이번 실습자료에서는 파이토치 모델에 이미지를 입력값으로 주기위해 전처리를 하는 방법을 배웁니다.\n",
        "- 파이토치는 torch.utils.data에 있는 Dataset, DataLoader 클래스가 이 작업을 간편하게 해줍니다.\n",
        "## 0. Libraries & Configurations\n",
        "- 시각화에 필요한 라이브러리와 데이터 경로를 설정합니다."
      ],
      "metadata": {
        "id": "geographic-foster"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import os\n",
        "import sys\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "from time import time\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "outputs": [],
      "metadata": {
        "id": "occasional-boxing"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "### Configurations\n",
        "data_dir = '/opt/ml/input/data/train'\n",
        "img_dir = f'{data_dir}/images'\n",
        "df_path = f'{data_dir}/train.csv'"
      ],
      "outputs": [],
      "metadata": {
        "id": "complex-israel"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "df = pd.read_csv(df_path)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id  gender   race  age                    path\n",
              "0  000001  female  Asian   45  000001_female_Asian_45\n",
              "1  000002  female  Asian   52  000002_female_Asian_52\n",
              "2  000004    male  Asian   54    000004_male_Asian_54\n",
              "3  000005  female  Asian   58  000005_female_Asian_58\n",
              "4  000006  female  Asian   59  000006_female_Asian_59"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>race</th>\n",
              "      <th>age</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000001</td>\n",
              "      <td>female</td>\n",
              "      <td>Asian</td>\n",
              "      <td>45</td>\n",
              "      <td>000001_female_Asian_45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000002</td>\n",
              "      <td>female</td>\n",
              "      <td>Asian</td>\n",
              "      <td>52</td>\n",
              "      <td>000002_female_Asian_52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000004</td>\n",
              "      <td>male</td>\n",
              "      <td>Asian</td>\n",
              "      <td>54</td>\n",
              "      <td>000004_male_Asian_54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000005</td>\n",
              "      <td>female</td>\n",
              "      <td>Asian</td>\n",
              "      <td>58</td>\n",
              "      <td>000005_female_Asian_58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>000006</td>\n",
              "      <td>female</td>\n",
              "      <td>Asian</td>\n",
              "      <td>59</td>\n",
              "      <td>000006_female_Asian_59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "metadata": {
        "id": "whole-computer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Image Statistics"
      ],
      "metadata": {
        "id": "minute-neighborhood"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2강 실습자료에서 사용되었던 데이터셋의 RGB 평균, 표준편차를 구하는 함수를 이용하여 그에 대해 계산합니다."
      ],
      "metadata": {
        "id": "christian-protest"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "def get_ext(img_dir, img_id):\n",
        "    filename = os.listdir(os.path.join(img_dir, img_id))[0]\n",
        "    ext = os.path.splitext(filename)[-1].lower()\n",
        "    return ext"
      ],
      "outputs": [],
      "metadata": {
        "id": "crude-frank"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "def get_img_stats(img_dir, img_ids):\n",
        "    img_info = dict(heights=[], widths=[], means=[], stds=[])\n",
        "    for img_id in tqdm(img_ids):\n",
        "        for path in glob(os.path.join(img_dir, img_id, '*')):\n",
        "            img = np.array(Image.open(path))\n",
        "            h, w, _ = img.shape\n",
        "            img_info['heights'].append(h)\n",
        "            img_info['widths'].append(w)\n",
        "            img_info['means'].append(img.mean(axis=(0,1)))\n",
        "            img_info['stds'].append(img.std(axis=(0,1)))\n",
        "    return img_info"
      ],
      "outputs": [],
      "metadata": {
        "id": "superb-profession"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "img_info = get_img_stats(img_dir, df.path.values)\n",
        "\n",
        "print(f'RGB Mean: {np.mean(img_info[\"means\"], axis=0) / 255.}')\n",
        "print(f'RGB Standard Deviation: {np.mean(img_info[\"stds\"], axis=0) / 255.}')"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-65570445b329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_img_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'RGB Mean: {np.mean(img_info[\"means\"], axis=0) / 255.}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'RGB Standard Deviation: {np.mean(img_info[\"stds\"], axis=0) / 255.}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-fd98c34b4d44>\u001b[0m in \u001b[0;36mget_img_stats\u001b[0;34m(img_dir, img_ids)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_img_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimg_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimg_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         self.container = self.status_printer(\n\u001b[0m\u001b[1;32m    224\u001b[0m             self.fp, total, self.desc, self.ncols)\n\u001b[1;32m    225\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Prepare IPython progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# #187 #451 #558 #872\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
          ]
        }
      ],
      "metadata": {
        "tags": [],
        "id": "undefined-patrol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset\n",
        "- 이 부분에서는 Dataset을 정의하는 방법을 간단하게 배웁니다."
      ],
      "metadata": {
        "id": "compliant-operation"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Augmentation Function\n",
        "- 3강에서 배운 Augmentation 함수를 정의합니다.\n",
        "- mean, std는 임의로 설정하였으나 파트 1에서 계산한 값을 입력해도 괜찮습니다."
      ],
      "metadata": {
        "id": "democratic-juvenile"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "mean, std = (0.5, 0.5, 0.5), (0.2, 0.2, 0.2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "horizontal-strain"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Torchvision Style Augmentation Function\n",
        "- Torchvision에서 제공되는 transforms를 이용한 Augmentation 함수입니다.\n",
        "- 이를 사용하여 Dataset을 정의하여도 괜찮지만, 이번 실습자료에서는 강의에서 배웠던 Albumentation을 활용해봅시다."
      ],
      "metadata": {
        "id": "distant-recycling"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "''' Torchvision-Style Transforms '''\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Resize, ToTensor, Normalize, GaussianBlur, RandomRotation, ColorJitter\n",
        "\n",
        "\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "\n",
        "def get_transforms(need=('train', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
        "    transformations = {}\n",
        "    if 'train' in need:\n",
        "        transformations['train'] = transforms.Compose([\n",
        "            Resize((img_size[0], img_size[1])),\n",
        "            RandomRotation([-8, +8]),\n",
        "            GaussianBlur(51, (0.1, 2.0)),\n",
        "            ColorJitter(brightness=0.5, saturation=0.5, hue=0.5),  # todo : param\n",
        "            ToTensor(),\n",
        "            Normalize(mean=mean, std=std),\n",
        "            AddGaussianNoise(0., 1.)\n",
        "        ])\n",
        "    if 'val' in need:\n",
        "        transformations['val'] = transforms.Compose([\n",
        "            Resize((img_size[0], img_size[1])),\n",
        "            ToTensor(),\n",
        "            Normalize(mean=mean, std=std),\n",
        "        ])\n",
        "    return transformations\n"
      ],
      "outputs": [],
      "metadata": {
        "tags": [],
        "id": "current-hometown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 Albumentation Style Augmentation Function\n",
        "- Albumentation은 numpy 형식으로 이미지를 받아 데이터를 변형시킵니다.\n",
        "- opencv 기반으로 빠르고, 다양한 Augmentation 방법이 제공되는 점에서 장점이 있습니다."
      ],
      "metadata": {
        "id": "sudden-survivor"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "from albumentations import *\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "\n",
        "def get_transforms(need=('train', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
        "    \"\"\"\n",
        "    train 혹은 validation의 augmentation 함수를 정의합니다. train은 데이터에 많은 변형을 주어야하지만, validation에는 최소한의 전처리만 주어져야합니다.\n",
        "    \n",
        "    Args:\n",
        "        need: 'train', 혹은 'val' 혹은 둘 다에 대한 augmentation 함수를 얻을 건지에 대한 옵션입니다.\n",
        "        img_size: Augmentation 이후 얻을 이미지 사이즈입니다.\n",
        "        mean: 이미지를 Normalize할 때 사용될 RGB 평균값입니다.\n",
        "        std: 이미지를 Normalize할 때 사용될 RGB 표준편차입니다.\n",
        "\n",
        "    Returns:\n",
        "        transformations: Augmentation 함수들이 저장된 dictionary 입니다. transformations['train']은 train 데이터에 대한 augmentation 함수가 있습니다.\n",
        "    \"\"\"\n",
        "    transformations = {}\n",
        "    if 'train' in need:\n",
        "        transformations['train'] = Compose([\n",
        "            Resize(img_size[0], img_size[1], p=1.0),\n",
        "            HorizontalFlip(p=0.5),\n",
        "            ShiftScaleRotate(p=0.5),\n",
        "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
        "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
        "            GaussNoise(p=0.5),\n",
        "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ], p=1.0)\n",
        "    if 'val' in need:\n",
        "        transformations['val'] = Compose([\n",
        "            Resize(img_size[0], img_size[1]),\n",
        "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ], p=1.0)\n",
        "    return transformations"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'albumentations'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e6e36d179127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToTensorV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.548\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.504\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.479\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.237\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.247\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.246\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'albumentations'"
          ]
        }
      ],
      "metadata": {
        "id": "national-diameter"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Define Dataset\n",
        "\n",
        "- 여기에서는 이미지와 레이블을 출력하는 Dataset 클래스를 정의합니다.\n",
        "- 레이블은 마스크 여부, 성별, 나이로 결정이 됩니다.\n",
        "- 레이블은 3(마스크 착용, 미착용, 잘못착용) * 2(남성, 여성) * 3(30세 미만, 30세-60세, 60세 이상) 으로 총 18개가 존재합니다."
      ],
      "metadata": {
        "id": "partial-lafayette"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "### 마스크 여부, 성별, 나이를 mapping할 클래스를 생성합니다.\n",
        "\n",
        "class MaskLabels:\n",
        "    mask = 0\n",
        "    incorrect = 1\n",
        "    normal = 2\n",
        "\n",
        "class GenderLabels:\n",
        "    male = 0\n",
        "    female = 1\n",
        "\n",
        "class AgeGroup:\n",
        "    map_label = lambda x: 0 if int(x) < 30 else 1 if int(x) < 60 else 2"
      ],
      "outputs": [],
      "metadata": {
        "id": "annoying-emission"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "class MaskBaseDataset(data.Dataset):\n",
        "    num_classes = 3 * 2 * 3\n",
        "\n",
        "    _file_names = {\n",
        "        \"mask1.jpg\": MaskLabels.mask,\n",
        "        \"mask2.jpg\": MaskLabels.mask,\n",
        "        \"mask3.jpg\": MaskLabels.mask,\n",
        "        \"mask4.jpg\": MaskLabels.mask,\n",
        "        \"mask5.jpg\": MaskLabels.mask,\n",
        "        \"incorrect_mask.jpg\": MaskLabels.incorrect,\n",
        "        \"normal.jpg\": MaskLabels.normal\n",
        "    }\n",
        "\n",
        "    image_paths = []\n",
        "    mask_labels = []\n",
        "    gender_labels = []\n",
        "    age_labels = []\n",
        "\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        \"\"\"\n",
        "        MaskBaseDataset을 initialize 합니다.\n",
        "\n",
        "        Args:\n",
        "            img_dir: 학습 이미지 폴더의 root directory 입니다.\n",
        "            transform: Augmentation을 하는 함수입니다.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.transform = transform\n",
        "\n",
        "        self.setup()\n",
        "\n",
        "    def set_transform(self, transform):\n",
        "        \"\"\"\n",
        "        transform 함수를 설정하는 함수입니다.\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        \n",
        "    def setup(self):\n",
        "        \"\"\"\n",
        "        image의 경로와 각 이미지들의 label을 계산하여 저장해두는 함수입니다.\n",
        "        \"\"\"\n",
        "        profiles = os.listdir(self.img_dir)\n",
        "        for profile in profiles:\n",
        "            for file_name, label in self._file_names.items():\n",
        "                img_path = os.path.join(self.img_dir, profile, file_name)  # (resized_data, 000004_male_Asian_54, mask1.jpg)\n",
        "                if os.path.exists(img_path):\n",
        "                    self.image_paths.append(img_path)\n",
        "                    self.mask_labels.append(label)\n",
        "\n",
        "                    id, gender, race, age = profile.split(\"_\")\n",
        "                    gender_label = getattr(GenderLabels, gender)\n",
        "                    age_label = AgeGroup.map_label(age)\n",
        "\n",
        "                    self.gender_labels.append(gender_label)\n",
        "                    self.age_labels.append(age_label)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        데이터를 불러오는 함수입니다. \n",
        "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
        "        \n",
        "        Args:\n",
        "            index: 불러올 데이터의 인덱스값입니다.\n",
        "        \"\"\"\n",
        "        # 이미지를 불러옵니다.\n",
        "        image_path = self.image_paths[index]\n",
        "        image = Image.open(image_path)\n",
        "        \n",
        "        # 레이블을 불러옵니다.\n",
        "        mask_label = self.mask_labels[index]\n",
        "        gender_label = self.gender_labels[index]\n",
        "        age_label = self.age_labels[index]\n",
        "        multi_class_label = mask_label * 6 + gender_label * 3 + age_label\n",
        "        \n",
        "        # 이미지를 Augmentation 시킵니다.\n",
        "        image_transform = self.transform(image=np.array(image))['image']\n",
        "        return image_transform, multi_class_label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)"
      ],
      "outputs": [],
      "metadata": {
        "id": "divine-transmission"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
        "transform = get_transforms(mean=mean, std=std)\n",
        "\n",
        "dataset = MaskBaseDataset(\n",
        "    img_dir=img_dir\n",
        ")\n",
        "\n",
        "# train dataset과 validation dataset을 8:2 비율로 나눕니다.\n",
        "n_val = int(len(dataset) * 0.2)\n",
        "n_train = len(dataset) - n_val\n",
        "train_dataset, val_dataset = data.random_split(dataset, [n_train, n_val])\n",
        "\n",
        "# 각 dataset에 augmentation 함수를 설정합니다.\n",
        "train_dataset.dataset.set_transform(transform['train'])\n",
        "val_dataset.dataset.set_transform(transform['val'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "secure-plasma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. DataLoader\n",
        "- 정의한 Dataset을 바탕으로 DataLoader을 생성합니다.\n",
        "- Dataset은 이미지 한장을 주는 모듈이라면, DataLoader은 여러 이미지를 batch_size만큼 묶어 전달해줍니다."
      ],
      "metadata": {
        "id": "regulation-membrane"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "# training dataloader은 데이터를 섞어주어야 합니다. (shuffle=True)\n",
        "train_loader = data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=12,\n",
        "    num_workers=4,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=12,\n",
        "    num_workers=4,\n",
        "    shuffle=False\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "blessed-robert"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.  Visualize Processed Data\n",
        "- 파트 4에선 정의한 DataLoader을 이용하여 데이터가 어떻게 전처리 되었는지 시각화하여 확인합니다."
      ],
      "metadata": {
        "id": "tracked-richardson"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "images, labels = next(iter(train_loader))\n",
        "print(f'images shape: {images.shape}')\n",
        "print(f'labels shape: {labels.shape}')"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataset.py\", line 272, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"<ipython-input-12-fcde65d92f94>\", line 78, in __getitem__\n    image_transform = self.transform(image=np.array(image))['image']\nTypeError: __call__() got an unexpected keyword argument 'image'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-703d6e4411c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'images shape: {images.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'labels shape: {labels.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataset.py\", line 272, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"<ipython-input-12-fcde65d92f94>\", line 78, in __getitem__\n    image_transform = self.transform(image=np.array(image))['image']\nTypeError: __call__() got an unexpected keyword argument 'image'\n"
          ]
        }
      ],
      "metadata": {
        "id": "explicit-bathroom"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Augmentation으로 이미지를 Normalize했기 때문에, 역으로 다시 Normalize 해주어야합니다.\n",
        "inv_normalize = transforms.Normalize(\n",
        "    mean=[-m / s for m, s in zip(mean, std)],\n",
        "    std=[1 / s for s in std]\n",
        ")\n",
        "\n",
        "n_rows, n_cols = 4, 3\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, sharex=True, sharey=True, figsize=(16, 24))\n",
        "for i in range(n_rows*n_cols):\n",
        "    axes[i%n_rows][i//(n_cols+1)].imshow(inv_normalize(images[i]).permute(1, 2, 0))\n",
        "    axes[i%n_rows][i//(n_cols+1)].set_title(f'Label: {labels[i]}', color='r')\n",
        "plt.tight_layout()"
      ],
      "outputs": [],
      "metadata": {
        "id": "several-reynolds"
      }
    }
  ]
}