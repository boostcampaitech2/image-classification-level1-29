{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d041a8db-95df-43e5-8f88-2399062ddb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import wandb\n",
    "import copy\n",
    "import timm\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import gridspec\n",
    "\n",
    "from torchinfo import summary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cwd=os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e58627-850a-4a57-9043-793a52e61f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_images(meta,img_dir,train):\n",
    "    images=[]\n",
    "    labels=[]\n",
    "    if train:\n",
    "        for idx in range(len(meta)):\n",
    "            folder_path=os.path.join(img_dir, meta.path[idx])\n",
    "            for img in os.listdir(folder_path):\n",
    "                if '._' in img:\n",
    "                    continue\n",
    "                images.append(os.path.join(folder_path,img))\n",
    "                labels.append((('incorrect' in img)+('normal' in img)*2)*6+(meta.gender[idx]=='female')*3+(30<=meta.age[idx])+(60<=meta.age[idx]))\n",
    "    else:\n",
    "        for img_id in meta.ImageID:\n",
    "            images.append(os.path.join(img_dir, img_id))\n",
    "    return images,labels\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self,train=True):\n",
    "        self.train=train\n",
    "        self.md=['info','train']\n",
    "        self.path=[os.path.join(cwd,'input/data/eval'),os.path.join(cwd,'input/data/train')]\n",
    "        self.meta=pd.read_csv(os.path.join(self.path[train], f'{self.md[train]}.csv'))\n",
    "        self.img_dir=os.path.join(self.path[train],'images')\n",
    "        self.classes=[('Wear','Incorrect','Not Wear'),('남','여'),('<30','>=30 and <60','>=60')]\n",
    "        \n",
    "        self.images,self.labels=make_images(self.meta,self.img_dir,train)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image=ToTensor()(Image.open(self.images[idx]))\n",
    "        if self.train:\n",
    "            label=torch.tensor(self.labels[idx])\n",
    "        else:\n",
    "            label=0\n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7000c4fd-8923-4387-bfac-c22ea5e21be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18900\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4c82e527884ea797e52040b9c2ba16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "600\n",
      "900\n",
      "1200\n",
      "1500\n",
      "1800\n",
      "2100\n",
      "2400\n",
      "2700\n",
      "3000\n",
      "3300\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3041abcbc39b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data=ImageDataset()\n",
    "print(len(data))\n",
    "with tqdm(enumerate(data)) as pbar:\n",
    "    for n,(image,label) in pbar:\n",
    "        if n==0:\n",
    "            images=image.view(-1)\n",
    "            continue\n",
    "        images=torch.vstack((images,image.view(-1)))\n",
    "        if n%300==0:\n",
    "            print(n)\n",
    "        \n",
    "labels=torch.tensor(data.labels)\n",
    "\n",
    "print(images.shape,labels.shape)\n",
    "print(pd.Series(data.labels).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33cf545-2ef3-4282-baf2-9f5cb0792a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote=SMOTE(random_state=0)\n",
    "smote_images,smote_labels=smote.fit_sample(images,labels)\n",
    "\n",
    "print(smote_images.shape,smote_labels.shape)\n",
    "print(pd.Series(smote_labels).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b408c3-bcb6-497d-9b0b-4bcd5f2b23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
